documents:
- id: arxiv.org/2408.12622
  name: 'The AI Risk Repository: A Comprehensive Meta-Review, Database, and Taxonomy
    of Risks From Artificial Intelligence'
  description: 'The risks posed by Artificial Intelligence (AI) are of considerable
    concern to academics, auditors, policymakers, AI companies, and the public. However,
    a lack of shared understanding of AI risks can impede our ability to comprehensively
    discuss, research, and react to them. This paper addresses this gap by creating
    an AI Risk Repository to serve as a common frame of reference. This comprises
    a living database of 777 risks extracted from 43 taxonomies, which can be filtered
    based on two overarching taxonomies and easily accessed, modified, and updated
    via our website and online spreadsheets. We construct our Repository with a systematic
    review of taxonomies and other structured classifications of AI risk followed
    by an expert consultation. We develop our taxonomies of AI risk using a best-fit
    framework synthesis. Our high-level Causal Taxonomy of AI Risks classifies each
    risk by its causal factors (1) Entity: Human, AI; (2) Intentionality: Intentional,
    Unintentional; and (3) Timing: Pre-deployment; Post-deployment. Our mid-level
    Domain Taxonomy of AI Risks classifies risks into seven AI risk domains: (1) Discrimination
    & toxicity, (2) Privacy & security, (3) Misinformation, (4) Malicious actors &
    misuse, (5) Human-computer interaction, (6) Socioeconomic & environmental, and
    (7) AI system safety, failures, & limitations. These are further divided into
    23 subdomains. The AI Risk Repository is, to our knowledge, the first attempt
    to rigorously curate, analyze, and extract AI risk frameworks into a publicly
    accessible, comprehensive, extensible, and categorized risk database. This creates
    a foundation for a more coordinated, coherent, and complete approach to defining,
    auditing, and managing the risks posed by AI systems.'
  url: https://arxiv.org/abs/2408.12622
  dateCreated: 2024-08-14
  dateModified: 2024-08-14
taxonomies:
- id: mit-ai-risk-repository
  name: The AI Risk Repository
  description: A comprehensive living database of over 700 AI risks categorized by
    their cause and risk domain.
  url: https://airisk.mit.edu/
  dateCreated: 2024-08-16
  version: '1'
  hasDocumentation:
  - arxiv.org/2408.12622
riskgroups:
- id: mit-ai-risk-domain-1
  name: Discrimination & Toxicity
  isDefinedByTaxonomy: mit-ai-risk-repository
- id: mit-ai-risk-domain-2
  name: Privacy & Security
  isDefinedByTaxonomy: mit-ai-risk-repository
- id: mit-ai-risk-domain-3
  name: Misinformation
  isDefinedByTaxonomy: mit-ai-risk-repository
- id: mit-ai-risk-domain-4
  name: Malicious actors
  isDefinedByTaxonomy: mit-ai-risk-repository
- id: mit-ai-risk-domain-5
  name: Human- Computer Interaction
  isDefinedByTaxonomy: mit-ai-risk-repository
- id: mit-ai-risk-domain-6
  name: Socioeconomic & Environmental
  isDefinedByTaxonomy: mit-ai-risk-repository
- id: mit-ai-risk-domain-7
  name: AI system safety, failures, & limitations
  isDefinedByTaxonomy: mit-ai-risk-repository
risks:
- id: mit-ai-risk-subdomain-1.1
  name: Unfair discrimination and misrepresentation
  description: Unequal treatment of individuals or groups by AI, often based on race,
    gender, or other sensitive characteristics, resulting in unfair outcomes and representation
    of those groups.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-1
- id: mit-ai-risk-subdomain-1.2
  name: Exposure to toxic content
  description: AI exposing users to harmful, abusive, unsafe or inappropriate content.
    May involve AI creating, describing, providing advice, or encouraging action.
    Examples of toxic content include hate-speech, violence, extremism, illegal acts,
    child sexual abuse material, as well as content that violates community norms
    such as profanity, inflammatory political speech, or pornography.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-1
- id: mit-ai-risk-subdomain-1.3
  name: Unequal performance across groups
  description: Accuracy and effectiveness of AI decisions and actions is dependent
    on group membership, where decisions in AI system design and biased training data
    lead to unequal outcomes, reduced benefits, increased effort, and alienation of
    users.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-1
- id: mit-ai-risk-subdomain-2.1
  name: Compromise of privacy by obtaining, leaking or correctly inferring sensitive
    information
  description: AI systems that memorize and leak sensitive personal data or infer
    private information about individuals without their consent. Unexpected or unauthorized
    sharing of data and information can compromise user expectation of privacy, assist
    identity theft, or loss of confidential intellectual property.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-2
- id: mit-ai-risk-subdomain-2.2
  name: AI system security vulnerabilities and attacks
  description: Vulnerabilities in AI systems, software development toolchains, and
    hardware that can be exploited, resulting in unauthorized access, data and privacy
    breaches, or system manipulation causing unsafe outputs or behavior.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-2
- id: mit-ai-risk-subdomain-3.1
  name: False or misleading information
  description: AI systems that inadvertently generate or spread incorrect or deceptive
    information, which can lead to inaccurate beliefs in users and undermine their
    autonomy. Humans that make decisions based on false beliefs can experience physical,
    emotional or material harms
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-3
- id: mit-ai-risk-subdomain-3.2
  name: Pollution of information ecosystem and loss of consensus reality
  description: Highly personalized AI-generated misinformation creating ï¿½filter
    bubblesï¿½ where individuals only see what matches their existing beliefs, undermining
    shared reality, weakening social cohesion and political processes.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-3
- id: mit-ai-risk-subdomain-4.1
  name: Disinformation, surveillance, and influence at scale
  description: Using AI systems to conduct large-scale disinformation campaigns, malicious
    surveillance, or targeted and sophisticated automated censorship and propaganda,
    with the aim to manipulate political processes, public opinion and behavior.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-4
- id: mit-ai-risk-subdomain-4.2
  name: Cyberattacks, weapon development or use, and mass harm
  description: Using AI systems to develop cyber weapons (e.g., coding cheaper, more
    effective malware), develop new or enhance existing weapons (e.g., Lethal Autonomous
    Weapons or CBRNE), or use weapons to cause mass harm.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-4
- id: mit-ai-risk-subdomain-4.3
  name: Fraud, scams, and targeted manipulation
  description: Using AI systems to gain a personal advantage over others such as through
    cheating, fraud, scams, blackmail or targeted manipulation of beliefs or behavior.
    Examples include AI-facilitated plagiarism for research or education, impersonating
    a trusted or fake individual for illegitimate financial benefit, or creating humiliating
    or sexual imagery.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-4
- id: mit-ai-risk-subdomain-5.1
  name: Overreliance and unsafe use
  description: Users anthropomorphizing, trusting, or relying on AI systems, leading
    to emotional or material dependence and inappropriate relationships with or expectations
    of AI systems. Trust can be exploited by malicious actors (e.g., to harvest personal
    information or enable manipulation), or result in harm from inappropriate use
    of AI in critical situations (e.g., medical emergency). Overreliance on AI systems
    can compromise autonomy and weaken social ties.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-5
- id: mit-ai-risk-subdomain-5.2
  name: Loss of human agency and autonomy
  description: Humans delegating key decisions to AI systems, or AI systems making
    decisions that diminish human control and autonomy, potentially leading to humans
    feeling disempowered, losing the ability to shape a fulfilling life trajectory
    or becoming cognitively enfeebled.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-5
- id: mit-ai-risk-subdomain-6.1
  name: Power centralization and unfair distribution of benefits
  description: AI-driven concentration of power and resources within certain entities
    or groups, especially those with access to or ownership of powerful AI systems,
    leading to inequitable distribution of benefits and increased societal inequality.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-6
- id: mit-ai-risk-subdomain-6.2
  name: Increased inequality and decline in employment quality
  description: Widespread use of AI increasing social and economic inequalities, such
    as by automating jobs, reducing the quality of employment, or producing exploitative
    dependencies between workers and their employers.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-6
- id: mit-ai-risk-subdomain-6.3
  name: Economic and cultural devaluation of human effort
  description: AI systems capable of creating economic or cultural value, including
    through reproduction of human innovation or creativity (e.g., art, music, writing,
    code, invention), can destabilize economic and social systems that rely on human
    effort. This may lead to reduced appreciation for human skills, disruption of
    creative and knowledge-based industries, and homogenization of cultural experiences
    due to the ubiquity of AI-generated content.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-6
- id: mit-ai-risk-subdomain-6.4
  name: Competitive dynamics
  description: AI developers or state-like actors competing in an AI ï¿½raceï¿½ by
    rapidly developing, deploying, and applying AI systems to maximize strategic or
    economic advantage, increasing the risk they release unsafe and error-prone systems.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-6
- id: mit-ai-risk-subdomain-6.5
  name: Governance failure
  description: Inadequate regulatory frameworks and oversight mechanisms failing to
    keep pace with AI development, leading to ineffective governance and the inability
    to manage AI risks appropriately.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-6
- id: mit-ai-risk-subdomain-6.6
  name: Environmental harm
  description: The development and operation of AI systems causing environmental harm,
    such as through energy consumption of data centers, or material and carbon footprints
    associated with AI hardware.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-6
- id: mit-ai-risk-subdomain-7.1
  name: AI pursuing its own goals in conflict with human goals or values
  description: AI systems acting in conflict with human goals or values, especially
    the goals of designers or users, or ethical standards. These misaligned behaviors
    may be introduced by humans during design and development, such as through reward
    hacking and goal misgeneralisation, or may result from AI using dangerous capabilities
    such as manipulation, deception, situational awareness to seek power, self-proliferate,
    or achieve other goals.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-7
- id: mit-ai-risk-subdomain-7.2
  name: AI possessing dangerous capabilities
  description: AI systems that develop, access, or are provided with capabilities
    that increase their potential to cause mass harm through deception, weapons development
    and acquisition, persuasion and manipulation, political strategy, cyber-offense,
    AI development, situational awareness, and self-proliferation. These capabilities
    may cause mass harm due to malicious human actors, misaligned AI systems, or failure
    in the AI system.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-7
- id: mit-ai-risk-subdomain-7.3
  name: Lack of capability or robustness
  description: AI systems that fail to perform reliably or effectively under varying
    conditions, exposing them to errors and failures that can have significant consequences,
    especially in critical applications or areas that require moral reasoning.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-7
- id: mit-ai-risk-subdomain-7.4
  name: Lack of transparency or interpretability
  description: Challenges in understanding or explaining the decision-making processes
    of AI systems, which can lead to mistrust, difficulty in enforcing compliance
    standards or holding relevant actors accountable for harms, and the inability
    to identify and correct errors.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-7
- id: mit-ai-risk-subdomain-7.5
  name: AI welfare and rights
  description: Ethical considerations regarding the treatment of potentially sentient
    AI entities, including discussions around their potential rights and welfare,
    particularly as AI systems become more advanced and autonomous.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-7

