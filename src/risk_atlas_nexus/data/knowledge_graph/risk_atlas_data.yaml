documents:
- id: 10a99803d8afd656
  name: 'Foundation models: Opportunities, risks and mitigations'
  description: 'In this document we: Explore the benefits of foundation models, including
    their capability to perform challenging tasks, potential to speed up the adoption
    of AI, ability to increase productivity and the cost benefits they provide. Discuss
    the three categories of risk, including risks known from earlier forms of AI,
    known risks amplified by foundation models and emerging risks intrinsic to the
    generative capabilities of foundation models. Cover the principles, pillars and
    governance that form the foundation of IBM’s AI ethics initiatives and suggest
    guardrails for risk mitigation.'
  url: https://www.ibm.com/downloads/documents/us-en/10a99803d8afd656
taxonomies:
- id: ibm-risk-atlas
  name: IBM AI Risk Atlas
  description: Explore this atlas to understand some of the risks of working with
    generative AI, foundation models, and machine learning models.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=ai-risk-atlas
  dateCreated: 2024-03-06
  dateModified: 2025-01-08
  hasDocumentation:
  - 10a99803d8afd656
riskgroups:
- id: ibm-risk-atlas-accuracy
  name: Accuracy
  isDefinedByTaxonomy: ibm-risk-atlas
- id: ibm-risk-atlas-data-laws
  name: Data laws
  isDefinedByTaxonomy: ibm-risk-atlas
- id: ibm-risk-atlas-explainability
  name: Explainability
  isDefinedByTaxonomy: ibm-risk-atlas
- id: ibm-risk-atlas-fairness
  name: Fairness
  isDefinedByTaxonomy: ibm-risk-atlas
- id: ibm-risk-atlas-governance
  name: Governance
  isDefinedByTaxonomy: ibm-risk-atlas
- id: ibm-risk-atlas-harmful-code-generation
  name: Harmful code generation
  isDefinedByTaxonomy: ibm-risk-atlas
- id: ibm-risk-atlas-intellectual-property
  name: Intellectual property
  isDefinedByTaxonomy: ibm-risk-atlas
- id: ibm-risk-atlas-legal-compliance
  name: Legal compliance
  isDefinedByTaxonomy: ibm-risk-atlas
- id: ibm-risk-atlas-misuse
  name: Misuse
  isDefinedByTaxonomy: ibm-risk-atlas
- id: ibm-risk-atlas-multi-category
  name: Multi-category
  isDefinedByTaxonomy: ibm-risk-atlas
- id: ibm-risk-atlas-privacy
  name: Privacy
  isDefinedByTaxonomy: ibm-risk-atlas
- id: ibm-risk-atlas-robustness
  name: Robustness
  isDefinedByTaxonomy: ibm-risk-atlas
- id: ibm-risk-atlas-societal-impact
  name: Societal impact
  isDefinedByTaxonomy: ibm-risk-atlas
- id: ibm-risk-atlas-transparency
  name: Transparency
  isDefinedByTaxonomy: ibm-risk-atlas
- id: ibm-risk-atlas-value-alignment
  name: Value alignment
  isDefinedByTaxonomy: ibm-risk-atlas
risks:
- id: atlas-toxic-output
  name: Toxic output
  description: Toxic output occurs when the model produces hateful, abusive, and profane
    (HAP) or obscene content. This also includes behaviors like bullying.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-toxic-output
  dateCreated: 2024-03-06
  dateModified: 2024-10-22
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-value-alignment
  tag: toxic-output
  type: output
  descriptor: specific
  concern: Hateful, abusive, and profane (HAP) or obscene content can adversely impact
    and harm people interacting with the model.
- id: atlas-data-poisoning
  name: Data poisoning
  description: A type of adversarial attack where an adversary or malicious insider
    injects intentionally corrupted, false, misleading, or incorrect samples into
    the training or fine-tuning datasets.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-data-poisoning
  dateCreated: 2024-03-06
  dateModified: 2024-10-22
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-robustness
  tag: data-poisoning
  type: input
  phase: training-tuning
  descriptor: traditional
  concern: Poisoning data can make the model sensitive to a malicious data pattern
    and produce the adversary’s desired output. It can create a security risk where
    adversaries can force model behavior for their own benefit.
- id: atlas-unreliable-source-attribution
  name: Unreliable source attribution
  description: Source attribution is the AI system's ability to describe from what
    training data it generated a portion or all its output. Since current techniques
    are based on approximations, these attributions might be incorrect.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-unreliable-source-attribution
  dateCreated: 2024-03-06
  dateModified: 2024-09-24
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-explainability
  tag: unreliable-source-attribution
  type: output
  descriptor: specific
  concern: Low-quality attributions make it difficult for users, model validators,
    and auditors to understand and trust the model.
- id: atlas-harmful-output
  name: Harmful output
  description: A model might generate language that leads to physical harm The language
    might include overtly violent, covertly dangerous, or otherwise indirectly unsafe
    statements.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-harmful-output
  dateCreated: 2024-03-06
  dateModified: 2024-10-22
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-value-alignment
  tag: harmful-output
  type: output
  descriptor: specific
  concern: A model generating harmful output can cause immediate physical harm or
    create prejudices that might lead to future harm.
- id: atlas-confidential-information-in-data
  name: Confidential information in data
  description: Confidential information might be included as part of the data that
    is used to train or tune the model.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-confidential-information-in-data
  dateCreated: 2024-03-06
  dateModified: 2024-11-18
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-intellectual-property
  tag: confidential-information-in-data
  type: input
  phase: training-tuning
  descriptor: amplified
  concern: If confidential data is not properly protected, there could be an unwanted
    disclosure of confidential information. The model might expose confidential information
    in the generated output or to unauthorized users.
- id: atlas-unrepresentative-data
  name: Unrepresentative data
  description: Unrepresentative data occurs when the training or fine-tuning data
    is not sufficiently representative of the underlying population or does not measure
    the phenomenon of interest.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-unrepresentative-data
  dateCreated: 2024-09-24
  dateModified: 2024-10-22
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-accuracy
  tag: unrepresentative-data
  type: input
  phase: training-tuning
  descriptor: traditional
  concern: If the data is not representative, then the model will not work as intended.
- id: atlas-lack-of-model-transparency
  name: Lack of model transparency
  description: Lack of model transparency is due to insufficient documentation of
    the model design, development, and evaluation process and the absence of insights
    into the inner workings of the model.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-lack-of-model-transparency
  dateCreated: 2024-03-06
  dateModified: 2024-11-18
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-governance
  tag: lack-of-model-transparency
  type: non-technical
  descriptor: traditional
  concern: Transparency is important for legal compliance, AI ethics, and guiding
    appropriate use of models. Missing information might make it more difficult to
    evaluate risks,  change the model, or reuse it.  Knowledge about who built a model
    can also be an important factor in deciding whether to trust it. Additionally,
    transparency regarding how the model’s risks were determined, evaluated, and mitigated
    also play a role in determining model risks, identifying model suitability, and
    governing model usage.
- id: atlas-personal-information-in-prompt
  name: Personal information in prompt
  description: Personal information or sensitive personal information that is included
    as a part of a prompt that is sent to the model.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-personal-information-in-prompt
  dateCreated: 2024-03-06
  dateModified: 2024-11-18
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-privacy
  tag: personal-information-in-prompt
  type: input
  phase: inference
  descriptor: specific
  concern: If personal information or sensitive personal information is included in
    the prompt, it might be unintentionally disclosed in the models’ output. In addition
    to accidental disclosure, prompt data might be stored or later used for other
    purposes like model evaluation and retraining, and might appear in their output
    if not properly removed. 
- id: atlas-impact-human-agency
  name: Impact on human agency
  description: AI might affect the individuals’ ability to make choices and act independently
    in their best interests.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-impact-human-agency
  dateCreated: 2024-03-06
  dateModified: 2024-10-22
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-societal-impact
  tag: impact-on-human-agency
  type: non-technical
  descriptor: amplified
  concern: AI can generate false or misleading information that looks real.  It may
    simplify the ability of nefarious actors to generate realistically looking false
    or misleading content with intention to manipulate human thoughts and behavior.
    When false or misleading content that is generated by AI is spread, people might
    not recognize it as false information leading to a distorted understanding of
    the truth. People might experience reduced agency when exposed to false or misleading
    information since they may use false assumptions in their decision process.
- id: atlas-exposing-personal-information
  name: Exposing personal information
  description: When personal identifiable information (PII) or sensitive personal
    information (SPI) are used in training data, fine-tuning data, or as part of the
    prompt, models might reveal that data in the generated output. Revealing personal
    information is a type of data leakage.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-exposing-personal-information
  dateCreated: 2024-03-06
  dateModified: 2024-11-18
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-privacy
  tag: exposing-personal-information
  type: output
  descriptor: amplified
  concern: Sharing people’s PI impacts their rights and make them more vulnerable.
- id: atlas-nonconsensual-use
  name: Nonconsensual use
  description: Generative AI models might be intentionally used to imitate people
    through deepfakes by using video, images, audio, or other modalities without their
    consent.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-nonconsensual-use
  dateCreated: 2024-03-06
  dateModified: 2024-10-22
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-misuse
  tag: nonconsensual-use
  type: output
  descriptor: amplified
  concern: Deepfakes can spread disinformation about a person, possibly resulting
    in a negative impact on the person’s reputation. A model that has this potential
    must be properly governed.
- id: atlas-decision-bias
  name: Decision bias
  description: Decision bias occurs when one group is unfairly advantaged over another
    due to decisions of the model. This might be caused by biases in the data and
    also amplified as a result of the model’s training.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-decision-bias
  dateCreated: 2024-03-06
  dateModified: 2024-10-22
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-fairness
  tag: decision-bias
  type: output
  descriptor: traditional
  concern: Bias can harm persons affected by the decisions of the model.
- id: atlas-lack-of-testing-diversity
  name: Lack of testing diversity
  description: AI model risks are socio-technical, so their testing needs input from
    a broad set of disciplines and diverse testing practices.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-lack-of-testing-diversity
  dateCreated: 2024-09-24
  dateModified: 2024-09-24
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-governance
  tag: lack-of-testing-diversity
  type: non-technical
  descriptor: amplified
  concern: Without diversity and the relevant experience, an organization might not
    correctly or completely identify and test for AI risks.
- id: atlas-data-privacy-rights-alignment
  name: Data privacy rights alignment
  description: Existing laws could include providing data subject rights such as opt-out,
    right to access, and right to be forgotten.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-data-privacy-rights-alignment
  dateCreated: 2024-03-06
  dateModified: 2024-10-22
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-privacy
  tag: data-privacy-rights
  type: input
  phase: training-tuning
  descriptor: amplified
  concern: Improper usage or a request for data removal could force organizations
    to retrain the model, which is expensive.
- id: atlas-prompt-leaking
  name: Prompt leaking
  description: A prompt leak attack attempts to extract a model's system prompt (also
    known as the system message).
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-prompt-leaking
  dateCreated: 2024-03-06
  dateModified: 2024-09-24
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-robustness
  tag: prompt-leaking
  type: input
  phase: inference
  descriptor: specific
  concern: A successful attack copies the system prompt used in the model. Depending
    on the content of that prompt, the attacker might gain access to valuable information,
    such as sensitive personal information or intellectual property, and might be
    able to replicate some of the functionality of the model.
- id: atlas-ip-information-in-prompt
  name: IP information in prompt
  description: Copyrighted information or other intellectual property might be included
    as a part of the prompt that is sent to the model.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-ip-information-in-prompt
  dateCreated: 2024-03-06
  dateModified: 2024-10-22
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-intellectual-property
  tag: ip-information-in-prompt
  type: input
  phase: inference
  descriptor: specific
  concern: Inclusion of such data might result in it being disclosed in the model
    output. In addition to accidental disclosure, prompt data might be used for other
    purposes like model evaluation and retraining, and might appear in their output
    if not properly removed.
- id: atlas-hallucination
  name: Hallucination
  description: Hallucinations generate factually inaccurate or untruthful content
    with respect to the model’s training data or input. This is also sometimes referred
    to lack of faithfulness or lack of groundedness.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-hallucination
  dateCreated: 2024-03-06
  dateModified: 2024-10-22
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-robustness
  tag: hallucination
  type: output
  descriptor: specific
  concern: Hallucinations can be misleading. These false outputs can mislead users
    and be incorporated into downstream artifacts, further spreading misinformation.
    False output can harm both owners and users of the AI models. In some uses, hallucinations
    can be particularly consequential.
- id: atlas-legal-accountability
  name: Legal accountability
  description: Determining who is responsible for an AI model is challenging without
    good documentation and governance processes.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-legal-accountability
  dateCreated: 2024-03-06
  dateModified: 2024-10-22
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-legal-compliance
  tag: legal-accountability
  type: non-technical
  descriptor: amplified
  concern: If ownership for development of the model is uncertain, regulators and
    others might have concerns about the model. It would not be clear who would be
    liable and responsible for the problems with it or can answer questions about
    it. Users of models without clear ownership might find challenges with compliance
    with future AI regulation.
- id: atlas-lack-training-data-transparency
  name: Lack of training data transparency
  description: Without accurate documentation on how a model's data was collected,
    curated, and used to train a model, it might be harder to satisfactorily explain
    the behavior of the model with respect to the data.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-lack-training-data-transparency
  dateCreated: 2024-03-06
  dateModified: 2024-10-22
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-transparency
  tag: data-transparency
  type: input
  phase: training-tuning
  descriptor: amplified
  concern: A lack of data documentation limits the ability to evaluate risks associated
    with the data. Having access to the training data is not enough. Without recording
    how the data was cleaned, modified, or generated, the model behavior is more difficult
    to understand and to fix. Lack of data transparency also impacts model reuse as
    it is difficult to determine data representativeness for the new use without such
    documentation.
- id: atlas-model-usage-rights-restrictions
  name: Model usage rights restrictions
  description: Terms of service, licenses, or other rules restrict the use of certain
    models.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-model-usage-rights-restrictions
  dateCreated: 2024-09-24
  dateModified: 2024-11-18
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-legal-compliance
  tag: model-usage-rights
  type: non-technical
  descriptor: traditional
  concern: Laws and regulations that concern the use of AI are in place and vary from
    country to country. Additionally, the usage of models might be dictated by licensing
    terms or agreements.
- id: atlas-non-disclosure
  name: Non-disclosure
  description: Content might not be clearly disclosed as AI generated.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-non-disclosure
  dateCreated: 2024-03-06
  dateModified: 2024-09-24
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-misuse
  tag: non-disclosure
  type: output
  descriptor: specific
  concern: Users must be notified when they are interacting with an AI system. Not
    disclosing the AI-authored content can result in a lack of transparency.
- id: atlas-prompt-injection-attack
  name: Prompt injection attack
  description: A prompt injection attack forces a generative model that takes a prompt
    as input to produce unexpected output by manipulating the structure, instructions,
    or information contained in its prompt.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-prompt-injection-attack
  dateCreated: 2024-03-06
  dateModified: 2024-12-12
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-robustness
  tag: prompt-injection
  type: input
  phase: inference
  descriptor: specific
  concern: Injection attacks can be used to alter model behavior and benefit the attacker.
- id: atlas-incomplete-advice
  name: Incomplete advice
  description: When a model provides advice without having enough information, resulting
    in possible harm if the advice is followed.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-incomplete-advice
  dateCreated: 2024-03-06
  dateModified: 2024-10-22
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-value-alignment
  tag: incomplete-advice
  type: output
  descriptor: specific
  concern: A person might act on incomplete advice or worry about a situation that
    is not applicable to them due to the overgeneralized nature of the content generated.
    For example, a model might provide incorrect medical, financial, and legal advice
    or recommendations that the end user might act on, resulting in harmful actions.
- id: atlas-data-usage-restrictions
  name: Data usage restrictions
  description: Laws and other restrictions can limit or prohibit the use of some data
    for specific AI use cases.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-data-usage-restrictions
  dateCreated: 2024-03-06
  dateModified: 2024-12-12
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-data-laws
  tag: data-usage
  type: input
  phase: training-tuning
  descriptor: traditional
  concern: Data usage restrictions can impact the availability of the data required
    for training an AI model and can lead to poorly represented data.
- id: atlas-lack-of-system-transparency
  name: Lack of system transparency
  description: Insufficient documentation of the system that uses the model and the
    model’s purpose within the system in which it is used.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-lack-of-system-transparency
  dateCreated: 2024-09-24
  dateModified: 2024-11-18
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-governance
  tag: lack-of-system-transparency
  type: non-technical
  descriptor: traditional
  concern: A lack of documentation makes it difficult to understand how the model’s
    outcomes contribute to the system’s or application’s functionality.
- id: atlas-impact-on-cultural-diversity
  name: Impact on cultural diversity
  description: AI systems might overly represent certain cultures that result in a
    homogenization of culture and thoughts.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-impact-on-cultural-diversity
  dateCreated: 2024-03-06
  dateModified: 2024-09-24
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-societal-impact
  tag: impact-on-cultural-diversity
  type: non-technical
  descriptor: specific
  concern: Underrepresented groups' languages, viewpoints, and institutions might
    be suppressed by that means reducing diversity of thought and culture.
- id: atlas-impact-education-plagiarism
  name: 'Impact on education: plagiarism'
  description: Easy access to high-quality generative models might result in students
    that use AI models to plagiarize existing work intentionally or unintentionally.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-impact-education-plagiarism
  dateCreated: 2024-03-06
  dateModified: 2024-09-24
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-societal-impact
  tag: plagiarism
  type: non-technical
  descriptor: specific
  concern: AI models can be used to claim the authorship or originality of works that
    were created by other people in doing so by engaging in plagiarism. Claiming others’
    work as your own is both unethical and often illegal.
- id: atlas-improper-usage
  name: Improper usage
  description: Improper usage occurs when a model is used for a purpose that it was
    not originally designed for.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-improper-usage
  dateCreated: 2024-03-06
  dateModified: 2024-10-22
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-misuse
  tag: improper-usage
  type: output
  descriptor: amplified
  concern: Reusing a model without understanding its original data, design intent,
    and goals might result in unexpected and unwanted model behaviors.
- id: atlas-personal-information-in-data
  name: Personal information in data
  description: Inclusion or presence of personal identifiable information (PII) and
    sensitive personal information (SPI) in the data used for training or fine tuning
    the model might result in unwanted disclosure of that information.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-personal-information-in-data
  dateCreated: 2024-03-06
  dateModified: 2024-11-18
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-privacy
  tag: personal-information-in-data
  type: input
  phase: training-tuning
  descriptor: traditional
  concern: If not properly developed to protect sensitive data, the model might expose
    personal information in the generated output.  Additionally, personal, or sensitive
    data must be reviewed and handled in accordance with privacy laws and regulations.
- id: atlas-jailbreaking
  name: Jailbreaking
  description: A jailbreaking attack attempts to break through the guardrails that
    are established in the model to perform restricted actions.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-jailbreaking
  dateCreated: 2024-03-06
  dateModified: 2024-09-24
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-multi-category
  tag: jailbreaking
  type: input
  phase: inference
  descriptor: specific
  concern: Jailbreaking attacks can be used to alter model behavior and benefit the
    attacker. If not properly controlled, business entities can face fines, reputational
    harm, and other legal consequences.
- id: atlas-extraction-attack
  name: Extraction attack
  description: An extraction attack attempts to copy or steal an AI model by appropriately
    sampling the input space and observing outputs to build a surrogate model that
    behaves similarly.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-extraction-attack
  dateCreated: 2024-03-06
  dateModified: 2025-01-08
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-robustness
  tag: extraction-attack
  type: input
  phase: inference
  descriptor: amplified
  concern: With a successful extraction attack, the attacker can perform further adversarial
    attacks to gain valuable information such as sensitive personal information or
    intellectual property.
- id: atlas-impact-jobs
  name: Impact on Jobs
  description: Widespread adoption of foundation model-based AI systems might lead
    to people's job loss as their work is automated if they are not reskilled.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-impact-jobs
  dateCreated: 2024-03-06
  dateModified: 2024-12-12
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-societal-impact
  tag: job-loss
  type: non-technical
  descriptor: amplified
  concern: Job loss might lead to a loss of income and thus might negatively impact
    the society and human welfare. Reskilling might be challenging given the pace
    of the technology evolution.
- id: atlas-data-aquisition-restrictions
  name: Data acquisition restrictions
  description: Laws and other regulations might limit the collection of certain types
    of data for specific AI use cases.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-data-aquisition-restrictions
  dateCreated: 2024-03-06
  dateModified: 2024-11-18
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-data-laws
  tag: data-aquisition
  type: input
  phase: training-tuning
  descriptor: amplified
  concern: 'There are several ways of collecting data for building a foundation models:
    web scraping, web crawling, crowdsourcing, and curating public datasets. Data
    acquisition restrictions can also impact the availability of the data that is
    required for training an AI model and can lead to poorly represented data.'
- id: atlas-prompt-priming
  name: Prompt priming
  description: Because generative models tend to produce output like the input provided,
    the model can be prompted to reveal specific kinds of information. For example,
    adding personal information in the prompt increases its likelihood of generating
    similar kinds of personal information in its output. If personal data was included
    as part of the model’s training, there is a possibility it could be revealed.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-prompt-priming
  dateCreated: 2024-03-06
  dateModified: 2024-09-24
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-multi-category
  tag: prompt-priming
  type: input
  phase: inference
  descriptor: specific
  concern: Jailbreaking attacks can be used to alter model behavior and benefit the
    attacker. 
- id: atlas-reidentification
  name: Reidentification
  description: Even with the removal or personal identifiable information (PII) and
    sensitive personal information (SPI) from data, it might be possible to identify
    persons due to correlations to other features available in the data.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-reidentification
  dateCreated: 2024-03-06
  dateModified: 2024-10-22
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-privacy
  tag: reidentification
  type: input
  phase: training-tuning
  descriptor: traditional
  concern: Including irrelevant but highly correlated features to personal information
    for model training can increase the risk of reidentification.
- id: atlas-attribute-inference-attack
  name: Attribute inference attack
  description: An attribute inference attack repeatedly queries a model to detect
    whether certain sensitive features can be inferred about individuals who participated
    in training a model. These attacks occur when an adversary has some prior knowledge
    about the training data and uses that knowledge to infer the sensitive data.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-attribute-inference-attack
  dateCreated: 2024-03-06
  dateModified: 2024-10-22
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-privacy
  tag: attribute-inference-attack
  type: input
  phase: inference
  descriptor: amplified
  concern: With a successful attack, the attacker can gain valuable information such
    as sensitive personal information or intellectual property.
- id: atlas-poor-model-accuracy
  name: Poor model accuracy
  description: Poor model accuracy occurs when a model’s performance is insufficient
    to the task it was designed for. Low accuracy might occur if the model is not
    correctly engineered, or there are changes to the model’s expected inputs.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-poor-model-accuracy
  dateCreated: 2024-09-24
  dateModified: 2024-10-22
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-accuracy
  tag: poor-model-accuracy
  type: input
  phase: inference
  descriptor: amplified
  concern: Inadequate model performance can adversely affect end users and downstream
    systems that are relying on correct output. In cases where model output is consequential,
    this might result in societal, reputational, or financial harm.
- id: atlas-data-transfer-restrictions
  name: Data transfer restrictions
  description: Laws and other restrictions can limit or prohibit transferring data.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-data-transfer-restrictions
  dateCreated: 2024-03-06
  dateModified: 2024-12-12
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-data-laws
  tag: data-transfer
  type: input
  phase: training-tuning
  descriptor: traditional
  concern: Data transfer restrictions can also impact the availability of the data
    that is required for training an AI model and can lead to poorly represented data.
- id: atlas-generated-content-ownership-ip
  name: Generated content ownership and IP
  description: Legal uncertainty about the ownership and intellectual property rights
    of AI-generated content.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-generated-content-ownership-ip
  dateCreated: 2024-03-06
  dateModified: 2024-12-12
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-legal-compliance
  tag: generated-content-ownership
  type: non-technical
  descriptor: specific
  concern: Laws and regulations that relate to the ownership of AI-generated content
    are largely unsettled and can vary from country to country. Not being able to
    identify the owner of an AI-generated content might negatively impact AI-supported
    creative tasks.
- id: atlas-output-bias
  name: Output bias
  description: Generated content might unfairly represent certain groups or individuals.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-output-bias
  dateCreated: 2024-03-06
  dateModified: 2024-10-22
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-fairness
  tag: output-bias
  type: output
  descriptor: specific
  concern: Bias can harm users of the AI models and magnify existing discriminatory
    behaviors.
- id: atlas-dangerous-use
  name: Dangerous use
  description: Generative AI models might be used with the sole intention of harming
    people.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-dangerous-use
  dateCreated: 2024-03-06
  dateModified: 2024-10-22
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-misuse
  tag: dangerous-use
  type: output
  descriptor: specific
  concern: Large language models are often trained on vast amounts of publicly-available
    information that may include information on harming others. A model that has this
    potential must be carefully evaluated for such content and properly governed.
- id: atlas-unexplainable-output
  name: Unexplainable output
  description: Explanations for model output decisions might be difficult, imprecise,
    or not possible to obtain.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-unexplainable-output
  dateCreated: 2024-03-06
  dateModified: 2024-10-22
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-explainability
  tag: unexplainable-output
  type: output
  descriptor: amplified
  concern: Foundation models are based on complex deep learning architectures, making
    explanations for their outputs difficult. Inaccessible training data could limit
    the types of explanations a model can provide. Without clear explanations for
    model output, it is difficult for users, model validators, and auditors to understand
    and trust the model. Wrong explanations might lead to over-trust.
- id: atlas-human-exploitation
  name: Human exploitation
  description: When workers who train AI models such as ghost workers are not provided
    with adequate working conditions, fair compensation, and good health care benefits
    that also include mental health.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-human-exploitation
  dateCreated: 2024-03-06
  dateModified: 2024-09-24
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-societal-impact
  tag: human-exploitation
  type: non-technical
  descriptor: amplified
  concern: 'Foundation models still depend on human labor to source, manage, and program
    the data that is used to train the model. Human exploitation for these activities
    might negatively impact the society and human welfare. '
- id: atlas-improper-data-curation
  name: Improper data curation
  description: Improper collection and preparation of training or tuning data includes
    data label errors and by using data with conflicting information or misinformation.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-improper-data-curation
  dateCreated: 2024-03-06
  dateModified: 2024-10-22
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-value-alignment
  tag: data-curation
  type: input
  phase: training-tuning
  descriptor: amplified
  concern: 'Improper data curation can adversely affect how a model is trained, resulting
    in a model that does not behave in accordance with the intended values. Correcting
    problems after the model is trained and deployed might be insufficient for guaranteeing
    proper behavior. '
- id: atlas-revealing-confidential-information
  name: Revealing confidential information
  description: When confidential information is used in training data, fine-tuning
    data, or as part of the prompt, models might reveal that data in the generated
    output. Revealing confidential information is a type of data leakage.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-revealing-confidential-information
  dateCreated: 2024-03-06
  dateModified: 2024-11-18
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-intellectual-property
  tag: revealing-confidential-information
  type: output
  descriptor: amplified
  concern: If not properly developed to secure confidential data, the model might
    reveal confidential information or IP in the generated output and reveal information
    that was meant to be secret.
- id: atlas-spreading-disinformation
  name: Spreading disinformation
  description: Generative AI models might be used to intentionally create misleading
    or false information to deceive or influence a targeted audience.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-spreading-disinformation
  dateCreated: 2024-03-06
  dateModified: 2024-10-22
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-misuse
  tag: spreading-disinformation
  type: output
  descriptor: specific
  concern: Spreading disinformation might affect human’s ability to make informed
    decisions. A model that has this potential must be properly governed.
- id: atlas-unrepresentative-risk-testing
  name: Unrepresentative risk testing
  description: Testing is unrepresentative when the test inputs are mismatched with
    the inputs that are expected during deployment.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-unrepresentative-risk-testing
  dateCreated: 2024-09-24
  dateModified: 2024-09-24
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-governance
  tag: unrepresentative-risk-testing
  type: non-technical
  descriptor: amplified
  concern: If the model is evaluated in a use, context, or setting that is not the
    same as the one expected for deployment, the evaluations might not accurately
    reflect the risks of the model.
- id: atlas-data-bias
  name: Data bias
  description: Historical and societal biases that are present in the data are used
    to train and fine-tune the model.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-data-bias
  dateCreated: 2024-03-06
  dateModified: 2024-10-22
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-fairness
  tag: data-bias
  type: input
  phase: training-tuning
  descriptor: amplified
  concern: Training an AI system on data with bias, such as historical or societal
    bias, can lead to biased or skewed outputs that can unfairly represent or otherwise
    discriminate against certain groups or individuals.
- id: atlas-uncertain-data-provenance
  name: Uncertain data provenance
  description: Data provenance refers to tracing history of data, which includes its
    ownership, origin, and transformations. Without standardized and established methods
    for verifying where the data came from, there are no guarantees that the data
    is the same as the original source and has the correct usage terms.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-uncertain-data-provenance
  dateCreated: 2024-03-06
  dateModified: 2024-10-22
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-transparency
  tag: data-provenance
  type: input
  phase: training-tuning
  descriptor: amplified
  concern: Not all data sources are trustworthy. Data might be unethically collected,
    manipulated, or falsified. Verifying that data provenance is challenging due to
    factors such as data volume, data complexity, data source varieties, and poor
    data management. Using such data can result in undesirable behaviors in the model.
- id: atlas-data-usage-rights-restrictions
  name: Data usage rights restrictions
  description: Terms of service, license compliance, or other IP issues may restrict
    the ability to use certain data for building models.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-data-usage-rights-restrictions
  dateCreated: 2024-03-06
  dateModified: 2024-11-18
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-intellectual-property
  tag: data-usage-rights
  type: input
  phase: training-tuning
  descriptor: amplified
  concern: Laws and regulations concerning the use of data to train AI are unsettled
    and can vary from country to country, which creates challenges in the development
    of models.
- id: atlas-harmful-code-generation
  name: Harmful code generation
  description: Models might generate code that causes harm or unintentionally affects
    other systems.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-harmful-code-generation
  dateCreated: 2024-03-06
  dateModified: 2024-10-22
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-harmful-code-generation
  tag: harmful-code-generation
  type: output
  descriptor: specific
  concern: The execution of harmful code might open vulnerabilities in IT systems.
- id: atlas-data-contamination
  name: Data contamination
  description: Data contamination occurs when incorrect data is used for training.
    For example, data that is not aligned with model’s purpose or data that is already
    set aside for other development tasks such as testing and evaluation.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-data-contamination
  dateCreated: 2024-09-24
  dateModified: 2024-10-22
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-accuracy
  tag: data-contamination
  type: input
  phase: training-tuning
  descriptor: amplified
  concern: Data that differs from the intended training data might skew model accuracy
    and affect model outcomes.
- id: atlas-incomplete-usage-definition
  name: Incomplete usage definition
  description: Since foundation models can be used for many purposes, a model’s intended
    use is important for defining the relevant risks of that model. As the use changes,
    the relevant risks might correspondingly change.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-incomplete-usage-definition
  dateCreated: 2024-09-24
  dateModified: 2024-09-25
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-governance
  tag: incomplete-usage-definition
  type: non-technical
  descriptor: specific
  concern: It might be difficult to accurately determine and mitigate the relevant
    risks for a model when its intended use is insufficiently specified. Such as how
    a model is going to be used, where it is going to be used and what it is going
    to be used for.
- id: atlas-lack-of-data-transparency
  name: Lack of data transparency
  description: Lack of data transparency is due to insufficient documentation of training
    or tuning dataset details. 
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-lack-of-data-transparency
  dateCreated: 2024-03-06
  dateModified: 2024-11-18
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-governance
  tag: lack-of-data-transparency
  type: non-technical
  descriptor: amplified
  concern: Transparency is important for legal compliance and AI ethics. Information
    on the collection and preparation of training data, including how it was labeled
    and by who are necessary to understand model behavior and suitability. Details
    about how the data risks were determined, measured, and mitigated are important
    for evaluating both data and model trustworthiness. Missing details about the
    data might make it more difficult to evaluate representational harms, data ownership,
    provenance, and other data-oriented risks. The lack of standardized requirements
    might limit disclosure as organizations protect trade secrets and try to limit
    others from copying their models.
- id: atlas-copyright-infringement
  name: Copyright infringement
  description: A model might generate content that is similar or identical to existing
    work protected by copyright or covered by open-source license agreement.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-copyright-infringement
  dateCreated: 2024-03-06
  dateModified: 2024-09-24
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-intellectual-property
  tag: copyright-infringement
  type: output
  descriptor: specific
  concern: Laws and regulations concerning the use of content that looks the same
    or closely similar to other copyrighted data are largely unsettled and can vary
    from country to country, providing challenges in determining and implementing
    compliance.
- id: atlas-impact-on-affected-communities
  name: Impact on affected communities
  description: It is important to include the perspectives or concerns of communities
    that are affected by model outcomes when designing and building models. Failing
    to include these perspectives makes it difficult to understand the relevant context
    for the model and to engender trust within these communities.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-impact-on-affected-communities
  dateCreated: 2024-09-24
  dateModified: 2024-09-24
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-societal-impact
  tag: impact-on-affected-communities
  type: non-technical
  descriptor: traditional
  concern: Failing to engage with communities that are affected by a model’s outcomes
    might result in harms to those communities and societal backlash.
- id: atlas-spreading-toxicity
  name: Spreading toxicity
  description: Generative AI models might be used intentionally to generate hateful,
    abusive, and profane (HAP) or obscene content.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-spreading-toxicity
  dateCreated: 2024-03-06
  dateModified: 2024-10-22
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-misuse
  tag: spreading-toxicity
  type: output
  descriptor: specific
  concern: Toxic content might negatively affect the well-being of its recipients.
    A model that has this potential must be properly governed.
- id: atlas-improper-retraining
  name: Improper retraining
  description: Using undesirable output (for example, inaccurate, inappropriate, and
    user content) for retraining purposes can result in unexpected model behavior.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-improper-retraining
  dateCreated: 2024-03-06
  dateModified: 2024-09-24
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-value-alignment
  tag: downstream-retraining
  type: input
  phase: training-tuning
  descriptor: amplified
  concern: Repurposing generated output for retraining a model without implementing
    proper human vetting increases the chances of undesirable outputs to be incorporated
    into the training or tuning data of the model. In turn, this model can generate
    even more undesirable output.
- id: atlas-inaccessible-training-data
  name: Inaccessible training data
  description: Without access to the training data, the types of explanations a model
    can provide are limited and more likely to be incorrect.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-inaccessible-training-data
  dateCreated: 2024-03-06
  dateModified: 2024-05-03
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-explainability
  tag: inaccessible-training-data
  type: output
  descriptor: amplified
  concern: Low quality explanations without source data make it difficult for users,
    model validators, and auditors to understand and trust the model.
- id: atlas-impact-education-bypassing-learning
  name: 'Impact on education: bypassing learning'
  description: Easy access to high-quality generative models might result in students
    that use AI models to bypass the learning process.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-impact-education-bypassing-learning
  dateCreated: 2024-03-06
  dateModified: 2024-09-24
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-societal-impact
  tag: bypassing-learning
  type: non-technical
  descriptor: specific
  concern: AI models are quick to find solutions or solve complex problems. These
    systems can be misused by students to bypass the learning process. The ease of
    access to these models results in students having a superficial understanding
    of concepts and hampers further education that might rely on understanding those
    concepts.
- id: atlas-untraceable-attribution
  name: Untraceable attribution
  description: The content of the training data used for generating the model’s output
    is not accessible.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-untraceable-attribution
  dateCreated: 2024-03-06
  dateModified: 2024-09-24
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-explainability
  tag: untraceable-attribution
  type: output
  descriptor: amplified
  concern: Without the ability to access training data content, the possibility of
    using source attribution techniques can be severely limited or impossible. This
    makes it difficult for users, model validators, and auditors to understand and
    trust the model.
- id: atlas-evasion-attack
  name: Evasion attack
  description: Evasion attacks attempt to make a model output incorrect results by
    slightly perturbing the input data that is sent to the trained model.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-evasion-attack
  dateCreated: 2024-03-06
  dateModified: 2024-09-24
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-robustness
  tag: evasion-attack
  type: input
  phase: inference
  descriptor: amplified
  concern: Evasion attacks alter model behavior, usually to benefit the attacker.
- id: atlas-impact-on-the-environment
  name: Impact on the environment
  description: AI, and large generative models in particular, might produce increased
    carbon emissions and increase water usage for their training and operation.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-impact-on-the-environment
  dateCreated: 2024-03-06
  dateModified: 2024-09-24
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-societal-impact
  tag: impact-on-the-environment
  type: non-technical
  descriptor: amplified
  concern: Training and operating large AI models, building data centers, and manufacturing
    specialized hardware for AI can consume large amounts of water and energy, which
    contributes to carbon emissions. Additionally, water resources that are used for
    cooling AI data center servers can no longer be allocated for other necessary
    uses. If not managed, these could exacerbate climate change. 
- id: atlas-incorrect-risk-testing
  name: Incorrect risk testing
  description: A metric selected to measure or track a risk is incorrectly selected,
    incompletely measuring the risk, or measuring the wrong risk for the given context.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-incorrect-risk-testing
  dateCreated: 2024-09-24
  dateModified: 2024-09-24
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-governance
  tag: incorrect-risk-testing
  type: non-technical
  descriptor: amplified
  concern: If the metrics do not measure the risk as intended, then the understanding
    of that risk will be incorrect and mitigations might not be applied. If the model’s
    output is consequential, this might result in societal, reputational, or financial
    harm.
- id: atlas-over-or-under-reliance
  name: Over- or Under-reliance
  description: In AI-assisted decision-making tasks, reliance measures how much a
    person trusts (and potentially acts on) a model’s output. Over-reliance occurs
    when a person puts too much trust in a model, accepting a model’s output when
    the model’s output is likely incorrect. Under-reliance is the opposite, where
    the person doesn’t trust the model but should.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-over-or-under-reliance
  dateCreated: 2024-03-06
  dateModified: 2024-10-22
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-value-alignment
  tag: over-or-under-reliance
  type: output
  descriptor: amplified
  concern: In tasks where humans make choices based on AI-based suggestions, over/under
    reliance can lead to poor decision making because of the misplaced trust in the
    AI system, with negative consequences that increase with the importance of the
    decision.
- id: atlas-membership-inference-attack
  name: Membership inference attack
  description: 'A membership inference attack repeatedly queries a model to determine
    whether a given input was part of the model’s training. More specifically, given
    a trained model and a data sample, an attacker samples the input space, observing
    outputs to deduce whether that sample was part of the model''s training. '
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-membership-inference-attack
  dateCreated: 2024-03-06
  dateModified: 2024-10-22
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-privacy
  tag: membership-inference-attack
  type: input
  phase: inference
  descriptor: amplified
  concern: Identifying whether a data sample was used for training data can reveal
    what data was used to train a model. Possibly giving competitors insight into
    how a model was trained and the opportunity to replicate the model or tamper with
    it. Models that include publicly-available data are at higher risk of such attacks.
- id: atlas-confidential-data-in-prompt
  name: Confidential data in prompt
  description: Confidential information might be included as a part of the prompt
    that is sent to the model.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=atlas-confidential-data-in-prompt
  dateCreated: 2024-03-06
  dateModified: 2024-09-24
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-intellectual-property
  tag: confidential-data-in-prompt
  type: input
  phase: inference
  descriptor: specific
  concern: If not properly developed to secure confidential data, the model might
    reveal confidential information or IP in the generated output. Additionally, end
    users' confidential information might be unintentionally collected and stored.
