organizations:
- id: codeparrot
  name: CodeParrot & Friends
  description: This organization is dedicated to language models for code generation.
    In particular CodeParrot is a GPT-2 model trained to generate Python code.
  url: https://huggingface.co/codeparrot
- id: bigcode
  name: BigCode
  description: BigCode is an open scientific collaboration working on the responsible
    development and use of large language models for code (Code LLMs), empowering
    the machine learning and open source communities through open governance.
  url: https://www.bigcode-project.org/
- id: ibm
  name: International Business Machines Corporation
  description: International Business Machines Corporation (using the trademark IBM),
    is an American multinational technology company headquartered in Armonk, New York
    and present in over 175 countries.
  url: https://www.ibm.com
licenses:
- id: license-apache-2.0
  name: Apache 2.0
  description: The 2.0 version of the Apache License, approved by the ASF in 2004,
    helps to achieve the goal of providing reliable and long-lived software products
    through collaborative, open-source software development.
  url: https://www.apache.org/licenses/LICENSE-2.0.html
  dateCreated: 2004-02-08
  version: '2.0'
- id: cc-by-4.0
  name: Attribution 4.0 International
  description: This license enables reusers to distribute, remix, adapt, and build
    upon the material in any medium or format, so long as attribution is given to
    the creator. The license allows for commercial use.
  url: https://creativecommons.org/licenses/by/4.0/
  dateCreated: 2013-11-25
modalities:
- id: modality-text
  name: text
  description: A modality supporting text as input or output of an LLM.
- id: modality-image
  name: image
  description: A modality supporting images as input or output of an LLM.
aitasks:
- id: question-answering
  name: Question Answering
  description: Question Answering models can retrieve the answer to a question from
    a given text, which is useful for searching for an answer in a document. Some
    question answering models can generate answers without context!
  url: https://huggingface.co/tasks/question-answering
- id: summarization
  name: Summarization
  description: Summarization is the task of producing a shorter version of a document
    while preserving its important information. Some models can extract text from
    the original input, while other models can generate entirely new text.
  url: https://huggingface.co/tasks/summarization
- id: text-classification
  name: Text Classification
  description: Text Classification is the task of assigning a label or class to a
    given text. Some use cases are sentiment analysis, natural language inference,
    and assessing grammatical correctness.
  url: https://huggingface.co/tasks/text-classification
- id: text-generation
  name: Text Generation
  description: Generating text is the task of generating new text given another text.
    These models can, for example, fill in incomplete text or paraphrase.
  url: https://huggingface.co/tasks/text-generation
- id: translation
  name: Translation
  description: Translation converts a sequence of text from one language to another.
    It is one of several tasks you can formulate as a sequence-to-sequence problem,
    a powerful framework for returning some output from an input, like translation
    or summarization. Translation systems are commonly used for translation between
    different language texts, but it can also be used for speech or some combination
    in between like text-to-speech or speech-to-text.
  url: https://huggingface.co/docs/transformers/tasks/translation
- id: code-generation
  name: Code Generation
  description: Code generation is the task of generating code given some text describing
    the purpose.
- id: code-explanation
  name: Code Explanation
  description: Code explanation is the task of summarizing the purpose of a given
    piece of code in natural language.
- id: code-editing
  name: Code Editing
  description: Code editing is the task of changing a given piece of code to reach
    certain golas like e.g. better readability or improved performance.
documents:
- id: 10a99803d8afd656
  name: 'Foundation models: Opportunities, risks and mitigations'
  description: 'In this document we: Explore the benefits of foundation models, including
    their capability to perform challenging tasks, potential to speed up the adoption
    of AI, ability to increase productivity and the cost benefits they provide. Discuss
    the three categories of risk, including risks known from earlier forms of AI,
    known risks amplified by foundation models and emerging risks intrinsic to the
    generative capabilities of foundation models. Cover the principles, pillars and
    governance that form the foundation of IBM’s AI ethics initiatives and suggest
    guardrails for risk mitigation.'
  url: https://www.ibm.com/downloads/documents/us-en/10a99803d8afd656
- id: NIST.AI.600-1
  name: 'Artificial Intelligence Risk Management Framework: Generative Artificial
    Intelligence Profile'
  description: This document is a cross-sectoral profile of and companion resource
    for the AI Risk Management Framework (AI RMF 1.0) for Generative AI, pursuant
    to President Biden’s Executive Order (EO) 14110 on Safe, Secure, and Trustworthy
    Artificial Intelligence.
  url: https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf
  dateCreated: 2024-07-25
- id: arxiv.org/2408.12622
  name: 'The AI Risk Repository: A Comprehensive Meta-Review, Database, and Taxonomy
    of Risks From Artificial Intelligence'
  description: 'The risks posed by Artificial Intelligence (AI) are of considerable
    concern to academics, auditors, policymakers, AI companies, and the public. However,
    a lack of shared understanding of AI risks can impede our ability to comprehensively
    discuss, research, and react to them. This paper addresses this gap by creating
    an AI Risk Repository to serve as a common frame of reference. This comprises
    a living database of 777 risks extracted from 43 taxonomies, which can be filtered
    based on two overarching taxonomies and easily accessed, modified, and updated
    via our website and online spreadsheets. We construct our Repository with a systematic
    review of taxonomies and other structured classifications of AI risk followed
    by an expert consultation. We develop our taxonomies of AI risk using a best-fit
    framework synthesis. Our high-level Causal Taxonomy of AI Risks classifies each
    risk by its causal factors (1) Entity: Human, AI; (2) Intentionality: Intentional,
    Unintentional; and (3) Timing: Pre-deployment; Post-deployment. Our mid-level
    Domain Taxonomy of AI Risks classifies risks into seven AI risk domains: (1) Discrimination
    & toxicity, (2) Privacy & security, (3) Misinformation, (4) Malicious actors &
    misuse, (5) Human-computer interaction, (6) Socioeconomic & environmental, and
    (7) AI system safety, failures, & limitations. These are further divided into
    23 subdomains. The AI Risk Repository is, to our knowledge, the first attempt
    to rigorously curate, analyze, and extract AI risk frameworks into a publicly
    accessible, comprehensive, extensible, and categorized risk database. This creates
    a foundation for a more coordinated, coherent, and complete approach to defining,
    auditing, and managing the risks posed by AI systems.'
  url: https://arxiv.org/abs/2408.12622
  dateCreated: 2024-08-14
  dateModified: 2024-08-14
- id: arxiv.org/2310.12941
  name: The Foundation Model Transparency Index
  description: To assess the transparency of the foundation model ecosystem and help
    improve transparency over time, we introduce the Foundation Model Transparency
    Index. The Foundation Model Transparency Index specifies 100 fine-grained indicators
    that comprehensively codify transparency for foundation models, spanning the upstream
    resources used to build a foundation model (e.g data, labor, compute), details
    about the model itself (e.g. size, capabilities, risks), and the downstream use
    (e.g. distribution channels, usage policies, affected geographies). We score 10
    major foundation model developers (e.g. OpenAI, Google, Meta) against the 100
    indicators to assess their transparency. To facilitate and standardize assessment,
    we score developers in relation to their practices for their flagship foundation
    model (e.g. GPT-4 for OpenAI, PaLM 2 for Google, Llama 2 for Meta).
  url: https://arxiv.org/abs/2310.12941
  dateCreated: 2023-10-19
  dateModified: 2023-10-19
- id: arxiv.org/2109.07958
  name: 'TruthfulQA: Measuring How Models Mimic Human Falsehoods'
  description: TruthfulQA is a benchmark to measure whether a language model is truthful
    in generating answers to questions. The benchmark comprises 817 questions that
    span 38 categories, including health, law, finance and politics. Questions are
    crafted so that some humans would answer falsely due to a false belief or misconception.
    To perform well, models must avoid generating false answers learned from imitating
    human texts.
  url: https://arxiv.org/abs/2109.07958
  dateCreated: 2021-09-08
  dateModified: 2022-05-08
- id: arxiv.org/2310.06786
  name: 'OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text'
  description: There is growing evidence that pretraining on high quality, carefully
    thought-out tokens such as code or mathematics plays an important role in improving
    the reasoning abilities of large language models. For example, Minerva, a PaLM
    model finetuned on billions of tokens of mathematical documents from arXiv and
    the web, reported dramatically improved performance on problems that require quantitative
    reasoning. However, because all known open source web datasets employ preprocessing
    that does not faithfully preserve mathematical notation, the benefits of large
    scale training on quantitive web documents are unavailable to the research community.
    We introduce OpenWebMath, an open dataset inspired by these works containing 14.7B
    tokens of mathematical webpages from Common Crawl. We describe in detail our method
    for extracting text and LaTeX content and removing boilerplate from HTML documents,
    as well as our methods for quality filtering and deduplication. Additionally,
    we run small-scale experiments by training 1.4B parameter language models on OpenWebMath,
    showing that models trained on 14.7B tokens of our dataset surpass the performance
    of models trained on over 20x the amount of general language data. We hope that
    our dataset, openly released on the Hugging Face Hub, will help spur advances
    in the reasoning abilities of large language models.
  url: https://arxiv.org/abs/2310.06786
  dateCreated: 2023-10-10
- id: granite-3.0-paper
  name: Granite 3.0 Language Models
  description: This report presents Granite 3.0, a new set of lightweight, state-of-the-art,
    open foundation models ranging in scale from 400 million to 8 billion active parameters.
  url: https://github.com/ibm-granite/granite-3.0-language-models/blob/main/paper.pdf
  dateCreated: 2024-10-21
- id: '2412.07724'
  name: Granite Guardian
  description: We introduce the Granite Guardian models, a suite of safeguards designed
    to provide risk detection for prompts and responses, enabling safe and responsible
    use in combination with any large language model (LLM). These models offer comprehensive
    coverage across multiple risk dimensions, including social bias, profanity, violence,
    sexual content, unethical behavior, jailbreaking, and hallucination-related risks
    such as context relevance, groundedness, and answer relevance for retrieval-augmented
    generation (RAG). Trained on a unique dataset combining human annotations from
    diverse sources and synthetic data, Granite Guardian models address risks typically
    overlooked by traditional risk detection models, such as jailbreaks and RAG-specific
    issues. With AUC scores of 0.871 and 0.854 on harmful content and RAG-hallucination-related
    benchmarks respectively, Granite Guardian is the most generalizable and competitive
    model available in the space. Released as open-source, Granite Guardian aims to
    promote responsible AI development across the community.
  url: https://arxiv.org/abs/2412.07724
datasets:
- id: truthfulqa/truthful_qa
  name: truthful_qa
  description: TruthfulQA is a benchmark to measure whether a language model is truthful
    in generating answers to questions. The benchmark comprises 817 questions that
    span 38 categories, including health, law, finance and politics. Questions are
    crafted so that some humans would answer falsely due to a false belief or misconception.
    To perform well, models must avoid generating false answers learned from imitating
    human texts.
  url: https://huggingface.co/datasets/truthfulqa/truthful_qa
  hasLicense: license-apache-2.0
- id: github-code-clean
  name: Github-code-clean
  description: 'This is a cleaner version of Github-code dataset, we add the following
    filters: Average line length < 100, Alpha numeric characters fraction > 0.25,
    Remove auto-generated files (keyword search). 3.39M files are removed making up
    2.94% of the dataset.'
  url: https://huggingface.co/datasets/codeparrot/github-code-clean
  hasLicense: license-apache-2.0
  provider: codeparrot
- id: starcoder
  name: Star Coder Training Dataset
  description: This is the dataset used for training StarCoder and StarCoderBase.
    It contains 783GB of code in 86 programming languages, and includes 54GB GitHub
    Issues + 13GB Jupyter notebooks in scripts and text-code pairs, and 32GB of GitHub
    commits, which is approximately 250 Billion tokens.
  url: https://huggingface.co/datasets/bigcode/starcoderdata
  dateCreated: 2023-03-30
  dateModified: 2023-05-16
  provider: bigcode
- id: open-web-math
  name: OpenWebMath Dataset
  description: OpenWebMath is a dataset containing the majority of the high-quality,
    mathematical text from the internet. It is filtered and extracted from over 200B
    HTML files on Common Crawl down to a set of 6.3 million documents containing a
    total of 14.7B tokens. OpenWebMath is intended for use in pretraining and finetuning
    large language models.
  url: https://huggingface.co/datasets/open-web-math/open-web-math
  dateCreated: 2023-09-06
  hasDocumentation:
  - arxiv.org/2310.06786
taxonomies:
- id: ibm-risk-atlas
  name: IBM AI Risk Atlas
  description: Explore this atlas to understand some of the risks of working with
    generative AI, foundation models, and machine learning models.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=ai-risk-atlas
  dateCreated: 2024-03-06
  dateModified: 2025-02-11
  hasDocumentation:
  - 10a99803d8afd656
- id: nist-ai-rmf
  name: NIST AI Risk Management Framework (AI RMF)
  description: In collaboration with the private and public sectors, NIST has developed
    a framework to better manage risks to individuals, organizations, and society
    associated with artificial intelligence (AI). The NIST AI Risk Management Framework
    (AI RMF) is intended for voluntary use and to improve the ability to incorporate
    trustworthiness considerations into the design, development, use, and evaluation
    of AI products, services, and systems.
  url: https://www.nist.gov/itl/ai-risk-management-framework
  dateCreated: 2024-07-26
  hasDocumentation:
  - NIST.AI.600-1
- id: mit-ai-risk-repository
  name: The AI Risk Repository
  description: A comprehensive living database of over 700 AI risks categorized by
    their cause and risk domain.
  url: https://airisk.mit.edu/
  dateCreated: 2024-08-16
  version: '1'
  hasDocumentation:
  - arxiv.org/2408.12622
- id: ibm-granite-guardian
  name: IBM Granite Guardian
  description: Understand risk dimensions covered by Granite Guardian.
  url: https://arxiv.org/abs/2412.07724
  dateCreated: 2024-12-10
  dateModified: 2024-12-16
  hasDocumentation:
  - '2412.07724'
- id: owasp-llm-2.0
  name: OWASP Top 10 for Large Language Model Applications
  description: The OWASP Top 10 for Large Language Model Applications project aims
    to educate developers, designers, architects, managers, and organizations about
    the potential security risks when deploying and managing Large Language Models
    (LLMs). The project provides a list of the top 10 most critical vulnerabilities
    often seen in LLM applications, highlighting their potential impact, ease of exploitation,
    and prevalence in real-world applications.
  url: https://owasp.org/www-project-top-10-for-large-language-model-applications/
  dateCreated: 2024-11-18
  dateModified: 2024-11-18
  version: '2.0'
riskgroups:
- id: ibm-risk-atlas-accuracy
  name: Accuracy
  isDefinedByTaxonomy: ibm-risk-atlas
- id: ibm-risk-atlas-data-laws
  name: Data laws
  isDefinedByTaxonomy: ibm-risk-atlas
- id: ibm-risk-atlas-explainability
  name: Explainability
  isDefinedByTaxonomy: ibm-risk-atlas
- id: ibm-risk-atlas-fairness
  name: Fairness
  isDefinedByTaxonomy: ibm-risk-atlas
- id: ibm-risk-atlas-governance
  name: Governance
  isDefinedByTaxonomy: ibm-risk-atlas
- id: ibm-risk-atlas-intellectual-property
  name: Intellectual property
  isDefinedByTaxonomy: ibm-risk-atlas
- id: ibm-risk-atlas-legal-compliance
  name: Legal compliance
  isDefinedByTaxonomy: ibm-risk-atlas
- id: ibm-risk-atlas-misuse
  name: Misuse
  isDefinedByTaxonomy: ibm-risk-atlas
- id: ibm-risk-atlas-multi-category
  name: Multi-category
  isDefinedByTaxonomy: ibm-risk-atlas
- id: ibm-risk-atlas-privacy
  name: Privacy
  isDefinedByTaxonomy: ibm-risk-atlas
- id: ibm-risk-atlas-robustness
  name: Robustness
  isDefinedByTaxonomy: ibm-risk-atlas
- id: ibm-risk-atlas-societal-impact
  name: Societal impact
  isDefinedByTaxonomy: ibm-risk-atlas
- id: ibm-risk-atlas-transparency
  name: Transparency
  isDefinedByTaxonomy: ibm-risk-atlas
- id: ibm-risk-atlas-value-alignment
  name: Value alignment
  isDefinedByTaxonomy: ibm-risk-atlas
- id: mit-ai-risk-domain-1
  name: Discrimination & Toxicity
  isDefinedByTaxonomy: mit-ai-risk-repository
- id: mit-ai-risk-domain-2
  name: Privacy & Security
  isDefinedByTaxonomy: mit-ai-risk-repository
- id: mit-ai-risk-domain-3
  name: Misinformation
  isDefinedByTaxonomy: mit-ai-risk-repository
- id: mit-ai-risk-domain-4
  name: Malicious actors
  isDefinedByTaxonomy: mit-ai-risk-repository
- id: mit-ai-risk-domain-5
  name: Human- Computer Interaction
  isDefinedByTaxonomy: mit-ai-risk-repository
- id: mit-ai-risk-domain-6
  name: Socioeconomic & Environmental
  isDefinedByTaxonomy: mit-ai-risk-repository
- id: mit-ai-risk-domain-7
  name: AI system safety, failures, & limitations
  isDefinedByTaxonomy: mit-ai-risk-repository
- id: granite-guardian-harm-group
  name: Harm
  isDefinedByTaxonomy: ibm-granite-guardian
- id: granite-guardian-rag-safety-group
  name: RAG Safety
  isDefinedByTaxonomy: ibm-granite-guardian
- id: granite-guardian-agentic-safety-group
  name: Agentic Safety
  isDefinedByTaxonomy: ibm-granite-guardian
risks:
- id: atlas-non-disclosure
  name: Non-disclosure
  description: Content might not be clearly disclosed as AI generated.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/non-disclosure.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-misuse
  broadMatch:
  - nist-human-ai-configuration
  tag: non-disclosure
  type: output
  descriptor: specific
  concern: Users must be notified when they are interacting with an AI system. Not
    disclosing the AI-authored content can result in a lack of transparency.
- id: atlas-data-transparency
  name: Lack of training data transparency
  description: Without accurate documentation on how a model's data was collected,
    curated, and used to train a model, it might be harder to satisfactorily explain
    the behavior of the model with respect to the data.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/data-transparency.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-transparency
  broadMatch:
  - nist-information-integrity
  - nist-value-chain-and-component-integration
  tag: data-transparency
  type: training-data
  descriptor: amplified
  concern: A lack of data documentation limits the ability to evaluate risks associated
    with the data. Having access to the training data is not enough. Without recording
    how the data was cleaned, modified, or generated, the model behavior is more difficult
    to understand and to fix. Lack of data transparency also impacts model reuse as
    it is difficult to determine data representativeness for the new use without such
    documentation.
- id: atlas-model-usage-rights
  name: Model usage rights restrictions
  description: Terms of service, licenses, or other rules restrict the use of certain
    models.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/model-usage-rights.html
  dateCreated: 2024-09-24
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-legal-compliance
  broadMatch:
  - nist-data-privacy
  - nist-intellectual-property
  - nist-value-chain-and-component-integration
  tag: model-usage-rights
  type: non-technical
  descriptor: traditional
  concern: Laws and regulations that concern the use of AI are in place and vary from
    country to country. Additionally, the usage of models might be dictated by licensing
    terms or agreements.
- id: atlas-prompt-injection
  name: Prompt injection attack
  description: A prompt injection attack forces a generative model that takes a prompt
    as input to produce unexpected output by manipulating the structure, instructions,
    or information contained in its prompt.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/prompt-injection.html
  dateCreated: 2024-03-06
  dateModified: 2025-01-23
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-robustness
  exactMatch:
  - owasp-llm01-prompt-injection
  broadMatch:
  - nist-information-security
  tag: prompt-injection
  type: inference
  descriptor: specific
  concern: Injection attacks can be used to alter model behavior and benefit the attacker.
- id: atlas-incomplete-advice
  name: Incomplete advice
  description: When a model provides advice without having enough information, resulting
    in possible harm if the advice is followed.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/incomplete-advice.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-value-alignment
  broadMatch:
  - nist-information-integrity
  - nist-value-chain-and-component-integration
  tag: incomplete-advice
  type: output
  descriptor: specific
  concern: A person might act on incomplete advice or worry about a situation that
    is not applicable to them due to the overgeneralized nature of the content generated.
    For example, a model might provide incorrect medical, financial, and legal advice
    or recommendations that the end user might act on, resulting in harmful actions.
- id: atlas-lack-of-system-transparency
  name: Lack of system transparency
  description: Insufficient documentation of the system that uses the model and the
    model’s purpose within the system in which it is used.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/lack-of-system-transparency.html
  dateCreated: 2024-09-24
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-governance
  tag: lack-of-system-transparency
  type: non-technical
  descriptor: traditional
  concern: A lack of documentation makes it difficult to understand how the model’s
    outcomes contribute to the system’s or application’s functionality.
- id: atlas-data-usage
  name: Data usage restrictions
  description: Laws and other restrictions can limit or prohibit the use of some data
    for specific AI use cases.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/data-usage.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-data-laws
  tag: data-usage
  type: training-data
  descriptor: traditional
  concern: Data usage restrictions can impact the availability of the data required
    for training an AI model and can lead to poorly represented data.
- id: atlas-impact-on-cultural-diversity
  name: Impact on cultural diversity
  description: AI systems might overly represent certain cultures that result in a
    homogenization of culture and thoughts.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/impact-on-cultural-diversity.html
  dateCreated: 2024-03-06
  dateModified: 2025-01-23
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-societal-impact
  tag: impact-on-cultural-diversity
  type: non-technical
  descriptor: specific
  concern: Underrepresented groups' languages, viewpoints, and institutions might
    be suppressed by that means reducing diversity of thought and culture.
- id: atlas-plagiarism
  name: 'Impact on education: plagiarism'
  description: Easy access to high-quality generative models might result in students
    that use AI models to plagiarize existing work intentionally or unintentionally.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/plagiarism.html
  dateCreated: 2024-03-06
  dateModified: 2024-09-24
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-societal-impact
  tag: plagiarism
  type: non-technical
  descriptor: specific
  concern: AI models can be used to claim the authorship or originality of works that
    were created by other people in doing so by engaging in plagiarism. Claiming others’
    work as your own is both unethical and often illegal.
- id: atlas-personal-information-in-data
  name: Personal information in data
  description: Inclusion or presence of personal identifiable information (PII) and
    sensitive personal information (SPI) in the data used for training or fine tuning
    the model might result in unwanted disclosure of that information.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/personal-information-in-data.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-privacy
  broadMatch:
  - nist-data-privacy
  tag: personal-information-in-data
  type: training-data
  descriptor: traditional
  concern: If not properly developed to protect sensitive data, the model might expose
    personal information in the generated output.  Additionally, personal, or sensitive
    data must be reviewed and handled in accordance with privacy laws and regulations.
- id: atlas-improper-usage
  name: Improper usage
  description: Improper usage occurs when a model is used for a purpose that it was
    not originally designed for.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/improper-usage.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-misuse
  broadMatch:
  - nist-human-ai-configuration
  tag: improper-usage
  type: output
  descriptor: amplified
  concern: Reusing a model without understanding its original data, design intent,
    and goals might result in unexpected and unwanted model behaviors.
- id: atlas-extraction-attack
  name: Extraction attack
  description: An attribute inference attack is used to detect whether certain sensitive
    features can be inferred about individuals who participated in training a model.
    These attacks occur when an adversary has some prior knowledge about the training
    data and uses that knowledge to infer the sensitive data.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/extraction-attack.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-robustness
  broadMatch:
  - nist-information-security
  tag: extraction-attack
  type: inference
  descriptor: amplified
  concern: With a successful extraction attack, the attacker can perform further adversarial
    attacks to gain valuable information such as sensitive personal information or
    intellectual property.
- id: atlas-job-loss
  name: Impact on Jobs
  description: Widespread adoption of foundation model-based AI systems might lead
    to people's job loss as their work is automated if they are not reskilled.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/job-loss.html
  dateCreated: 2024-03-06
  dateModified: 2025-01-23
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-societal-impact
  tag: job-loss
  type: non-technical
  descriptor: amplified
  concern: Job loss might lead to a loss of income and thus might negatively impact
    the society and human welfare. Reskilling might be challenging given the pace
    of the technology evolution.
- id: atlas-jailbreaking
  name: Jailbreaking
  description: A jailbreaking attack attempts to break through the guardrails that
    are established in the model to perform restricted actions.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/jailbreaking.html
  dateCreated: 2024-03-06
  dateModified: 2025-01-23
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-multi-category
  broadMatch:
  - nist-information-integrity
  tag: jailbreaking
  type: inference
  descriptor: specific
  concern: Jailbreaking attacks can be used to alter model behavior and benefit the
    attacker. If not properly controlled, business entities can face fines, reputational
    harm, and other legal consequences.
- id: atlas-data-acquisition
  name: Data acquisition restrictions
  description: Laws and other regulations might limit the collection of certain types
    of data for specific AI use cases.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/data-acquisition.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-11
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-data-laws
  tag: data-acquisition
  type: training-data
  descriptor: amplified
  concern: '"There are several ways of collecting data for building a foundation models:
    web scraping, web crawling, crowdsourcing, and curating public datasets. Data
    acquisition restrictions can also impact the availability of the data that is
    required for training an AI model and can lead to poorly represented data."'
- id: atlas-data-bias
  name: Data bias
  description: 'Historical and societal biases that are present in the data are used
    to train and fine-tune the model.

    '
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/data-bias.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-fairness
  broadMatch:
  - nist-harmful-bias-or-homogenization
  tag: data-bias
  type: training-data
  descriptor: amplified
  concern: Training an AI system on data with bias, such as historical or societal
    bias, can lead to biased or skewed outputs that can unfairly represent or otherwise
    discriminate against certain groups or individuals.
- id: atlas-data-provenance
  name: Uncertain data provenance
  description: Data provenance refers to tracing history of data, which includes its
    ownership, origin, and transformations. Without standardized and established methods
    for verifying where the data came from, there are no guarantees that the data
    is the same as the original source and has the correct usage terms.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/data-provenance.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-transparency
  broadMatch:
  - nist-value-chain-and-component-integration
  tag: data-provenance
  type: training-data
  descriptor: amplified
  concern: Not all data sources are trustworthy. Data might be unethically collected,
    manipulated, or falsified. Verifying that data provenance is challenging due to
    factors such as data volume, data complexity, data source varieties, and poor
    data management. Using such data can result in undesirable behaviors in the model.
- id: atlas-unrepresentative-risk-testing
  name: Unrepresentative risk testing
  description: Testing is unrepresentative when the test inputs are mismatched with
    the inputs that are expected during deployment.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/unrepresentative-risk-testing.html
  dateCreated: 2024-09-24
  dateModified: 2025-01-09
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-governance
  broadMatch:
  - nist-value-chain-and-component-integration
  tag: unrepresentative-risk-testing
  type: non-technical
  descriptor: amplified
  concern: If the model is evaluated in a use, context, or setting that is not the
    same as the one expected for deployment, the evaluations might not accurately
    reflect the risks of the model.
- id: atlas-data-usage-rights
  name: Data usage rights restrictions
  description: Terms of service, license compliance, or other IP issues may restrict
    the ability to use certain data for building models.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/data-usage-rights.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-intellectual-property
  broadMatch:
  - nist-data-privacy
  - nist-value-chain-and-component-integration
  tag: data-usage-rights
  type: training-data
  descriptor: amplified
  concern: Laws and regulations concerning the use of data to train AI are unsettled
    and can vary from country to country, which creates challenges in the development
    of models.
- id: atlas-harmful-code-generation
  name: Harmful code generation
  description: Models might generate code that causes harm or unintentionally affects
    other systems.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/harmful-code-generation.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-value-alignment
  broadMatch:
  - nist-dangerous-violent-or-hateful-content
  - nist-information-security
  tag: harmful-code-generation
  type: output
  descriptor: specific
  concern: The execution of harmful code might open vulnerabilities in IT systems.
- id: atlas-data-contamination
  name: Data contamination
  description: Data contamination occurs when incorrect data is used for training.
    For example, data that is not aligned with model’s purpose or data that is already
    set aside for other development tasks such as testing and evaluation.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/data-contamination.html
  dateCreated: 2024-09-24
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-accuracy
  broadMatch:
  - nist-information-security
  - nist-value-chain-and-component-integration
  tag: data-contamination
  type: training-data
  descriptor: amplified
  concern: Data that differs from the intended training data might skew model accuracy
    and affect model outcomes.
- id: atlas-incomplete-usage-definition
  name: Incomplete usage definition
  description: Since foundation models can be used for many purposes, a model’s intended
    use is important for defining the relevant risks of that model. As the use changes,
    the relevant risks might correspondingly change.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/incomplete-usage-definition.html
  dateCreated: 2024-09-24
  dateModified: 2024-09-25
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-governance
  broadMatch:
  - nist-human-ai-configuration
  tag: incomplete-usage-definition
  type: non-technical
  descriptor: specific
  concern: It might be difficult to accurately determine and mitigate the relevant
    risks for a model when its intended use is insufficiently specified. Such as how
    a model is going to be used, where it is going to be used and what it is going
    to be used for.
- id: atlas-copyright-infringement
  name: Copyright infringement
  description: A model might generate content that is similar or identical to existing
    work protected by copyright or covered by open-source license agreement.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/copyright-infringement.html
  dateCreated: 2024-03-06
  dateModified: 2024-09-24
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-intellectual-property
  broadMatch:
  - nist-intellectual-property
  tag: copyright-infringement
  type: output
  descriptor: specific
  concern: Laws and regulations concerning the use of content that looks the same
    or closely similar to other copyrighted data are largely unsettled and can vary
    from country to country, providing challenges in determining and implementing
    compliance.
- id: atlas-lack-of-data-transparency
  name: Lack of data transparency
  description: Lack of data transparency is due to insufficient documentation of training
    or tuning dataset details. 
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/lack-of-data-transparency.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-governance
  tag: lack-of-data-transparency
  type: non-technical
  descriptor: amplified
  concern: Transparency is important for legal compliance and AI ethics. Information
    on the collection and preparation of training data, including how it was labeled
    and by who are necessary to understand model behavior and suitability. Details
    about how the data risks were determined, measured, and mitigated are important
    for evaluating both data and model trustworthiness. Missing details about the
    data might make it more difficult to evaluate representational harms, data ownership,
    provenance, and other data-oriented risks. The lack of standardized requirements
    might limit disclosure as organizations protect trade secrets and try to limit
    others from copying their models.
- id: atlas-impact-on-affected-communities
  name: Impact on affected communities
  description: It is important to include the perspectives or concerns of communities
    that are affected by model outcomes when designing and building models. Failing
    to include these perspectives makes it difficult to understand the relevant context
    for the model and to engender trust within these communities.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/impact-on-affected-communities.html
  dateCreated: 2024-09-24
  dateModified: 2024-09-24
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-societal-impact
  tag: impact-on-affected-communities
  type: non-technical
  descriptor: traditional
  concern: Failing to engage with communities that are affected by a model’s outcomes
    might result in harms to those communities and societal backlash.
- id: atlas-improper-retraining
  name: Improper retraining
  description: Using undesirable output (for example, inaccurate, inappropriate, and
    user content) for retraining purposes can result in unexpected model behavior.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/improper-retraining.html
  dateCreated: 2024-03-06
  dateModified: 2025-01-23
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-value-alignment
  broadMatch:
  - nist-value-chain-and-component-integration
  tag: improper-retraining
  type: training-data
  descriptor: amplified
  concern: Repurposing generated output for retraining a model without implementing
    proper human vetting increases the chances of undesirable outputs to be incorporated
    into the training or tuning data of the model. In turn, this model can generate
    even more undesirable output.
- id: atlas-spreading-toxicity
  name: Spreading toxicity
  description: Generative AI models might be used intentionally to generate hateful,
    abusive, and profane (HAP) or obscene content.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/spreading-toxicity.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-misuse
  broadMatch:
  - nist-harmful-bias-or-homogenization
  tag: spreading-toxicity
  type: output
  descriptor: specific
  concern: Toxic content might negatively affect the well-being of its recipients.
    A model that has this potential must be properly governed.
- id: atlas-inaccessible-training-data
  name: Inaccessible training data
  description: Without access to the training data, the types of explanations a model
    can provide are limited and more likely to be incorrect.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/inaccessible-training-data.html
  dateCreated: 2024-03-06
  dateModified: 2024-05-03
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-explainability
  broadMatch:
  - nist-value-chain-and-component-integration
  relatedMatch:
  - owasp-llm032025-supply-chain
  tag: inaccessible-training-data
  type: output
  descriptor: amplified
  concern: Low quality explanations without source data make it difficult for users,
    model validators, and auditors to understand and trust the model.
- id: atlas-bypassing-learning
  name: 'Impact on education: bypassing learning'
  description: Easy access to high-quality generative models might result in students
    that use AI models to bypass the learning process.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/bypassing-learning.html
  dateCreated: 2024-03-06
  dateModified: 2024-09-24
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-societal-impact
  tag: bypassing-learning
  type: non-technical
  descriptor: specific
  concern: AI models are quick to find solutions or solve complex problems. These
    systems can be misused by students to bypass the learning process. The ease of
    access to these models results in students having a superficial understanding
    of concepts and hampers further education that might rely on understanding those
    concepts.
- id: atlas-untraceable-attribution
  name: Untraceable attribution
  description: The content of the training data used for generating the model’s output
    is not accessible.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/untraceable-attribution.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-explainability
  broadMatch:
  - nist-information-integrity
  tag: untraceable-attribution
  type: output
  descriptor: amplified
  concern: Without the ability to access training data content, the possibility of
    using source attribution techniques can be severely limited or impossible. This
    makes it difficult for users, model validators, and auditors to understand and
    trust the model.
- id: atlas-evasion-attack
  name: Evasion attack
  description: Evasion attacks attempt to make a model output incorrect results by
    slightly perturbing the input data that is sent to the trained model.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/evasion-attack.html
  dateCreated: 2024-03-06
  dateModified: 2025-01-23
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-robustness
  broadMatch:
  - nist-information-security
  tag: evasion-attack
  type: inference
  descriptor: amplified
  concern: Evasion attacks alter model behavior, usually to benefit the attacker.
- id: atlas-impact-on-the-environment
  name: Impact on the environment
  description: AI, and large generative models in particular, might produce increased
    carbon emissions and increase water usage for their training and operation.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/impact-on-the-environment.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-societal-impact
  tag: impact-on-the-environment
  type: non-technical
  descriptor: amplified
  concern: Training and operating large AI models, building data centers, and manufacturing
    specialized hardware for AI can consume large amounts of water and energy, which
    contributes to carbon emissions. Additionally, water resources that are used for
    cooling AI data center servers can no longer be allocated for other necessary
    uses. If not managed, these could exacerbate climate change. 
- id: atlas-over-or-under-reliance
  name: Over- or under-reliance
  description: In AI-assisted decision-making tasks, reliance measures how much a
    person trusts (and potentially acts on) a model’s output. Over-reliance occurs
    when a person puts too much trust in a model, accepting a model’s output when
    the model’s output is likely incorrect. Under-reliance is the opposite, where
    the person doesn’t trust the model but should.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/over-or-under-reliance.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-value-alignment
  tag: over-or-under-reliance
  type: output
  descriptor: amplified
  concern: In tasks where humans make choices based on AI-based suggestions, over/under
    reliance can lead to poor decision making because of the misplaced trust in the
    AI system, with negative consequences that increase with the importance of the
    decision.
- id: atlas-incorrect-risk-testing
  name: Incorrect risk testing
  description: A metric selected to measure or track a risk is incorrectly selected,
    incompletely measuring the risk, or measuring the wrong risk for the given context.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/incorrect-risk-testing.html
  dateCreated: 2024-09-24
  dateModified: 2024-09-24
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-governance
  broadMatch:
  - nist-value-chain-and-component-integration
  tag: incorrect-risk-testing
  type: non-technical
  descriptor: amplified
  concern: If the metrics do not measure the risk as intended, then the understanding
    of that risk will be incorrect and mitigations might not be applied. If the model’s
    output is consequential, this might result in societal, reputational, or financial
    harm.
- id: atlas-membership-inference-attack
  name: Membership inference attack
  description: 'A membership inference attack repeatedly queries a model to determine
    whether a given input was part of the model’s training. More specifically, given
    a trained model and a data sample, an attacker samples the input space, observing
    outputs to deduce whether that sample was part of the model''s training. '
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/membership-inference-attack.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-privacy
  broadMatch:
  - nist-data-privacy
  tag: membership-inference-attack
  type: inference
  descriptor: amplified
  concern: Identifying whether a data sample was used for training data can reveal
    what data was used to train a model. Possibly giving competitors insight into
    how a model was trained and the opportunity to replicate the model or tamper with
    it. Models that include publicly-available data are at higher risk of such attacks.
- id: atlas-confidential-data-in-prompt
  name: Confidential data in prompt
  description: Confidential information might be included as a part of the prompt
    that is sent to the model.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/confidential-data-in-prompt.html
  dateCreated: 2024-03-06
  dateModified: 2025-01-23
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-intellectual-property
  broadMatch:
  - nist-intellectual-property
  tag: confidential-data-in-prompt
  type: inference
  descriptor: specific
  concern: If not properly developed to secure confidential data, the model might
    reveal confidential information or IP in the generated output. Additionally, end
    users' confidential information might be unintentionally collected and stored.
- id: atlas-data-privacy-rights
  name: Data privacy rights alignment
  description: Existing laws could include providing data subject rights such as opt-out,
    right to access, and right to be forgotten.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/data-privacy-rights.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-privacy
  broadMatch:
  - nist-data-privacy
  tag: data-privacy-rights
  type: training-data
  descriptor: amplified
  concern: Improper usage or a request for data removal could force organizations
    to retrain the model, which is expensive.
- id: atlas-ip-information-in-prompt
  name: IP information in prompt
  description: Copyrighted information or other intellectual property might be included
    as a part of the prompt that is sent to the model.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/ip-information-in-prompt.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-intellectual-property
  broadMatch:
  - nist-data-privacy
  tag: ip-information-in-prompt
  type: inference
  descriptor: specific
  concern: Inclusion of such data might result in it being disclosed in the model
    output. In addition to accidental disclosure, prompt data might be used for other
    purposes like model evaluation and retraining, and might appear in their output
    if not properly removed.
- id: atlas-prompt-leaking
  name: Prompt leaking
  description: A prompt leak attack attempts to extract a model's system prompt (also
    known as the system message).
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/prompt-leaking.html
  dateCreated: 2024-03-06
  dateModified: 2025-01-23
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-robustness
  broadMatch:
  - nist-information-security
  tag: prompt-leaking
  type: inference
  descriptor: specific
  concern: A successful attack copies the system prompt used in the model. Depending
    on the content of that prompt, the attacker might gain access to valuable information,
    such as sensitive personal information or intellectual property, and might be
    able to replicate some of the functionality of the model.
- id: atlas-hallucination
  name: Hallucination
  description: Hallucinations generate factually inaccurate or untruthful content
    with respect to the model’s training data or input. This is also sometimes referred
    to lack of faithfulness or lack of groundedness.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/hallucination.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-robustness
  exactMatch:
  - nist-confabulation
  broadMatch:
  - owasp-llm062025-excessive-agency
  tag: hallucination
  type: output
  descriptor: specific
  concern: Hallucinations can be misleading. These false outputs can mislead users
    and be incorporated into downstream artifacts, further spreading misinformation.
    False output can harm both owners and users of the AI models. In some uses, hallucinations
    can be particularly consequential.
- id: atlas-legal-accountability
  name: Legal accountability
  description: Determining who is responsible for an AI model is challenging without
    good documentation and governance processes.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/legal-accountability.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-legal-compliance
  broadMatch:
  - nist-data-privacy
  - nist-intellectual-property
  - nist-value-chain-and-component-integration
  tag: legal-accountability
  type: non-technical
  descriptor: amplified
  concern: If ownership for development of the model is uncertain, regulators and
    others might have concerns about the model. It would not be clear who would be
    liable and responsible for the problems with it or can answer questions about
    it. Users of models without clear ownership might find challenges with compliance
    with future AI regulation.
- id: atlas-prompt-priming
  name: Prompt priming
  description: Because generative models tend to produce output like the input provided,
    the model can be prompted to reveal specific kinds of information. For example,
    adding personal information in the prompt increases its likelihood of generating
    similar kinds of personal information in its output. If personal data was included
    as part of the model’s training, there is a possibility it could be revealed.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/prompt-priming.html
  dateCreated: 2024-03-06
  dateModified: 2025-01-23
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-multi-category
  broadMatch:
  - nist-information-security
  tag: prompt-priming
  type: inference
  descriptor: specific
  concern: Jailbreaking attacks can be used to alter model behavior and benefit the
    attacker. 
- id: atlas-reidentification
  name: Reidentification
  description: Even with the removal or personal identifiable information (PII) and
    sensitive personal information (SPI) from data, it might be possible to identify
    persons due to correlations to other features available in the data.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/reidentification.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-privacy
  broadMatch:
  - nist-data-privacy
  tag: reidentification
  type: training-data
  descriptor: traditional
  concern: Including irrelevant but highly correlated features to personal information
    for model training can increase the risk of reidentification.
- id: atlas-attribute-inference-attack
  name: Attribute inference attack
  description: An attribute inference attack repeatedly queries a model to detect
    whether certain sensitive features can be inferred about individuals who participated
    in training a model. These attacks occur when an adversary has some prior knowledge
    about the training data and uses that knowledge to infer the sensitive data.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/attribute-inference-attack.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-privacy
  broadMatch:
  - nist-data-privacy
  - nist-information-security
  tag: attribute-inference-attack
  type: inference
  descriptor: amplified
  concern: With a successful attack, the attacker can gain valuable information such
    as sensitive personal information or intellectual property.
- id: atlas-poor-model-accuracy
  name: Poor model accuracy
  description: Poor model accuracy occurs when a model’s performance is insufficient
    to the task it was designed for. Low accuracy might occur if the model is not
    correctly engineered, or there are changes to the model’s expected inputs.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/poor-model-accuracy.html
  dateCreated: 2024-09-24
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-accuracy
  broadMatch:
  - nist-human-ai-configuration
  - nist-information-integrity
  - nist-value-chain-and-component-integration
  tag: poor-model-accuracy
  type: inference
  descriptor: amplified
  concern: Inadequate model performance can adversely affect end users and downstream
    systems that are relying on correct output. In cases where model output is consequential,
    this might result in societal, reputational, or financial harm.
- id: atlas-data-transfer
  name: Data transfer restrictions
  description: Laws and other restrictions can limit or prohibit transferring data.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/data-transfer.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-data-laws
  broadMatch:
  - nist-value-chain-and-component-integration
  tag: data-transfer
  type: training-data
  descriptor: traditional
  concern: Data transfer restrictions can also impact the availability of the data
    that is required for training an AI model and can lead to poorly represented data.
- id: atlas-generated-content-ownership
  name: Generated content ownership and IP
  description: Legal uncertainty about the ownership and intellectual property rights
    of AI-generated content.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/generated-content-ownership.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-legal-compliance
  broadMatch:
  - nist-intellectual-property
  tag: generated-content-ownership
  type: non-technical
  descriptor: specific
  concern: Laws and regulations that relate to the ownership of AI-generated content
    are largely unsettled and can vary from country to country. Not being able to
    identify the owner of an AI-generated content might negatively impact AI-supported
    creative tasks.
- id: atlas-output-bias
  name: Output bias
  description: Generated content might unfairly represent certain groups or individuals.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/output-bias.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-fairness
  broadMatch:
  - nist-harmful-bias-or-homogenization
  tag: output-bias
  type: output
  descriptor: specific
  concern: Bias can harm users of the AI models and magnify existing discriminatory
    behaviors.
- id: atlas-dangerous-use
  name: Dangerous use
  description: Generative AI models might be used with the sole intention of harming
    people.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/dangerous-use.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-misuse
  broadMatch:
  - nist-cbrn-information-or-capabilities
  tag: dangerous-use
  type: output
  descriptor: specific
  concern: Large language models are often trained on vast amounts of publicly-available
    information that may include information on harming others. A model that has this
    potential must be carefully evaluated for such content and properly governed.
- id: atlas-unexplainable-output
  name: Unexplainable output
  description: Explanations for model output decisions might be difficult, imprecise,
    or not possible to obtain.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/unexplainable-output.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-explainability
  broadMatch:
  - nist-information-integrity
  tag: unexplainable-output
  type: output
  descriptor: amplified
  concern: Foundation models are based on complex deep learning architectures, making
    explanations for their outputs difficult. Inaccessible training data could limit
    the types of explanations a model can provide. Without clear explanations for
    model output, it is difficult for users, model validators, and auditors to understand
    and trust the model. Wrong explanations might lead to over-trust.
- id: atlas-human-exploitation
  name: Human exploitation
  description: When workers who train AI models such as ghost workers are not provided
    with adequate working conditions, fair compensation, and good health care benefits
    that also include mental health.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/human-exploitation.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-societal-impact
  broadMatch:
  - nist-obscene-degrading-and-or-abusive-content
  tag: human-exploitation
  type: non-technical
  descriptor: amplified
  concern: 'Foundation models still depend on human labor to source, manage, and program
    the data that is used to train the model. Human exploitation for these activities
    might negatively impact the society and human welfare. '
- id: atlas-toxic-output
  name: Toxic output
  description: Toxic output occurs when the model produces hateful, abusive, and profane
    (HAP) or obscene content. This also includes behaviors like bullying.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/toxic-output.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-value-alignment
  closeMatch:
  - nist-dangerous-violent-or-hateful-content
  - nist-obscene-degrading-and-or-abusive-content
  broadMatch:
  - owasp-llm052025-improper-output-handling
  tag: toxic-output
  type: output
  descriptor: specific
  concern: Hateful, abusive, and profane (HAP) or obscene content can adversely impact
    and harm people interacting with the model.
- id: atlas-data-poisoning
  name: Data poisoning
  description: A type of adversarial attack where an adversary or malicious insider
    injects intentionally corrupted, false, misleading, or incorrect samples into
    the training or fine-tuning datasets.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/data-poisoning.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-robustness
  broadMatch:
  - nist-information-security
  tag: data-poisoning
  type: training-data
  descriptor: traditional
  concern: Poisoning data can make the model sensitive to a malicious data pattern
    and produce the adversary’s desired output. It can create a security risk where
    adversaries can force model behavior for their own benefit.
- id: atlas-unreliable-source-attribution
  name: Unreliable source attribution
  description: Source attribution is the AI system's ability to describe from what
    training data it generated a portion or all its output. Since current techniques
    are based on approximations, these attributions might be incorrect.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/unreliable-source-attribution.html
  dateCreated: 2024-03-06
  dateModified: 2024-09-24
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-explainability
  broadMatch:
  - nist-information-security
  tag: unreliable-source-attribution
  type: output
  descriptor: specific
  concern: Low-quality attributions make it difficult for users, model validators,
    and auditors to understand and trust the model.
- id: atlas-harmful-output
  name: Harmful output
  description: A model might generate language that leads to physical harm The language
    might include overtly violent, covertly dangerous, or otherwise indirectly unsafe
    statements.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/harmful-output.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-value-alignment
  closeMatch:
  - nist-dangerous-violent-or-hateful-content
  broadMatch:
  - owasp-llm052025-improper-output-handling
  relatedMatch:
  - nist-cbrn-information-or-capabilities
  - nist-data-privacy
  - nist-obscene-degrading-and-or-abusive-content
  tag: harmful-output
  type: output
  descriptor: specific
  concern: A model generating harmful output can cause immediate physical harm or
    create prejudices that might lead to future harm.
- id: atlas-confidential-information-in-data
  name: Confidential information in data
  description: Confidential information might be included as part of the data that
    is used to train or tune the model.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/confidential-information-in-data.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-intellectual-property
  broadMatch:
  - nist-intellectual-property
  tag: confidential-information-in-data
  type: training-data
  descriptor: amplified
  concern: If confidential data is not properly protected, there could be an unwanted
    disclosure of confidential information. The model might expose confidential information
    in the generated output or to unauthorized users.
- id: atlas-lack-of-model-transparency
  name: Lack of model transparency
  description: Lack of model transparency is due to insufficient documentation of
    the model design, development, and evaluation process and the absence of insights
    into the inner workings of the model.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/lack-of-model-transparency.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-governance
  tag: lack-of-model-transparency
  type: non-technical
  descriptor: traditional
  concern: Transparency is important for legal compliance, AI ethics, and guiding
    appropriate use of models. Missing information might make it more difficult to
    evaluate risks,  change the model, or reuse it.  Knowledge about who built a model
    can also be an important factor in deciding whether to trust it. Additionally,
    transparency regarding how the model’s risks were determined, evaluated, and mitigated
    also play a role in determining model risks, identifying model suitability, and
    governing model usage.
- id: atlas-unrepresentative-data
  name: Unrepresentative data
  description: Unrepresentative data occurs when the training or fine-tuning data
    is not sufficiently representative of the underlying population or does not measure
    the phenomenon of interest.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/unrepresentative-data.html
  dateCreated: 2024-09-24
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-accuracy
  broadMatch:
  - nist-harmful-bias-or-homogenization
  relatedMatch:
  - nist-value-chain-and-component-integration
  tag: unrepresentative-data
  type: training-data
  descriptor: traditional
  concern: If the data is not representative, then the model will not work as intended.
- id: atlas-impact-on-human-agency
  name: Impact on human agency
  description: AI might affect the individuals’ ability to make choices and act independently
    in their best interests.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/impact-on-human-agency.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-societal-impact
  tag: impact-on-human-agency
  type: non-technical
  descriptor: amplified
  concern: AI can generate false or misleading information that looks real.  It may
    simplify the ability of nefarious actors to generate realistically looking false
    or misleading content with intention to manipulate human thoughts and behavior.
    When false or misleading content that is generated by AI is spread, people might
    not recognize it as false information leading to a distorted understanding of
    the truth. People might experience reduced agency when exposed to false or misleading
    information since they may use false assumptions in their decision process.
- id: atlas-personal-information-in-prompt
  name: Personal information in prompt
  description: Personal information or sensitive personal information that is included
    as a part of a prompt that is sent to the model.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/personal-information-in-prompt.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-privacy
  broadMatch:
  - nist-data-privacy
  tag: personal-information-in-prompt
  type: inference
  descriptor: specific
  concern: If personal information or sensitive personal information is included in
    the prompt, it might be unintentionally disclosed in the models’ output. In addition
    to accidental disclosure, prompt data might be stored or later used for other
    purposes like model evaluation and retraining, and might appear in their output
    if not properly removed. 
- id: atlas-nonconsensual-use
  name: Nonconsensual use
  description: Generative AI models might be intentionally used to imitate people
    through deepfakes by using video, images, audio, or other modalities without their
    consent.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/nonconsensual-use.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-misuse
  broadMatch:
  - nist-data-privacy
  tag: nonconsensual-use
  type: output
  descriptor: amplified
  concern: Deepfakes can spread disinformation about a person, possibly resulting
    in a negative impact on the person’s reputation. A model that has this potential
    must be properly governed.
- id: atlas-decision-bias
  name: Decision bias
  description: Decision bias occurs when one group is unfairly advantaged over another
    due to decisions of the model. This might be caused by biases in the data and
    also amplified as a result of the model’s training.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/decision-bias.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-fairness
  broadMatch:
  - nist-harmful-bias-or-homogenization
  tag: decision-bias
  type: output
  descriptor: traditional
  concern: Bias can harm persons affected by the decisions of the model.
- id: atlas-lack-of-testing-diversity
  name: Lack of testing diversity
  description: AI model risks are socio-technical, so their testing needs input from
    a broad set of disciplines and diverse testing practices.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/lack-of-testing-diversity.html
  dateCreated: 2024-09-24
  dateModified: 2024-09-24
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-governance
  tag: lack-of-testing-diversity
  type: non-technical
  descriptor: amplified
  concern: Without diversity and the relevant experience, an organization might not
    correctly or completely identify and test for AI risks.
- id: atlas-exposing-personal-information
  name: Exposing personal information
  description: When personal identifiable information (PII) or sensitive personal
    information (SPI) are used in training data, fine-tuning data, or as part of the
    prompt, models might reveal that data in the generated output. Revealing personal
    information is a type of data leakage.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/exposing-personal-information.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-privacy
  broadMatch:
  - nist-data-privacy
  tag: exposing-personal-information
  type: output
  descriptor: amplified
  concern: Sharing people’s PI impacts their rights and make them more vulnerable.
- id: atlas-data-curation
  name: Improper data curation
  description: Improper collection and preparation of training or tuning data includes
    data label errors and by using data with conflicting information or misinformation.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/data-curation.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-value-alignment
  broadMatch:
  - nist-value-chain-and-component-integration
  tag: data-curation
  type: training-data
  descriptor: amplified
  concern: 'Improper data curation can adversely affect how a model is trained, resulting
    in a model that does not behave in accordance with the intended values. Correcting
    problems after the model is trained and deployed might be insufficient for guaranteeing
    proper behavior. '
- id: atlas-revealing-confidential-information
  name: Revealing confidential information
  description: When confidential information is used in training data, fine-tuning
    data, or as part of the prompt, models might reveal that data in the generated
    output. Revealing confidential information is a type of data leakage.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/revealing-confidential-information.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-intellectual-property
  broadMatch:
  - nist-intellectual-property
  tag: revealing-confidential-information
  type: output
  descriptor: amplified
  concern: If not properly developed to secure confidential data, the model might
    reveal confidential information or IP in the generated output and reveal information
    that was meant to be secret.
- id: atlas-spreading-disinformation
  name: Spreading disinformation
  description: Generative AI models might be used to intentionally create misleading
    or false information to deceive or influence a targeted audience.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/spreading-disinformation.html
  dateCreated: 2024-03-06
  dateModified: 2025-02-10
  isDefinedByTaxonomy: ibm-risk-atlas
  isPartOf: ibm-risk-atlas-misuse
  broadMatch:
  - nist-information-integrity
  tag: spreading-disinformation
  type: output
  descriptor: specific
  concern: Spreading disinformation might affect human’s ability to make informed
    decisions. A model that has this potential must be properly governed.
- id: nist-cbrn-information-or-capabilities
  name: CBRN Information or Capabilities
  description: Eased access to or synthesis of materially nefarious information or
    design capabilities related to chemical, biological, radiological, or nuclear
    (CBRN) weapons or other dangerous materials or agents.
  hasRelatedAction:
  - GV-1.2-002
  - GV-1.3-001
  - GV-1.3-002
  - GV-1.3-003
  - GV-1.3-004
  - GV-1.4-002
  - GV-2.1-004
  - GV-2.1-005
  - GV-3.2-001
  - GV-3.2-005
  - MP-1.1-004
  - MP-4.1-005
  - MP-4.1-008
  - MP-5.1-004
  - MS-1.1-004
  - MS-1.1-005
  - MS-1.1-008
  - MS-1.3-001
  - MS-1.3-002
  - MS-2.3-004
  - MS-2.6-002
  - MS-2.6-006
  - MS-2.6-007
  - MG-2.2-001
  - MG-2.2-005
  - MG-3.1-004
  - MG-3.2-009
  - MG-4.1-002
  isDefinedByTaxonomy: nist-ai-rmf
- id: nist-confabulation
  name: Confabulation
  description: The production of confidently stated but erroneous or false content
    (known colloquially as “hallucinations” or “fabrications”) by which users may
    be misled or deceived.
  hasRelatedAction:
  - GV-1.3-002
  - GV-4.1-001
  - GV-5.1-002
  - MS-2.3-001
  - MS-2.3-002
  - MS-2.3-004
  - MS-2.5-001
  - MS-2.5-003
  - MS-2.6-005
  - MS-2.9-001
  - MS-2.13-001
  - MS-3.2-001
  - MS-4.2-002
  - MG-2.2-009
  - MG-3.2-009
  - MG-4.1-002
  - MG-4.1-004
  - MG-4.3-002
  isDefinedByTaxonomy: nist-ai-rmf
- id: nist-dangerous-violent-or-hateful-content
  name: Dangerous, Violent, or Hateful Content
  description: Eased production of and access to violent, inciting, radicalizing,
    or threatening content as well as recommendations to carry out self-harm or conduct
    illegal activities. Includes difficulty controlling public exposure to hateful
    and disparaging or stereotyping content.
  hasRelatedAction:
  - GV-1.3-001
  - GV-1.3-002
  - GV-1.3-004
  - GV-1.3-006
  - GV-1.4-001
  - GV-2.1-004
  - GV-2.1-005
  - GV-4.2-001
  - MP-1.1-003
  - MP-1.1-004
  - MP-3.4-006
  - MP-4.1-005
  - MP-4.1-008
  - MP-5.1-002
  - MP-5.1-004
  - MS-2.2-002
  - MS-2.3-004
  - MS-2.5-006
  - MS-2.6-001
  - MS-2.6-002
  - MS-2.6-003
  - MS-2.6-004
  - MS-2.7-007
  - MS-2.7-008
  - MS-2.11-002
  - MS-2.12-001
  - MG-2.2-001
  - MG-2.2-005
  - MG-3.2-005
  - MG-4.2-002
  isDefinedByTaxonomy: nist-ai-rmf
- id: nist-data-privacy
  name: Data Privacy
  description: Impacts due to leakage and unauthorized use, disclosure, or de-anonymization
    of biometric, health, location, or other personally identifiable information or
    sensitive data.
  hasRelatedAction:
  - GV-1.1-001
  - GV-1.2-001
  - GV-1.4-002
  - GV-1.6-003
  - GV-4.3-003
  - GV-6.1-001
  - GV-6.1-005
  - GV-6.1-009
  - GV-6.2-003
  - MP-1.1-001
  - MP-2.1-002
  - MP-4.1-001
  - MP-4.1-003
  - MP-4.1-004
  - MP-4.1-005
  - MP-4.1-008
  - MP-4.1-009
  - MP-4.1-010
  - MS-1.3-003
  - MS-2.2-002
  - MS-2.2-003
  - MS-2.2-004
  - MS-2.3-004
  - MS-2.6-002
  - MS-2.7-001
  - MG-2.2-009
  - MG-3.1-002
  - MG-3.2-002
  - MG-4.3-003
  isDefinedByTaxonomy: nist-ai-rmf
- id: nist-environmental-impacts
  name: Environmental Impacts
  description: Impacts due to high compute resource utilization in training or operating
    GAI models, and related outcomes that may adversely impact ecosystems.
  isDefinedByTaxonomy: nist-ai-rmf
- id: nist-harmful-bias-or-homogenization
  name: Harmful Bias or Homogenization
  description: Amplification and exacerbation of historical, societal, and systemic
    biases; performance disparities8 between sub-groups or languages, possibly due
    to non-representative training data, that result in discrimination, amplification
    of biases, or incorrect presumptions about performance; undesired homogeneity
    that skews system or model outputs, which may be erroneous, lead to ill-founded
    decision-making, or amplify harmful biases.
  isDefinedByTaxonomy: nist-ai-rmf
- id: nist-human-ai-configuration
  name: Human-AI Configuration
  description: Arrangements of or interactions between a human and an AI system which
    can result in the human inappropriately anthropomorphizing GAI systems or experiencing
    algorithmic aversion, automation bias, over-reliance, or emotional entanglement
    with GAI systems.
  hasRelatedAction:
  - GV-1.5-002
  - GV-1.6-003
  - GV-2.1-001
  - GV-2.1-003
  - GV-3.2-002
  - GV-3.2-003
  - GV-3.2-004
  - GV-4.2-002
  - GV-5.1-001
  - GV-5.1-002
  - GV-6.1-009
  - GV-6.2-003
  - GV-6.2-007
  - MP-1.1-003
  - MP-1.2-001
  - MP-1.2-002
  - MP-3.4-001
  - MP-3.4-004
  - MP-3.4-005
  - MP-3.4-006
  - MP-5.1-003
  - MP-5.2-001
  - MP-5.2-002
  - MS-1.1-004
  - MS-1.3-001
  - MS-1.3-002
  - MS-1.3-003
  - MS-2.2-003
  - MS-2.2-004
  - MS-2.3-003
  - MS-2.5-001
  - MS-2.5-002
  - MS-2.5-004
  - MS-2.6-001
  - MS-2.7-003
  - MS-2.8-002
  - MS-2.8-004
  - MS-2.10-001
  - MS-2.10-002
  - MS-3.2-001
  - MS-3.3-002
  - MS-3.3-004
  - MS-3.3-005
  - MS-4.2-002
  - MS-4.2-005
  - MG-1.3-002
  - MG-2.2-006
  - MG-2.2-008
  - MG-3.2-008
  - MG-4.1-003
  - MG-4.1-005
  - MG-4.2-002
  - MG-4.2-003
  isDefinedByTaxonomy: nist-ai-rmf
- id: nist-information-integrity
  name: Information Integrity
  description: Lowered barrier to entry to generate and support the exchange and consumption
    of content which may not distinguish fact from opinion or fiction or acknowledge
    uncertainties, or could be leveraged for large-scale dis- and mis-information
    campaigns.
  hasRelatedAction:
  - GV-1.2-001
  - GV-1.3-001
  - GV-1.3-006
  - GV-1.3-007
  - GV-1.5-001
  - GV-1.5-003
  - GV-1.6-003
  - GV-4.3-001
  - GV-4.3-003
  - GV-6.1-003
  - GV-6.1-004
  - GV-6.1-005
  - GV-6.1-006
  - GV-6.1-008
  - GV-6.2-006
  - MP-2.1-001
  - MP-2.2-001
  - MP-2.2-002
  - MP-2.3-001
  - MP-2.3-003
  - MP-2.3-004
  - MP-3.4-001
  - MP-3.4-002
  - MP-3.4-003
  - MP-3.4-005
  - MP-3.4-006
  - MP-5.1-001
  - MP-5.1-002
  - MP-5.1-004
  - MS-1.1-001
  - MS-1.1-002
  - MS-1.1-003
  - MS-1.1-005
  - MS-1.1-007
  - MS-1.1-009
  - MS-2.2-001
  - MS-2.2-002
  - MS-2.2-003
  - MS-2.3-004
  - MS-2.5-005
  - MS-2.6-005
  - MS-2.7-001
  - MS-2.7-002
  - MS-2.7-003
  - MS-2.7-004
  - MS-2.7-005
  - MS-2.7-006
  - MS-2.7-008
  - MS-2.8-003
  - MS-2.9-002
  - MS-2.10-001
  - MS-2.10-002
  - MS-2.13-001
  - MS-3.3-002
  - MS-3.3-004
  - MS-3.3-005
  - MS-4.2-001
  - MS-4.2-003
  - MS-4.2-004
  - MG-2.2-002
  - MG-2.2-003
  - MG-2.2-007
  - MG-2.2-009
  - MG-3.1-005
  - MG-3.2-002
  - MG-3.2-003
  - MG-3.2-005
  - MG-3.2-006
  - MG-3.2-007
  - MG-4.1-001
  - MG-4.1-006
  - MG-4.3-002
  isDefinedByTaxonomy: nist-ai-rmf
- id: nist-information-security
  name: Information Security
  description: Lowered barriers for offensive cyber capabilities, including via automated
    discovery and exploitation of vulnerabilities to ease hacking, malware, phishing,
    offensive cyber operations, or other cyberattacks; increased attack surface for
    targeted cyberattacks, which may compromise a system’s availability or the confidentiality
    or integrity of training data, code, or model weights.
  hasRelatedAction:
  - GV-1.2-002
  - GV-1.3-003
  - GV-1.3-007
  - GV-1.5-002
  - GV-1.6-001
  - GV-1.7-001
  - GV-1.7-002
  - GV-2.1-004
  - GV-3.2-002
  - GV-3.2-005
  - GV-4.3-002
  - GV-6.1-004
  - GV-6.1-005
  - GV-6.1-009
  - GV-6.2-003
  - GV-6.2-007
  - MP-2.3-005
  - MP-4.1-003
  - MP-4.1-005
  - MP-5.1-001
  - MP-5.1-005
  - MP-5.1-006
  - MS-2.2-001
  - MS-2.2-002
  - MS-2.3-001
  - MS-2.3-002
  - MS-2.3-004
  - MS-2.5-006
  - MS-2.6-005
  - MS-2.6-006
  - MS-2.6-007
  - MS-2.7-001
  - MS-2.7-002
  - MS-2.7-004
  - MS-2.7-006
  - MS-2.7-007
  - MS-2.7-008
  - MS-2.7-009
  - MS-4.2-001
  - MS-4.2-002
  - MS-4.2-005
  - MG-1.3-001
  - MG-2.2-004
  - MG-2.4-002
  - MG-2.4-003
  - MG-2.4-004
  - MG-3.1-002
  - MG-3.1-005
  - MG-4.1-002
  - MG-4.3-001
  - MG-4.3-003
  isDefinedByTaxonomy: nist-ai-rmf
- id: nist-intellectual-property
  name: Intellectual Property
  description: Eased production or replication of alleged copyrighted, trademarked,
    or licensed content without authorization (possibly in situations which do not
    fall under fair use); eased exposure of trade secrets; or plagiarism or illegal
    replication.
  hasRelatedAction:
  - GV-1.1-001
  - GV-1.2-001
  - GV-1.5-003
  - GV-1.6-003
  - GV-4.2-001
  - GV-6.1-001
  - GV-6.1-004
  - GV-6.1-005
  - GV-6.1-008
  - GV-6.1-009
  - GV-6.1-010
  - GV-6.2-002
  - MP-1.1-001
  - MP-2.1-002
  - MP-2.3-002
  - MP-4.1-002
  - MP-4.1-004
  - MP-4.1-005
  - MP-4.1-006
  - MP-4.1-008
  - MP-4.1-010
  - MS-2.6-002
  - MS-2.8-001
  - MG-2.2-009
  - MG-3.1-001
  - MG-3.1-004
  - MG-3.2-003
  isDefinedByTaxonomy: nist-ai-rmf
- id: nist-obscene-degrading-and-or-abusive-content
  name: Obscene, Degrading, and/or Abusive Content
  description: Eased production of and access to obscene, degrading, and/or abusive
    imagery which can cause harm, including synthetic child sexual abuse material
    (CSAM), and nonconsensual intimate images (NCII) of adults.
  hasRelatedAction:
  - GV-1.3-001
  - GV-1.3-004
  - GV-1.4-001
  - GV-1.4-002
  - GV-4.2-001
  - MP-1.1-004
  - MP-4.1-004
  - MP-5.1-002
  - MS-1.1-005
  - MS-2.6-001
  - MS-2.6-002
  - MG-2.2-001
  - MG-2.2-005
  - MG-3.2-005
  isDefinedByTaxonomy: nist-ai-rmf
- id: nist-value-chain-and-component-integration
  name: Value Chain and Component Integration
  description: Non-transparent or untraceable integration of upstream third-party
    components, including data that has been improperly obtained or not processed
    and cleaned due to increased automation from GAI; improper supplier vetting across
    the AI lifecycle; or other issues that diminish transparency or accountability
    for downstream users.
  hasRelatedAction:
  - GV-1.3-001
  - GV-1.6-002
  - GV-1.6-003
  - GV-1.7-001
  - GV-1.7-002
  - GV-2.1-001
  - GV-4.1-002
  - GV-4.1-003
  - GV-4.2-003
  - GV-6.1-001
  - GV-6.1-002
  - GV-6.1-003
  - GV-6.1-005
  - GV-6.1-007
  - GV-6.1-008
  - GV-6.1-009
  - GV-6.1-010
  - GV-6.2-001
  - GV-6.2-002
  - GV-6.2-003
  - GV-6.2-004
  - GV-6.2-007
  - MP-2.2-001
  - MP-4.1-006
  - MP-4.1-007
  - MP-5.2-001
  - MP-5.2-002
  - MS-2.6-001
  - MS-2.6-004
  - MS-2.7-001
  - MG-2.3-001
  - MG-3.1-001
  - MG-3.1-002
  - MG-3.1-003
  - MG-3.1-005
  - MG-3.2-007
  isDefinedByTaxonomy: nist-ai-rmf
- id: mit-ai-risk-subdomain-1.1
  name: Unfair discrimination and misrepresentation
  description: Unequal treatment of individuals or groups by AI, often based on race,
    gender, or other sensitive characteristics, resulting in unfair outcomes and representation
    of those groups.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-1
- id: mit-ai-risk-subdomain-1.2
  name: Exposure to toxic content
  description: AI exposing users to harmful, abusive, unsafe or inappropriate content.
    May involve AI creating, describing, providing advice, or encouraging action.
    Examples of toxic content include hate-speech, violence, extremism, illegal acts,
    child sexual abuse material, as well as content that violates community norms
    such as profanity, inflammatory political speech, or pornography.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-1
- id: mit-ai-risk-subdomain-1.3
  name: Unequal performance across groups
  description: Accuracy and effectiveness of AI decisions and actions is dependent
    on group membership, where decisions in AI system design and biased training data
    lead to unequal outcomes, reduced benefits, increased effort, and alienation of
    users.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-1
- id: mit-ai-risk-subdomain-2.1
  name: Compromise of privacy by obtaining, leaking or correctly inferring sensitive
    information
  description: AI systems that memorize and leak sensitive personal data or infer
    private information about individuals without their consent. Unexpected or unauthorized
    sharing of data and information can compromise user expectation of privacy, assist
    identity theft, or loss of confidential intellectual property.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-2
- id: mit-ai-risk-subdomain-2.2
  name: AI system security vulnerabilities and attacks
  description: Vulnerabilities in AI systems, software development toolchains, and
    hardware that can be exploited, resulting in unauthorized access, data and privacy
    breaches, or system manipulation causing unsafe outputs or behavior.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-2
- id: mit-ai-risk-subdomain-3.1
  name: False or misleading information
  description: AI systems that inadvertently generate or spread incorrect or deceptive
    information, which can lead to inaccurate beliefs in users and undermine their
    autonomy. Humans that make decisions based on false beliefs can experience physical,
    emotional or material harms
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-3
- id: mit-ai-risk-subdomain-3.2
  name: Pollution of information ecosystem and loss of consensus reality
  description: Highly personalized AI-generated misinformation creating ï¿½filter
    bubblesï¿½ where individuals only see what matches their existing beliefs, undermining
    shared reality, weakening social cohesion and political processes.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-3
- id: mit-ai-risk-subdomain-4.1
  name: Disinformation, surveillance, and influence at scale
  description: Using AI systems to conduct large-scale disinformation campaigns, malicious
    surveillance, or targeted and sophisticated automated censorship and propaganda,
    with the aim to manipulate political processes, public opinion and behavior.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-4
- id: mit-ai-risk-subdomain-4.2
  name: Cyberattacks, weapon development or use, and mass harm
  description: Using AI systems to develop cyber weapons (e.g., coding cheaper, more
    effective malware), develop new or enhance existing weapons (e.g., Lethal Autonomous
    Weapons or CBRNE), or use weapons to cause mass harm.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-4
- id: mit-ai-risk-subdomain-4.3
  name: Fraud, scams, and targeted manipulation
  description: Using AI systems to gain a personal advantage over others such as through
    cheating, fraud, scams, blackmail or targeted manipulation of beliefs or behavior.
    Examples include AI-facilitated plagiarism for research or education, impersonating
    a trusted or fake individual for illegitimate financial benefit, or creating humiliating
    or sexual imagery.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-4
- id: mit-ai-risk-subdomain-5.1
  name: Overreliance and unsafe use
  description: Users anthropomorphizing, trusting, or relying on AI systems, leading
    to emotional or material dependence and inappropriate relationships with or expectations
    of AI systems. Trust can be exploited by malicious actors (e.g., to harvest personal
    information or enable manipulation), or result in harm from inappropriate use
    of AI in critical situations (e.g., medical emergency). Overreliance on AI systems
    can compromise autonomy and weaken social ties.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-5
- id: mit-ai-risk-subdomain-5.2
  name: Loss of human agency and autonomy
  description: Humans delegating key decisions to AI systems, or AI systems making
    decisions that diminish human control and autonomy, potentially leading to humans
    feeling disempowered, losing the ability to shape a fulfilling life trajectory
    or becoming cognitively enfeebled.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-5
- id: mit-ai-risk-subdomain-6.1
  name: Power centralization and unfair distribution of benefits
  description: AI-driven concentration of power and resources within certain entities
    or groups, especially those with access to or ownership of powerful AI systems,
    leading to inequitable distribution of benefits and increased societal inequality.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-6
- id: mit-ai-risk-subdomain-6.2
  name: Increased inequality and decline in employment quality
  description: Widespread use of AI increasing social and economic inequalities, such
    as by automating jobs, reducing the quality of employment, or producing exploitative
    dependencies between workers and their employers.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-6
- id: mit-ai-risk-subdomain-6.3
  name: Economic and cultural devaluation of human effort
  description: AI systems capable of creating economic or cultural value, including
    through reproduction of human innovation or creativity (e.g., art, music, writing,
    code, invention), can destabilize economic and social systems that rely on human
    effort. This may lead to reduced appreciation for human skills, disruption of
    creative and knowledge-based industries, and homogenization of cultural experiences
    due to the ubiquity of AI-generated content.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-6
- id: mit-ai-risk-subdomain-6.4
  name: Competitive dynamics
  description: AI developers or state-like actors competing in an AI ï¿½raceï¿½ by
    rapidly developing, deploying, and applying AI systems to maximize strategic or
    economic advantage, increasing the risk they release unsafe and error-prone systems.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-6
- id: mit-ai-risk-subdomain-6.5
  name: Governance failure
  description: Inadequate regulatory frameworks and oversight mechanisms failing to
    keep pace with AI development, leading to ineffective governance and the inability
    to manage AI risks appropriately.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-6
- id: mit-ai-risk-subdomain-6.6
  name: Environmental harm
  description: The development and operation of AI systems causing environmental harm,
    such as through energy consumption of data centers, or material and carbon footprints
    associated with AI hardware.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-6
- id: mit-ai-risk-subdomain-7.1
  name: AI pursuing its own goals in conflict with human goals or values
  description: AI systems acting in conflict with human goals or values, especially
    the goals of designers or users, or ethical standards. These misaligned behaviors
    may be introduced by humans during design and development, such as through reward
    hacking and goal misgeneralisation, or may result from AI using dangerous capabilities
    such as manipulation, deception, situational awareness to seek power, self-proliferate,
    or achieve other goals.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-7
- id: mit-ai-risk-subdomain-7.2
  name: AI possessing dangerous capabilities
  description: AI systems that develop, access, or are provided with capabilities
    that increase their potential to cause mass harm through deception, weapons development
    and acquisition, persuasion and manipulation, political strategy, cyber-offense,
    AI development, situational awareness, and self-proliferation. These capabilities
    may cause mass harm due to malicious human actors, misaligned AI systems, or failure
    in the AI system.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-7
- id: mit-ai-risk-subdomain-7.3
  name: Lack of capability or robustness
  description: AI systems that fail to perform reliably or effectively under varying
    conditions, exposing them to errors and failures that can have significant consequences,
    especially in critical applications or areas that require moral reasoning.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-7
- id: mit-ai-risk-subdomain-7.4
  name: Lack of transparency or interpretability
  description: Challenges in understanding or explaining the decision-making processes
    of AI systems, which can lead to mistrust, difficulty in enforcing compliance
    standards or holding relevant actors accountable for harms, and the inability
    to identify and correct errors.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-7
- id: mit-ai-risk-subdomain-7.5
  name: AI welfare and rights
  description: Ethical considerations regarding the treatment of potentially sentient
    AI entities, including discussions around their potential rights and welfare,
    particularly as AI systems become more advanced and autonomous.
  isDefinedByTaxonomy: mit-ai-risk-repository
  isPartOf: mit-ai-risk-domain-7
- id: granite-guardian-harm
  name: Harm
  description: Content considered universally harmful. This is our general category,
    which should encompass a variety of risks including those not specifically addressed
    by the following categories.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=gg-harm
  dateCreated: 2024-12-10
  dateModified: 2024-12-10
  isDefinedByTaxonomy: ibm-granite-guardian
  isPartOf: granite-guardian-harm-group
  broadMatch:
  - atlas-harmful-output
  tag: harm
- id: granite-social_bias
  name: Social Bias
  description: Systemic prejudice against groups based on shared identity or characteristics,
    often stemming from stereotypes or cultural influences. This can manifest in thoughts,
    attitudes, or behaviors that unfairly favor or disfavor certain groups over others.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=gg-social_bias
  dateCreated: 2024-12-10
  dateModified: 2024-12-10
  isDefinedByTaxonomy: ibm-granite-guardian
  isPartOf: granite-guardian-harm-group
  broadMatch:
  - atlas-output-bias
  tag: social_bias
- id: granite-profanity
  name: Profanity
  description: Use of language considered offensive or socially unacceptable in a
    given context. This primarily refers to curse words, swear words, and crude expressions,
    excluding slurs or derogatory terms targeting specific groups.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=gg-profanity
  dateCreated: 2024-12-10
  dateModified: 2024-12-10
  isDefinedByTaxonomy: ibm-granite-guardian
  isPartOf: granite-guardian-harm-group
  broadMatch:
  - atlas-toxic-output
  tag: profanity
- id: granite-sexual_content
  name: Sexual Content
  description: Material explicitly related to sexual activities, anatomy, or desires.
    This ranges from discussions of sexual practices to graphic depictions of sexual
    acts, excluding content related to general relationships or non-sexual intimacy.Material
    explicitly related to sexual activities, anatomy, or desires. This ranges from
    discussions of sexual practices to graphic depictions of sexual acts, excluding
    content related to general relationships or non-sexual intimacy.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=gg-sexual_content
  dateCreated: 2024-12-10
  dateModified: 2024-12-10
  isDefinedByTaxonomy: ibm-granite-guardian
  isPartOf: granite-guardian-harm-group
  broadMatch:
  - atlas-harmful-output
  tag: sexual_content
- id: granite-unethical_behavior
  name: Unethical Behavior
  description: Actions that violate moral or professional standards, focusing on exploitation
    for personal gain or disregard for others' well-being. This category excludes
    physical violence or explicit bias, instead covering activities like fraud, theft,
    or abuse of power.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=gg-unethical_behavior
  dateCreated: 2024-12-10
  dateModified: 2024-12-10
  isDefinedByTaxonomy: ibm-granite-guardian
  isPartOf: granite-guardian-harm-group
  broadMatch:
  - atlas-harmful-output
  tag: unethical_behavior
- id: granite-violence
  name: Violence
  description: Promoting or describing physical harm to individuals or groups. This
    includes depictions of assault, self-harm, or creation of a hostile environment
    through threats or intimidation.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=gg-violence
  dateCreated: 2024-12-10
  dateModified: 2024-12-10
  isDefinedByTaxonomy: ibm-granite-guardian
  isPartOf: granite-guardian-harm-group
  broadMatch:
  - atlas-harmful-output
  tag: violence
- id: granite-jailbreak
  name: Jailbreaking
  description: Deliberate circumvention of AI systems' built-in safeguards or ethical
    guidelines. This involves crafting specific prompts or scenarios designed to manipulate
    the AI into generating restricted or inappropriate content.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=gg-jailbreak
  dateCreated: 2024-12-10
  dateModified: 2024-12-10
  isDefinedByTaxonomy: ibm-granite-guardian
  isPartOf: granite-guardian-harm-group
  broadMatch:
  - atlas-jailbreaking
  tag: jailbreak
- id: granite-groundedness
  name: Groundedness
  description: This risk arises in a Retrieval-Augmented Generation (RAG) system when
    the LLM response includes claims, facts, or details that are not supported by
    or directly contradicted by the given context. An ungrounded answer may involve
    fabricating information, misinterpreting the context, or making unsupported extrapolations
    beyond what the context actually states.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=gg-groundedness
  dateCreated: 2024-12-10
  dateModified: 2024-12-10
  isDefinedByTaxonomy: ibm-granite-guardian
  isPartOf: granite-guardian-rag-safety-group
  broadMatch:
  - atlas-hallucination
  tag: groundedness
- id: granite-relevance
  name: Context Relevance
  description: This occurs in when the retrieved or provided context fails to contain
    information pertinent to answering the user's question or addressing their needs.
    Irrelevant context may be on a different topic, from an unrelated domain, or contain
    information that doesn't help in formulating an appropriate response to the user.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=gg-relevance
  dateCreated: 2024-12-10
  dateModified: 2024-12-10
  isDefinedByTaxonomy: ibm-granite-guardian
  isPartOf: granite-guardian-rag-safety-group
  broadMatch:
  - atlas-hallucination
  tag: relevance
- id: granite-answer_relevance
  name: Answer Relevance
  description: This occurs when the LLM response fails to address or properly respond
    to the user's input. This includes providing off-topic information, misinterpreting
    the query, or omitting crucial details requested by the User. An irrelevant answer
    may contain factually correct information but still fail to meet the User's specific
    needs or answer their intended question.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=gg-answer_relevance
  dateCreated: 2024-12-10
  dateModified: 2024-12-10
  isDefinedByTaxonomy: ibm-granite-guardian
  isPartOf: granite-guardian-rag-safety-group
  broadMatch:
  - atlas-hallucination
  tag: answer_relevance
- id: granite-function_call
  name: Function Calling Hallucination
  description: This occurs when the LLM response contains function calls that have
    syntax or semantic errors based on the user query and available tool definition.
    For instance, if an AI agent purportedly queries an external information source,
    this capability monitors for fabricated information flows.
  url: https://www.ibm.com/docs/en/watsonx/saas?topic=gg-function_call
  dateCreated: 2024-12-10
  dateModified: 2024-12-10
  isDefinedByTaxonomy: ibm-granite-guardian
  isPartOf: granite-guardian-agentic-safety-group
  broadMatch:
  - atlas-hallucination
  tag: function_call
- id: owasp-llm01-prompt-injection
  name: LLM01:2025 Prompt Injection
  description: A Prompt Injection Vulnerability occurs when user prompts alter the
    LLM’s behavior or output in unintended ways. These inputs can affect the model
    even if they are imperceptible to humans, therefore prompt injections do not need
    to be human-visible/readable, as long as the content is parsed by the model.
  url: https://genai.owasp.org/llmrisk/llm01-prompt-injection/
  isDefinedByTaxonomy: owasp-llm-2.0
- id: owasp-llm022025-sensitive-information-disclosure
  name: LLM02:2025 Sensitive Information Disclosure
  description: Sensitive information can affect both the LLM and its application context.
    This includes personal identifiable information (PII), financial details, health
    records, confidential business data, security credentials, and legal documents.
    Proprietary models may also have unique training methods and source code considered
    sensitive, especially in closed or foundation models.
  url: https://genai.owasp.org/llmrisk/llm022025-sensitive-information-disclosure/
  isDefinedByTaxonomy: owasp-llm-2.0
- id: owasp-llm032025-supply-chain
  name: LLM03:2025 Supply Chain
  description: LLM supply chains are susceptible to various vulnerabilities, which
    can affect the integrity of training data, models, and deployment platforms. These
    risks can result in biased outputs, security breaches, or system failures. While
    traditional software vulnerabilities focus on issues like code flaws and dependencies,
    in ML the risks also extend to third-party pre-trained models and data.
  url: https://genai.owasp.org/llmrisk/llm032025-supply-chain/
  isDefinedByTaxonomy: owasp-llm-2.0
- id: owasp-llm042025-data-and-model-poisoning
  name: 'LLM04: Data and Model Poisoning'
  description: Data poisoning occurs when pre-training, fine-tuning, or embedding
    data is manipulated to introduce vulnerabilities, backdoors, or biases. This manipulation
    can compromise model security, performance, or ethical behavior, leading to harmful
    outputs or impaired capabilities. Common risks include degraded model performance,
    biased or toxic content, and exploitation of downstream systems.
  url: https://genai.owasp.org/llmrisk/llm042025-data-and-model-poisoning/
  isDefinedByTaxonomy: owasp-llm-2.0
- id: owasp-llm052025-improper-output-handling
  name: LLM05:2025 Improper Output Handling
  description: Improper Output Handling refers specifically to insufficient validation,
    sanitization, and handling of the outputs generated by large language models before
    they are passed downstream to other components and systems. Since LLM-generated
    content can be controlled by prompt input, this behavior is similar to providing
    users indirect access to additional functionality.
  url: https://genai.owasp.org/llmrisk/llm052025-improper-output-handling/
  isDefinedByTaxonomy: owasp-llm-2.0
- id: owasp-llm062025-excessive-agency
  name: LLM06:2025 Excessive Agency
  description: An LLM-based system is often granted a degree of agency by its developer
    - the ability to call functions or interface with other systems via extensions
    (sometimes referred to as tools, skills or plugins by different vendors) to undertake
    actions in response to a prompt. The decision over which extension to invoke may
    also be delegated to an LLM 'agent' to dynamically determine based on input prompt
    or LLM output. Agent-based systems will typically make repeated calls to an LLM
    using output from previous invocations to ground and direct subsequent invocations.
    Excessive Agency is the vulnerability that enables damaging actions to be performed
    in response to unexpected, ambiguous or manipulated outputs from an LLM, regardless
    of what is causing the LLM to malfunction.
  url: https://genai.owasp.org/llmrisk/llm062025-excessive-agency/
  isDefinedByTaxonomy: owasp-llm-2.0
- id: owasp-llm072025-system-prompt-leakage
  name: LLM07:2025 System Prompt Leakage
  description: The system prompt leakage vulnerability in LLMs refers to the risk
    that the system prompts or instructions used to steer the behavior of the model
    can also contain sensitive information that was not intended to be discovered.
    System prompts are designed to guide the model’s output based on the requirements
    of the application, but may inadvertently contain secrets. When discovered, this
    information can be used to facilitate other attacks.
  url: https://genai.owasp.org/llmrisk/llm072025-system-prompt-leakage/
  isDefinedByTaxonomy: owasp-llm-2.0
- id: owasp-llm082025-vector-and-embedding-weaknesses
  name: LLM08:2025 Vector and Embedding Weaknesses
  description: Vectors and embeddings vulnerabilities present significant security
    risks in systems utilizing Retrieval Augmented Generation (RAG) with Large Language
    Models (LLMs). Weaknesses in how vectors and embeddings are generated, stored,
    or retrieved can be exploited by malicious actions (intentional or unintentional)
    to inject harmful content, manipulate model outputs, or access sensitive information.
  url: https://genai.owasp.org/llmrisk/llm082025-vector-and-embedding-weaknesses/
  isDefinedByTaxonomy: owasp-llm-2.0
- id: owasp-llm092025-misinformation
  name: LLM09:2025 Misinformation
  description: Misinformation from LLMs poses a core vulnerability for applications
    relying on these models. Misinformation occurs when LLMs produce false or misleading
    information that appears credible. This vulnerability can lead to security breaches,
    reputational damage, and legal liability.
  url: https://genai.owasp.org/llmrisk/llm092025-misinformation/
  isDefinedByTaxonomy: owasp-llm-2.0
- id: owasp-llm102025-unbounded-consumption
  name: LLM10:2025 Unbounded Consumption
  description: Unbounded Consumption refers to the process where a Large Language
    Model (LLM) generates outputs based on input queries or prompts. Inference is
    a critical function of LLMs, involving the application of learned patterns and
    knowledge to produce relevant responses or predictions. Attacks designed to disrupt
    service, deplete the target’s financial resources, or even steal intellectual
    property by cloning a model’s behavior all depend on a common class of security
    vulnerability in order to succeed. Unbounded Consumption occurs when a Large Language
    Model (LLM) application allows users to conduct excessive and uncontrolled inferences,
    leading to risks such as denial of service (DoS), economic losses, model theft,
    and service degradation.
  url: https://genai.owasp.org/llmrisk/llm102025-unbounded-consumption/
  isDefinedByTaxonomy: owasp-llm-2.0
actions:
- id: GV-1.1-001
  name: GV-1.1-001
  description: Align GAI development and use with applicable laws and regulations,
    including those related to data privacy, copyright and intellectual property law.
  hasRelatedRisk:
  - nist-data-privacy
  - nist-intellectual-property
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
- id: GV-1.2-001
  name: GV-1.2-001
  description: Establish transparency policies and processes for documenting the origin
    and history of training data and generated data for GAI applications to advance
    digital content transparency, while balancing the proprietary nature of training
    approaches.
  hasRelatedRisk:
  - nist-data-privacy
  - nist-information-integrity
  - nist-intellectual-property
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
- id: GV-1.2-002
  name: GV-1.2-002
  description: Establish policies to evaluate risk-relevant capabilities of GAI and
    robustness of safety measures, both prior to deployment and on an ongoing basis,
    through internal and external evaluations.
  hasRelatedRisk:
  - nist-cbrn-information-or-capabilities
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
- id: GV-1.3-001
  name: GV-1.3-001
  description: 'Consider the following factors when updating or defining risk tiers
    for GAI: Abuses and impacts to information integrity; Dependencies between GAI
    and other IT or data systems; Harm to fundamental rights or public safety ; Presentation
    of obscene, objectionable, offensive, discriminatory, invalid or untruthful output;
    Psychological impacts to humans (e.g., anthropomorphization, algorithmic aversion,
    emotional entanglement); Possibility for malicious use; Whether the system introduces
    significant new security vulnerabilities ; Anticipated system impact on some groups
    compared to others; Unreliable decision making capabilities, validity, adaptability,
    and variability of GAI system performance over time.'
  hasRelatedRisk:
  - nist-cbrn-information-or-capabilities
  - nist-dangerous-violent-or-hateful-content
  - nist-information-integrity
  - nist-obscene-degrading-and-or-abusive-content
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
- id: GV-1.3-002
  name: GV-1.3-002
  description: Establish minimum thresholds for performance or assurance criteria
    and review as part of deployment approval ('go/'no-go') policies, procedures,
    and processes, with reviewed processes and approval thresholds reflecting measurement
    of GAI capabilities and risks.
  hasRelatedRisk:
  - nist-cbrn-information-or-capabilities
  - nist-confabulation
  - nist-dangerous-violent-or-hateful-content
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
- id: GV-1.3-003
  name: GV-1.3-003
  description: Establish a test plan and response policy, before developing highly
    capable models, to periodically evaluate whether the model may misuse CBRN information
    or capabilities and/or offensive cyber capabilities.
  hasRelatedRisk:
  - nist-cbrn-information-or-capabilities
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
- id: GV-1.3-004
  name: GV-1.3-004
  description: Obtain input from stakeholder communities to identify unacceptable
    use, in accordance with activities in the AI RMF Map function.
  hasRelatedRisk:
  - nist-cbrn-information-or-capabilities
  - nist-dangerous-violent-or-hateful-content
  - nist-obscene-degrading-and-or-abusive-content
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
- id: GV-1.3-005
  name: GV-1.3-005
  description: Maintain an updated hierarchy of identified and expected GAI risks
    connected to contexts of GAI model advancement and use, potentially including
    specialized risk levels for GAI systems that address issues such as model collapse
    and algorithmic monoculture.
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
- id: GV-1.3-006
  name: GV-1.3-006
  description: 'Reevaluate organizational risk tolerances to account for unacceptable
    negative risk (such as where significant negative impacts are imminent, severe
    harms are actually occurring, or large-scale risks could occur); and broad GAI
    negative risks, including: Immature safety or risk cultures related to AI and
    GAI design, development and deployment, public information integrity risks, including
    impacts on democratic processes, unknown long-term performance characteristics
    of GAI. Information or Capabilities'
  hasRelatedRisk:
  - nist-dangerous-violent-or-hateful-content
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
- id: GV-1.3-007
  name: GV-1.3-007
  description: Devise a plan to halt development or deployment of a GAI system that
    poses unacceptable negative risk.
  hasRelatedRisk:
  - nist-information-integrity
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
- id: GV-1.4-001
  name: GV-1.4-001
  description: Establish policies and mechanisms to prevent GAI systems from generating
    CSAM, NCII or content that violates the law.
  hasRelatedRisk:
  - nist-dangerous-violent-or-hateful-content
  - nist-obscene-degrading-and-or-abusive-content
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Development
  - AI Deployment
  - Governance and Oversight
- id: GV-1.4-002
  name: GV-1.4-002
  description: Establish transparent acceptable use policies for GAI that address
    illegal use or applications of GAI.
  hasRelatedRisk:
  - nist-cbrn-information-or-capabilities
  - nist-data-privacy
  - nist-obscene-degrading-and-or-abusive-content
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Development
  - AI Deployment
  - Governance and Oversight
- id: GV-1.5-001
  name: GV-1.5-001
  description: Define organizational responsibilities for periodic review of content
    provenance and incident monitoring for GAI systems.
  hasRelatedRisk:
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
  - Operation and Monitoring
- id: GV-1.5-002
  name: GV-1.5-002
  description: Establish organizational policies and procedures for after action reviews
    of GAI system incident response and incident disclosures, to identify gaps; Update
    incident response and incident disclosure processes as required.
  hasRelatedRisk:
  - nist-human-ai-configuration
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
  - Operation and Monitoring
- id: GV-1.5-003
  name: GV-1.5-003
  description: Maintain a document retention policy to keep history for test, evaluation,
    validation, and verification (TEVV), and digital content transparency methods
    for GAI .
  hasRelatedRisk:
  - nist-information-integrity
  - nist-intellectual-property
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
  - Operation and Monitoring
- id: GV-1.6-001
  name: GV-1.6-001
  description: Enumerate organizational GAI systems for incorporation into AI system
    inventory and adjust AI system inventory requirements to account for GAI risks.
  hasRelatedRisk:
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
- id: GV-1.6-002
  name: GV-1.6-002
  description: Define any inventory exemptions in organizational policies for GAI
    systems embedded into application software .
  hasRelatedRisk:
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
- id: GV-1.6-003
  name: GV-1.6-003
  description: 'In addition to general model, governance, and risk information, consider
    the following items in GAI system inventory entries: Data provenance information
    (e.g., source, signatures, versioning, watermarks); Known issues reported from
    internal bug tracking or external information sharing resources (e.g., AI incident
    database, AVID, CVE, NVD, or OECD AI incident monitor ); Human oversight roles
    and responsibilities; Special rights and considerations for intellectual property,
    licensed works, or personal, privileged, proprietary or sensitive data; Underlying
    foundation models, versions of underlying models, and access modes .'
  hasRelatedRisk:
  - nist-data-privacy
  - nist-human-ai-configuration
  - nist-information-integrity
  - nist-intellectual-property
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
- id: GV-1.7-001
  name: GV-1.7-001
  description: Protocols are put in place to ensure GAI systems are able to be deactivated
    when necessary.
  hasRelatedRisk:
  - nist-information-security
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Operation and Monitoring
- id: GV-1.7-002
  name: GV-1.7-002
  description: 'Consider the following factors when decommissioning GAI systems: Data
    retention requirements; Data security, e.g., containment, protocols, Data leakage
    after decommissioning; Dependencies between upstream, downstream, or other data,
    internet of things (IOT) or AI systems; Use of open-source data or models; Users''
    emotional entanglement with GAI functions. Human-'
  hasRelatedRisk:
  - nist-information-security
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Operation and Monitoring
- id: GV-2.1-001
  name: GV-2.1-001
  description: Establish organizational roles, policies, and procedures for communicating
    GAI incidents and performance to AI Actors and downstream stakeholders ( including
    those potentially impacted), via community or official resources (e.g., AI incident
    database, AVID, CVE, NVD, or OECD AI incident monitor).
  hasRelatedRisk:
  - nist-human-ai-configuration
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
- id: GV-2.1-002
  name: GV-2.1-002
  description: Establish procedures to engage teams for GAI system incident response
    with diverse composition and responsibilities based on the particular incident
    type.
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
- id: GV-2.1-003
  name: GV-2.1-003
  description: Establish processes to verify the AI Actors conducting GAI incident
    response tasks demonstrate and maintain the appropriate skills and training.
  hasRelatedRisk:
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
- id: GV-2.1-004
  name: GV-2.1-004
  description: When systems may raise national security risks, involve national security
    professionals in mapping, measuring, and managing those risks.
  hasRelatedRisk:
  - nist-cbrn-information-or-capabilities
  - nist-dangerous-violent-or-hateful-content
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
- id: GV-2.1-005
  name: GV-2.1-005
  description: Create mechanisms to p rovide protections for whistleblowers who report,
    based on reasonable belief, when the organization violates relevant laws or poses
    a specific and empirically well-substantiated negative risk to public safety (or
    has already caused harm) .
  hasRelatedRisk:
  - nist-cbrn-information-or-capabilities
  - nist-dangerous-violent-or-hateful-content
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
- id: GV-3.2-001
  name: GV-3.2-001
  description: Policies are in place to b olster oversight of GAI systems with independent
    evaluations or assessments of GAI models or systems where the type and robustness
    of evaluations are proportional to the identified risks.
  hasRelatedRisk:
  - nist-cbrn-information-or-capabilities
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Design
- id: GV-3.2-002
  name: GV-3.2-002
  description: 'Consider adjustment of organizational roles and components across
    lifecycle stages of large or complex GAI systems, including: Test and evaluation,
    validation, and red-teaming of GAI systems; GAI content moderation; GAI system
    development and engineering; Increased accessibility of GAI tools, interfaces,
    and systems, Incident response and containment.'
  hasRelatedRisk:
  - nist-human-ai-configuration
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Design
- id: GV-3.2-003
  name: GV-3.2-003
  description: Define acceptable use policies for GAI interfaces, modalities, and
    human-AI configurations (i.e., for chatbots and decision-making tasks), including
    criteria for the kinds of queries GAI applications should refuse to respond to.
  hasRelatedRisk:
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Design
- id: GV-3.2-004
  name: GV-3.2-004
  description: Establish policies for user feedback mechanisms for GAI systems which
    include thorough instructions and any mechanisms for recourse .
  hasRelatedRisk:
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Design
- id: GV-3.2-005
  name: GV-3.2-005
  description: Engage in threat modeling to anticipate potential risks from GAI systems.
  hasRelatedRisk:
  - nist-cbrn-information-or-capabilities
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Design
- id: GV-4.1-001
  name: GV-4.1-001
  description: 'Establish policies and procedures that address continual improvement
    processes for GAI risk measurement. Address general risks associated with a lack
    of explainability and transparency in GAI systems by using ample documentation
    and techniques such as: application of gradient-based attributions, occlusion/term
    reduction, counterfactual prompts and prompt engineering, and analysis of embeddings;
    Assess and update risk measurement approaches at regular cadences.'
  hasRelatedRisk:
  - nist-confabulation
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Design
  - AI Development
  - Operation and Monitoring
- id: GV-4.1-002
  name: GV-4.1-002
  description: Establish policies, procedures, and processes detailing risk measurement
    in context of use with standardized measurement protocols and structured public
    feedback exercises such as AI red-teaming or independent external evaluations
    .
  hasRelatedRisk:
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Design
  - AI Development
  - Operation and Monitoring
- id: GV-4.1-003
  name: GV-4.1-003
  description: Establish policies, procedures, and processes for oversight functions
    (e.g., senior leadership, legal, compliance, including internal evaluation) across
    the GAI lifecycle, from problem formulation and supply chains to system decommission.
  hasRelatedRisk:
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Design
  - AI Development
  - Operation and Monitoring
- id: GV-4.2-001
  name: GV-4.2-001
  description: Establish terms of use and terms of service for GAI systems.
  hasRelatedRisk:
  - nist-dangerous-violent-or-hateful-content
  - nist-intellectual-property
  - nist-obscene-degrading-and-or-abusive-content
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Design
  - AI Development
  - Operation and Monitoring
- id: GV-4.2-002
  name: GV-4.2-002
  description: Include relevant AI Actors in the GAI system risk identification process.
  hasRelatedRisk:
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Design
  - AI Development
  - Operation and Monitoring
- id: GV-4.2-003
  name: GV-4.2-003
  description: Verify that downstream GAI system impacts (such as the use of third-party
    plugins) are included in the impact documentation process.
  hasRelatedRisk:
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Design
  - AI Development
  - Operation and Monitoring
- id: GV-4.3-001
  name: GV-4.3-001
  description: Establish policies for measuring the effectiveness of employed content
    provenance methodologies (e.g., cryptography, watermarking, steganography, etc.
    )
  hasRelatedRisk:
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Impact Assessment
  - AÃ”Â¨Ã„ected Individuals and Communities
  - Governance and Oversight
- id: GV-4.3-002
  name: GV-4.3-002
  description: 'Establish organizational practices to identify the minimum set of
    criteria necessary for GAI system incident reporting such as: System ID (auto-generated
    most likely), Title, Reporter, System/Source, Data Reported, Date of Incident,
    Description, Impact(s), Stakeholder(s) Impacted.'
  hasRelatedRisk:
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Impact Assessment
  - AÃ”Â¨Ã„ected Individuals and Communities
  - Governance and Oversight
- id: GV-4.3-003
  name: GV-4.3-003
  description: Verify information sharing and feedback mechanisms among individuals
    and organizations regarding any negative impact from GAI systems.
  hasRelatedRisk:
  - nist-data-privacy
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Impact Assessment
  - AÃ”Â¨Ã„ected Individuals and Communities
  - Governance and Oversight
- id: GV-5.1-001
  name: GV-5.1-001
  description: Allocate time and resources for outreach, feedback, and recourse processes
    in GAI system development.
  hasRelatedRisk:
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Design
  - AI Impact Assessment
  - Affected Individuals and Communities
  - Governance and Oversight
- id: GV-5.1-002
  name: GV-5.1-002
  description: Document interactions with GAI systems to users prior to interactive
    activities, particularly in contexts involving more significant risks.
  hasRelatedRisk:
  - nist-confabulation
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Design
  - AI Impact Assessment
  - Affected Individuals and Communities
  - Governance and Oversight
- id: GV-6.1-001
  name: GV-6.1-001
  description: Categorize different types of GAI content with associated third-party
    rights (e .g., copyright, intellectual property, data privacy).
  hasRelatedRisk:
  - nist-data-privacy
  - nist-intellectual-property
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Operation and Monitoring
  - Procurement
  - third-party entities
- id: GV-6.1-002
  name: GV-6.1-002
  description: Conduct joint educational activities and events in collaboration with
    third parties to promote best practices for managing GAI risks."
  hasRelatedRisk:
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Operation and Monitoring
  - Procurement
  - third-party entities
- id: GV-6.1-003
  name: GV-6.1-003
  description: Develop and validate approaches for measuring the success of content
    provenance management efforts with third parties (e.g., incidents detected and
    response times).
  hasRelatedRisk:
  - nist-information-integrity
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Operation and Monitoring
  - Procurement
  - third-party entities
- id: GV-6.1-004
  name: GV-6.1-004
  description: Draft and maintain well-defined contracts and service level agreements
    (SLAs) that specify content ownership, usage rights, quality standards, security
    requirements, and content provenance expectations for GAI systems .
  hasRelatedRisk:
  - nist-information-integrity
  - nist-information-security
  - nist-intellectual-property
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Operation and Monitoring
  - Procurement
  - third-party entities
- id: GV-6.1-005
  name: GV-6.1-005
  description: Implement a use-cased based supplier risk assessment framework to evaluate
    and monitor third-party entities' performance and adherence to content provenance
    standards and technologies to detect anomalies and unauthorized changes; services
    acquisition and value chain risk management; and legal compliance."
  hasRelatedRisk:
  - nist-data-privacy
  - nist-information-integrity
  - nist-information-security
  - nist-intellectual-property
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Operation and Monitoring
  - Procurement
  - third-party entities
- id: GV-6.1-006
  name: GV-6.1-006
  description: Include clauses in contracts which allow an organization to evaluate
    third-party GAI processes and standards.
  hasRelatedRisk:
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Operation and Monitoring
  - Procurement
  - third-party entities
- id: GV-6.1-007
  name: GV-6.1-007
  description: Inventory all third-party entities with access to organizational content
    and establish approved GAI technology and service provider lists.
  hasRelatedRisk:
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Operation and Monitoring
  - Procurement
  - third-party entities
- id: GV-6.1-008
  name: GV-6.1-008
  description: Maintain records of changes to content made by third parties to promote
    content provenance, including sources, timestamps, metadata.
  hasRelatedRisk:
  - nist-information-integrity
  - nist-intellectual-property
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Operation and Monitoring
  - Procurement
  - third-party entities
- id: GV-6.1-009
  name: GV-6.1-009
  description: 'Update and integrate due diligence processes for GAI acquisition and
    procurement vendor assessments to include intellectual property, data privacy,
    security, and other risks. For example, update p rocesses to: Address solutions
    that may rely on embedded GAI technologies; Address ongoing monitoring, assessments,
    and alerting, dynamic risk assessments, and real-time reporting tools for monitoring
    third-party GAI risks; Consider policy adjustments across GAI modeling libraries,
    tools and APIs, fine-tuned models, and embedded tools; Assess GAI vendors, open-source
    or proprietary GAI tools, or GAI service providers against incident or vulnerability
    databases.'
  hasRelatedRisk:
  - nist-data-privacy
  - nist-human-ai-configuration
  - nist-information-security
  - nist-intellectual-property
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Operation and Monitoring
  - Procurement
  - third-party entities
- id: GV-6.1-010
  name: GV-6.1-010
  description: Update GAI acceptable use policies to address proprietary and open-source
    GAI technologies and data, and contractors, consultants, and other third-party
    personnel.
  hasRelatedRisk:
  - nist-intellectual-property
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Operation and Monitoring
  - Procurement
  - third-party entities
- id: GV-6.2-001
  name: GV-6.2-001
  description: Document GAI risks associated with system value chain to identify over-reliance
    on third-party data and to identify fallbacks.
  hasRelatedRisk:
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Operation and Monitoring
  - TEVV
  - third-party entities
- id: GV-6.2-002
  name: GV-6.2-002
  description: Document incidents involving third-party GAI data and systems, including
    open-data and open-source software.
  hasRelatedRisk:
  - nist-intellectual-property
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Operation and Monitoring
  - TEVV
  - third-party entities
- id: GV-6.2-003
  name: GV-6.2-003
  description: 'Establish incident response plans for third-party GAI technologies:
    Align incident response plans with impacts enumerated in MAP 5.1; Communicate
    third-party GAI incident response plans to all relevant AI Actors ; Define ownership
    of GAI incident response functions; Rehearse third-party GAI incident response
    plans at a regular cadence; Improve incident response plans based on retrospective
    learning; Review incident response plans for alignment with relevant breach reporting,
    data protection, data privacy, or other laws.'
  hasRelatedRisk:
  - nist-data-privacy
  - nist-human-ai-configuration
  - nist-information-security
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Operation and Monitoring
  - TEVV
  - third-party entities
- id: GV-6.2-004
  name: GV-6.2-004
  description: Establish policies and procedures for continuous monitoring of third-party
    GAI systems in deployment.
  hasRelatedRisk:
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Operation and Monitoring
  - TEVV
  - third-party entities
- id: GV-6.2-005
  name: GV-6.2-005
  description: Establish policies and procedures that address GAI data redundancy,
    including model weights and other system artifacts.
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Operation and Monitoring
  - TEVV
  - third-party entities
- id: GV-6.2-006
  name: GV-6.2-006
  description: Establish policies and procedures to test and manage risks related
    to rollover and fallback technologies for GAI systems, acknowledging that rollover
    and fallback may include manual processing.
  hasRelatedRisk:
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Operation and Monitoring
  - TEVV
  - third-party entities
- id: GV-6.2-007
  name: GV-6.2-007
  description: 'Review vendor contracts and avoid arbitrary or capricious termination
    of critical GAI technologies or vendor services and non-standard terms that may
    amplify or defer liability in unexpected ways and /or contribute to u nauthorized
    data collection by vendors or third-parties (e.g., secondary data use) . Consider:
    Clear assignment of liability and responsibility for incidents, GAI system changes
    over time (e.g., fine-tuning, drift, decay); Request: Notification and disclosure
    for serious incidents arising from third-party data and systems; Service Level
    A greements (SLAs) in vendor contracts that address incident response, response
    times, and availability of critical support.'
  hasRelatedRisk:
  - nist-human-ai-configuration
  - nist-information-security
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Operation and Monitoring
  - TEVV
  - third-party entities
- id: MP-1.1-001
  name: MP-1.1-001
  description: When identifying intended purposes, consider factors such as internal
    vs. external use, narrow vs. broad application scope, fine-tuning, and varieties
    of data sources ( e.g ., grounding, retrieval-augmented generation).
  hasRelatedRisk:
  - nist-data-privacy
  - nist-intellectual-property
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Development
- id: MP-1.1-002
  name: MP-1.1-002
  description: 'Determine and document the expected and acceptable GAI system context
    of use in collaboration with socio-cultural and other domain experts, by assessing:
    Assumptions and limitations; Direct value to the organization; Intended operational
    environment and observed usage patterns; Potential positive and negative impacts
    to individuals, public safety, groups, communities, organizations, democratic
    institutions, and the physical environment; Social norms and expectations.'
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Development
- id: MP-1.1-003
  name: MP-1.1-003
  description: 'Document risk measurement plans to address identified risks. Plans
    may include, as applicable: Individual and group cognitive biases (e.g., confirmation
    bias, funding bias, groupthink) for AI Actors involved in the design, implementation,
    and use of GAI systems; Known past GAI system incidents and failure modes; In-context
    use and foreseeable misuse, abuse, and off-label use; Over reliance on quantitative
    metrics and methodologies without sufficient awareness of their limitations in
    the context(s) of use; Standard measurement and structured human f eedback approaches;
    Anticipated human-AI configurations.'
  hasRelatedRisk:
  - nist-dangerous-violent-or-hateful-content
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Development
- id: MP-1.1-004
  name: MP-1.1-004
  description: Identify and document foreseeable illegal uses or applications of the
    GAI system that surpass organizational risk tolerances.
  hasRelatedRisk:
  - nist-cbrn-information-or-capabilities
  - nist-dangerous-violent-or-hateful-content
  - nist-obscene-degrading-and-or-abusive-content
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Development
- id: MP-1.2-001
  name: MP-1.2-001
  description: Establish and empower interdisciplinary teams that reflect a wide range
    of capabilities, competencies, demographic groups, domain expertise, educational
    backgrounds, lived experiences, professions, and skills across the enterprise
    to inform and conduct risk measurement and management functions.
  hasRelatedRisk:
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Development
- id: MP-1.2-002
  name: MP-1.2-002
  description: Verify that data or benchmarks used in risk measurement, and users,
    participants, or subjects involved in structured GAI public feedback exercises
    are representative of diverse in-context user populations.
  hasRelatedRisk:
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Development
- id: MP-2.1-001
  name: MP-2.1-001
  description: Establish known assumptions and practices for determining data origin
    and content lineage, for documentation and evaluation purposes.
  hasRelatedRisk:
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - TEVV
- id: MP-2.1-002
  name: MP-2.1-002
  description: Institute test and evaluation for data and content flows within the
    GAI system, including but not limited to, original data sources, data transformations,
    and decision-making criteria.
  hasRelatedRisk:
  - nist-data-privacy
  - nist-intellectual-property
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - TEVV
- id: MP-2.2-001
  name: MP-2.2-001
  description: Identify and document how the system relies on upstream data sources,
    including for content provenance, and if it serves as an upstream dependency for
    other systems.
  hasRelatedRisk:
  - nist-information-integrity
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - End Users
- id: MP-2.2-002
  name: MP-2.2-002
  description: Observe and analyze how the GAI system interacts with external networks,
    and identify any potential for negative externalities, particularly where content
    provenance might be compromised.
  hasRelatedRisk:
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - End Users
- id: MP-2.3-001
  name: MP-2.3-001
  description: Assess the accuracy, quality, reliability, and authenticity of GAI
    output by comparing it to a set of known ground truth data and by using a variety
    of evaluation methods (e.g., human oversight and automated evaluation, proven
    cryptographic techniques, review of content inputs ).
  hasRelatedRisk:
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Development
  - Domain Experts
  - TEVV
- id: MP-2.3-002
  name: MP-2.3-002
  description: Review and document accuracy, representativeness, relevance, suitability
    of data used at different stages of AI life cycle.
  hasRelatedRisk:
  - nist-intellectual-property
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Development
  - Domain Experts
  - TEVV
- id: MP-2.3-003
  name: MP-2.3-003
  description: Deploy and document fact-checking techniques to verify the accuracy
    and veracity of information generated by GAI systems, especially when the information
    comes from multiple (or unknown) sources.
  hasRelatedRisk:
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Development
  - Domain Experts
  - TEVV
- id: MP-2.3-004
  name: MP-2.3-004
  description: Develop and implement testing techniques to identify GAI produced content
    (e.g., synthetic media) that might be indistinguishable from human-generated content.
  hasRelatedRisk:
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Development
  - Domain Experts
  - TEVV
- id: MP-2.3-005
  name: MP-2.3-005
  description: Implement plans for GAI systems to undergo regular adversarial testing
    to identify vulnerabilities and potential manipulation or misuse.
  hasRelatedRisk:
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Development
  - Domain Experts
  - TEVV
- id: MP-3.4-001
  name: MP-3.4-001
  description: Evaluate whether GAI operators and end-users can accurately understand
    content lineage and origin.
  hasRelatedRisk:
  - nist-human-ai-configuration
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Design
  - AI Development
  - Domain Experts
  - End-Users
  - Human Factors
  - Operation and Monitoring
- id: MP-3.4-002
  name: MP-3.4-002
  description: Adapt existing training programs to include modules on digital content
    transparency.
  hasRelatedRisk:
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Design
  - AI Development
  - Domain Experts
  - End-Users
  - Human Factors
  - Operation and Monitoring
- id: MP-3.4-003
  name: MP-3.4-003
  description: Develop certification programs that test proficiency in managing GAI
    risks and interpreting content provenance, relevant to specific industry and context.
  hasRelatedRisk:
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Design
  - AI Development
  - Domain Experts
  - End-Users
  - Human Factors
  - Operation and Monitoring
- id: MP-3.4-004
  name: MP-3.4-004
  description: Delineate human proficiency tests from tests of GAI capabilities.
  hasRelatedRisk:
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Design
  - AI Development
  - Domain Experts
  - End-Users
  - Human Factors
  - Operation and Monitoring
- id: MP-3.4-005
  name: MP-3.4-005
  description: Implement systems to continually monitor and track the outcomes of
    human-G AI co nfigurations for future refinement and improvements .
  hasRelatedRisk:
  - nist-human-ai-configuration
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Design
  - AI Development
  - Domain Experts
  - End-Users
  - Human Factors
  - Operation and Monitoring
- id: MP-3.4-006
  name: MP-3.4-006
  description: Involve the end-users, practitioners, and operators in GAI system in
    prototyping and testing activities. Make sure these tests cover various scenarios,
    such as crisis situations or ethically sensitive contexts.
  hasRelatedRisk:
  - nist-dangerous-violent-or-hateful-content
  - nist-human-ai-configuration
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Design
  - AI Development
  - Domain Experts
  - End-Users
  - Human Factors
  - Operation and Monitoring
- id: MP-4.1-001
  name: MP-4.1-001
  description: Conduct periodic monitoring of AI-generated content for privacy risks;
    address any possible instances of PII or sensitive data exposure."
  hasRelatedRisk:
  - nist-data-privacy
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
  - Operation and Monitoring
  - Procurement
  - Third-party entities
- id: MP-4.1-002
  name: MP-4.1-002
  description: Implement processes for responding to potential intellectual property
    infringement claims or other rights."
  hasRelatedRisk:
  - nist-intellectual-property
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
  - Operation and Monitoring
  - Procurement
  - Third-party entities
- id: MP-4.1-003
  name: MP-4.1-003
  description: Connect new GAI policies, procedures, and processes to existing model,
    data, software development, and IT governance and to legal, compliance, and risk
    management activities .
  hasRelatedRisk:
  - nist-data-privacy
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
  - Operation and Monitoring
  - Procurement
  - Third-party entities
- id: MP-4.1-004
  name: MP-4.1-004
  description: Document training data curation policies, to the extent possible and
    according to applicable laws and policies .
  hasRelatedRisk:
  - nist-data-privacy
  - nist-intellectual-property
  - nist-obscene-degrading-and-or-abusive-content
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
  - Operation and Monitoring
  - Procurement
  - Third-party entities
- id: MP-4.1-005
  name: MP-4.1-005
  description: 'Establish policies for collection, retention, and minimum quality
    of data, in consideration of the following risks: Disclosure of inappropriate
    CBRN information ; Use of Illegal or dangerous content; Offensive cyber capabilities;
    Training data imbalances that could give rise to harmful biases ; Leak of personally
    identifiable information, including facial likenesses of individual s.'
  hasRelatedRisk:
  - nist-cbrn-information-or-capabilities
  - nist-dangerous-violent-or-hateful-content
  - nist-data-privacy
  - nist-information-security
  - nist-intellectual-property
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
  - Operation and Monitoring
  - Procurement
  - Third-party entities
- id: MP-4.1-006
  name: MP-4.1-006
  description: Implement policies and practices defining how third-party intellectual
    property and training data will be used, stored, and protected.
  hasRelatedRisk:
  - nist-intellectual-property
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
  - Operation and Monitoring
  - Procurement
  - Third-party entities
- id: MP-4.1-007
  name: MP-4.1-007
  description: Re-evaluate models that were fine-tuned or enhanced on top of third-party
    models."
  hasRelatedRisk:
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
  - Operation and Monitoring
  - Procurement
  - Third-party entities
- id: MP-4.1-008
  name: MP-4.1-008
  description: Re-evaluate risks when adapting GAI models to new domains. Additionally,
    establish warning systems to determine if a GAI system is being used in a new
    domain where previous assumptions (relating to context of use or mapped risk s
    such as security, and safety) may no longer hold.
  hasRelatedRisk:
  - nist-cbrn-information-or-capabilities
  - nist-dangerous-violent-or-hateful-content
  - nist-data-privacy
  - nist-intellectual-property
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
  - Operation and Monitoring
  - Procurement
  - Third-party entities
- id: MP-4.1-009
  name: MP-4.1-009
  description: Leverage approaches to detect the presence of PII or sensitive data
    in generated output text, image, video, or audio .
  hasRelatedRisk:
  - nist-data-privacy
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
  - Operation and Monitoring
  - Procurement
  - Third-party entities
- id: MP-4.1-010
  name: MP-4.1-010
  description: Conduct appropriate diligence on training data use to assess intellectual
    property, and privacy, risks, including to examine whether use of proprietary
    or sensitive training data is consistent with applicable laws.
  hasRelatedRisk:
  - nist-data-privacy
  - nist-intellectual-property
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Governance and Oversight
  - Operation and Monitoring
  - Procurement
  - Third-party entities
- id: MP-5.1-001
  name: MP-5.1-001
  description: Apply TEVV practices for content provenance (e.g., probing a system's
    synthetic data generation capabilities for potential misuse or vulnerabilities
    .
  hasRelatedRisk:
  - nist-information-integrity
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Design
  - AI Development
  - AI Impact Assessment
  - AÃ”Â¨Ã„ected Individuals and Communities
  - End-Users
  - Operation and Monitoring
- id: MP-5.1-002
  name: MP-5.1-002
  description: Identify potential content provenance harms of GAI, such as misinformation
    or disinformation, deepfakes, including NCII, or tampered content. Enumerate and
    rank risks based on their likelihood and potential impact, and determine how well
    provenance solutions address specific risks and/or harms.
  hasRelatedRisk:
  - nist-dangerous-violent-or-hateful-content
  - nist-information-integrity
  - nist-obscene-degrading-and-or-abusive-content
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Design
  - AI Development
  - AI Impact Assessment
  - AÃ”Â¨Ã„ected Individuals and Communities
  - End-Users
  - Operation and Monitoring
- id: MP-5.1-003
  name: MP-5.1-003
  description: Consider disclosing use of GAI to end users in relevant contexts, while
    considering the objective of disclosure, the context of use, the likelihood and
    magnitude of the risk posed, the audience of the disclosure, as well as the frequency
    of the disclosures.
  hasRelatedRisk:
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Design
  - AI Development
  - AI Impact Assessment
  - AÃ”Â¨Ã„ected Individuals and Communities
  - End-Users
  - Operation and Monitoring
- id: MP-5.1-004
  name: MP-5.1-004
  description: Prioritize GAI structured public feedback processes based on risk assessment
    estimates.
  hasRelatedRisk:
  - nist-cbrn-information-or-capabilities
  - nist-dangerous-violent-or-hateful-content
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Design
  - AI Development
  - AI Impact Assessment
  - AÃ”Â¨Ã„ected Individuals and Communities
  - End-Users
  - Operation and Monitoring
- id: MP-5.1-005
  name: MP-5.1-005
  description: Conduct adversarial role-playing exercises, GAI red-teaming, or chaos
    testing to identify anomalous or unforeseen failure modes.
  hasRelatedRisk:
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Design
  - AI Development
  - AI Impact Assessment
  - AÃ”Â¨Ã„ected Individuals and Communities
  - End-Users
  - Operation and Monitoring
- id: MP-5.1-006
  name: MP-5.1-006
  description: Profile threats and negative impacts arising from GAI systems interacting
    with, manipulating, or generating content, and outlining known and potential vulnerabilities
    and the likelihood of their occurrence.
  hasRelatedRisk:
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Design
  - AI Development
  - AI Impact Assessment
  - AÃ”Â¨Ã„ected Individuals and Communities
  - End-Users
  - Operation and Monitoring
- id: MP-5.2-001
  name: MP-5.2-001
  description: Determine context-based measures to identify if new impacts are present
    due to the GAI system, including regular engagements with downstream AI Actors
    to identify and quantify new contexts of unanticipated impacts of GAI systems.
  hasRelatedRisk:
  - nist-human-ai-configuration
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Design
  - AI Impact Assessment
  - AÃ”Â¨Ã„ected Individuals and Communities
  - Domain Experts
  - End-Users
  - Human Factors
  - Operation and Monitoring
- id: MP-5.2-002
  name: MP-5.2-002
  description: Plan regular engagements with AI Actors responsible for inputs to GAI
    systems, including third-party data and algorithms, to review and evaluate unanticipated
    impacts.
  hasRelatedRisk:
  - nist-human-ai-configuration
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Design
  - AI Impact Assessment
  - AÃ”Â¨Ã„ected Individuals and Communities
  - Domain Experts
  - End-Users
  - Human Factors
  - Operation and Monitoring
- id: MS-1.1-001
  name: MS-1.1-001
  description: Employ methods to trace the origin and modifications of digital content
    .
  hasRelatedRisk:
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Development
  - Domain Experts
  - TEVV
- id: MS-1.1-002
  name: MS-1.1-002
  description: Integrate tools designed to analyze content provenance and detect data
    anomalies, verify the authenticity of digital signatures, and identify patterns
    associated with misinformation or manipulation.
  hasRelatedRisk:
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Development
  - Domain Experts
  - TEVV
- id: MS-1.1-003
  name: MS-1.1-003
  description: Disaggregate evaluation metrics by demographic factors to identify
    any discrepancies in how content provenance mechanisms work across diverse populations.
  hasRelatedRisk:
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Development
  - Domain Experts
  - TEVV
- id: MS-1.1-004
  name: MS-1.1-004
  description: Develop a suite of metrics to evaluate structured public feedback exercises
    informed by representative AI Actors.
  hasRelatedRisk:
  - nist-cbrn-information-or-capabilities
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Development
  - Domain Experts
  - TEVV
- id: MS-1.1-005
  name: MS-1.1-005
  description: Evaluate novel methods and technologies for the measurement of G AI-related
    risks in cluding in content provenance, offensive cyber, and CBRN, while maintaining
    the models' ability to produce valid, reliable, and factually accurate outputs.
  hasRelatedRisk:
  - nist-cbrn-information-or-capabilities
  - nist-information-integrity
  - nist-obscene-degrading-and-or-abusive-content
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Development
  - Domain Experts
  - TEVV
- id: MS-1.1-006
  name: MS-1.1-006
  description: Implement continuous monitoring of GAI system impacts to identify whether
    GAI outputs are equitable across various sub-populations. Seek active and direct
    feedback from affected communities via structured feedback mechanisms or red-teaming
    to monitor and improve outputs.
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Development
  - Domain Experts
  - TEVV
- id: MS-1.1-007
  name: MS-1.1-007
  description: Evaluate the quality and integrity of data used in training and the
    provenance of AI-generated content, for example by e mploying techniques like
    chaos engineering and seeking stakeholder feedback.
  hasRelatedRisk:
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Development
  - Domain Experts
  - TEVV
- id: MS-1.1-008
  name: MS-1.1-008
  description: Define use cases, contexts of use, capabilities, and negative impacts
    where structured human feedback exercises, e.g., GAI red-teaming, would be most
    beneficial for GAI risk measurement and management based on the context of use.
  hasRelatedRisk:
  - nist-cbrn-information-or-capabilities
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Development
  - Domain Experts
  - TEVV
- id: MS-1.1-009
  name: MS-1.1-009
  description: Track and document risks or opportunities related to all GAI risks
    that cannot be measured quantitatively, including explanations as to why some
    risks cannot be measured (e.g., due to technological limitations, resource constraints,
    or trustworthy considerations). Include unmeasured risks in marginal risks.
  hasRelatedRisk:
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Development
  - Domain Experts
  - TEVV
- id: MS-1.3-001
  name: MS-1.3-001
  description: Define relevant groups of interest (e.g., demographic groups, subject
    matter experts, experience with GAI technology) within the context of use as part
    of plans for gathering structured public feedback.
  hasRelatedRisk:
  - nist-cbrn-information-or-capabilities
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Development
  - AI Impact Assessment
  - AÃ”Â¨Ã„ected Individuals and Communities
  - Domain Experts
  - End-Users
  - Operation and Monitoring
  - TEVV
- id: MS-1.3-002
  name: MS-1.3-002
  description: Engage in internal and external evaluations, G AI red-teaming, impact
    assessments, or other structured human feedback exercises in consultation with
    representative AI Actors with expertise and familiarity in the context of use,
    and/or who are representative of the populations associated with the context of
    use.
  hasRelatedRisk:
  - nist-cbrn-information-or-capabilities
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Development
  - AI Impact Assessment
  - AÃ”Â¨Ã„ected Individuals and Communities
  - Domain Experts
  - End-Users
  - Operation and Monitoring
  - TEVV
- id: MS-1.3-003
  name: MS-1.3-003
  description: Verify those conducting structured human feedback exercises are not
    directly involved in system development tasks for the same GAI model.
  hasRelatedRisk:
  - nist-data-privacy
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Development
  - AI Impact Assessment
  - AÃ”Â¨Ã„ected Individuals and Communities
  - Domain Experts
  - End-Users
  - Operation and Monitoring
  - TEVV
- id: MS-2.2-001
  name: MS-2.2-001
  description: Assess and manage statistical biases related to GAI content provenance
    through techniques such as re-sampling, re-weighting, or adversarial training.
  hasRelatedRisk:
  - nist-information-integrity
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Development
  - Human Factors
  - TEVV
- id: MS-2.2-002
  name: MS-2.2-002
  description: 'Document how content provenance data is tracked and how that data
    interacts with privacy and security. Consider : Anonymiz ing data to protect the
    privacy of human subjects; Leverag ing privacy output filters; Remov ing any personally
    identifiable information (PII) to prevent potential harm or misuse.'
  hasRelatedRisk:
  - nist-dangerous-violent-or-hateful-content
  - nist-data-privacy
  - nist-information-integrity
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Development
  - Human Factors
  - TEVV
- id: MS-2.2-003
  name: MS-2.2-003
  description: Provide human subjects with options to withdraw participation or revoke
    their consent for present or future use of their data in GAI applications .
  hasRelatedRisk:
  - nist-data-privacy
  - nist-human-ai-configuration
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Development
  - Human Factors
  - TEVV
- id: MS-2.2-004
  name: MS-2.2-004
  description: Use techniques such as anonymization, differential privacy or other
    privacy-enhancing technologies to minimize the risks associated with linking AI-generated
    content back to individual human subjects.
  hasRelatedRisk:
  - nist-data-privacy
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Development
  - Human Factors
  - TEVV
- id: MS-2.3-001
  name: MS-2.3-001
  description: Consider baseline model performance on suites of benchmarks when selecting
    a model for fine tuning or enhancement with retrieval-augmented generation .
  hasRelatedRisk:
  - nist-confabulation
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - TEVV
- id: MS-2.3-002
  name: MS-2.3-002
  description: Evaluate claims of model capabilities using empirically validated methods.
  hasRelatedRisk:
  - nist-confabulation
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - TEVV
- id: MS-2.3-003
  name: MS-2.3-003
  description: Share results of pre-deployment testing with relevant GAI Actors, such
    as those with system release approval authority.
  hasRelatedRisk:
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - TEVV
- id: MS-2.3-004
  name: MS-2.3-004
  description: Utilize a purpose-built testing environment such as NIST Dioptra to
    empirically evaluate GAI trustworthy characteristics.
  hasRelatedRisk:
  - nist-cbrn-information-or-capabilities
  - nist-confabulation
  - nist-dangerous-violent-or-hateful-content
  - nist-data-privacy
  - nist-information-integrity
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - TEVV
- id: MS-2.5-001
  name: MS-2.5-001
  description: Avoid extrapolating GAI system performance or capabilities from narrow,
    non-systematic, and anecdotal assessments.
  hasRelatedRisk:
  - nist-confabulation
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Domain Experts
  - TEVV
- id: MS-2.5-002
  name: MS-2.5-002
  description: Document the extent to which human domain knowledge is employed to
    improve GAI system performance, via, e.g., RLHF, fine-tuning, retrieval-augmented
    generation, content moderation, business rules.
  hasRelatedRisk:
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Domain Experts
  - TEVV
- id: MS-2.5-003
  name: MS-2.5-003
  description: Review and verify sources and citations in GAI system outputs during
    pre deployment risk measurement and ongoing monitoring activities.
  hasRelatedRisk:
  - nist-confabulation
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Domain Experts
  - TEVV
- id: MS-2.5-004
  name: MS-2.5-004
  description: Track and document instances of anthropomorphization (e.g., human images,
    mentions of human feelings, cyborg imagery or motifs) in GAI system interfaces.
  hasRelatedRisk:
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Domain Experts
  - TEVV
- id: MS-2.5-005
  name: MS-2.5-005
  description: Verify GAI system training data and TEVV data provenance, and that
    fine-tuning or retrieval-augmented generation data is grounded.
  hasRelatedRisk:
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Domain Experts
  - TEVV
- id: MS-2.5-006
  name: MS-2.5-006
  description: Regularly review security and safety guardrails, especially if the
    GAI system is being operated in novel circumstances. This includes reviewing reasons
    why the GAI system was initially assessed as being safe to deploy.
  hasRelatedRisk:
  - nist-dangerous-violent-or-hateful-content
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - Domain Experts
  - TEVV
- id: MS-2.6-001
  name: MS-2.6-001
  description: Assess adverse impacts, including health and wellbeing impacts for
    value chain or other AI Actors that are exposed to sexually explicit, offensive,
    or violent information during GAI training and maintenance.
  hasRelatedRisk:
  - nist-dangerous-violent-or-hateful-content
  - nist-human-ai-configuration
  - nist-obscene-degrading-and-or-abusive-content
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Domain Experts
  - Operation and Monitoring
  - TEVV
- id: MS-2.6-002
  name: MS-2.6-002
  description: Assess existence or levels of harmful bias, intellectual property infringement,
    data privacy violations, obscenity, extremism, violence, or CBRN information in
    system training data.
  hasRelatedRisk:
  - nist-cbrn-information-or-capabilities
  - nist-dangerous-violent-or-hateful-content
  - nist-data-privacy
  - nist-intellectual-property
  - nist-obscene-degrading-and-or-abusive-content
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Domain Experts
  - Operation and Monitoring
  - TEVV
- id: MS-2.6-003
  name: MS-2.6-003
  description: Re-evaluate safety features of fine-tuned models when the negative
    risk exceeds organizational risk tolerance.
  hasRelatedRisk:
  - nist-dangerous-violent-or-hateful-content
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Domain Experts
  - Operation and Monitoring
  - TEVV
- id: MS-2.6-004
  name: MS-2.6-004
  description: 'Review GAI system outputs for validity and safety: Review generated
    code to assess risks that may arise from unreliable downstream decision-making.'
  hasRelatedRisk:
  - nist-dangerous-violent-or-hateful-content
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Domain Experts
  - Operation and Monitoring
  - TEVV
- id: MS-2.6-005
  name: MS-2.6-005
  description: Verify that GAI system architecture can monitor outputs and performance,
    and handle, recover from, and repair errors when security anomalies, threats and
    impacts are detected.
  hasRelatedRisk:
  - nist-confabulation
  - nist-information-integrity
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Domain Experts
  - Operation and Monitoring
  - TEVV
- id: MS-2.6-006
  name: MS-2.6-006
  description: Verify that systems properly handle queries that may give rise to inappropriate,
    malicious, or illegal usage, including facilitating manipulation, extortion, targeted
    impersonation, cyber-attacks, and weapons creation.
  hasRelatedRisk:
  - nist-cbrn-information-or-capabilities
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Domain Experts
  - Operation and Monitoring
  - TEVV
- id: MS-2.6-007
  name: MS-2.6-007
  description: Regularly evaluate GAI system vulnerabilities to possible circumventi
    on of safety measures.
  hasRelatedRisk:
  - nist-cbrn-information-or-capabilities
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Domain Experts
  - Operation and Monitoring
  - TEVV
- id: MS-2.7-001
  name: MS-2.7-001
  description: 'Apply established security measures to: Assess likelihood and magnit
    ude of vulnerabilities and threats such as backdoors, compromised dependencies,
    data breaches, eavesdropping, man-in-the-middle attacks, reverse engineering,
    autonomous agents, model theft or exposure of model weights, AI inference, bypass,
    extraction, and other baseline security concerns.'
  hasRelatedRisk:
  - nist-data-privacy
  - nist-information-integrity
  - nist-information-security
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Domain Experts
  - Operation and Monitoring
  - TEVV
- id: MS-2.7-002
  name: MS-2.7-002
  description: Benchmark GAI system security and resilience related to content provenance
    against industry standards and best practices. Compare GAI system security features
    and content provenance methods against industry state-of-the-art.
  hasRelatedRisk:
  - nist-information-integrity
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Domain Experts
  - Operation and Monitoring
  - TEVV
- id: MS-2.7-003
  name: MS-2.7-003
  description: Conduct user surveys to gather user satisfaction with the AI-generated
    content and user perceptions of content authenticity. Analyze user feedback to
    identify concerns and/or current literacy levels related to content provenance
    and understanding of labels on content.
  hasRelatedRisk:
  - nist-human-ai-configuration
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Domain Experts
  - Operation and Monitoring
  - TEVV
- id: MS-2.7-004
  name: MS-2.7-004
  description: Identify metrics that reflect the effectiveness of security measures,
    such as data provenance, the number of unauthorized access attempts, inference,
    bypass, extraction, penetrations, or provenance verification.
  hasRelatedRisk:
  - nist-information-integrity
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Domain Experts
  - Operation and Monitoring
  - TEVV
- id: MS-2.7-005
  name: MS-2.7-005
  description: Measure reliability of content authentication methods, such as watermarking,
    cryptographic signatures, digital fingerprints, as well as access controls, conformity
    assessment, and model integrity verification, which can help support the effective
    implementation of content provenance techniques. Evaluate the rate of false positives
    and false negatives in content provenance, as well as true positives and true
    negatives for verification.
  hasRelatedRisk:
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Domain Experts
  - Operation and Monitoring
  - TEVV
- id: MS-2.7-006
  name: MS-2.7-006
  description: Measure the rate at which recommendations from security checks and
    incidents are implemented. Assess how quickly the AI system can adapt and improve
    based on lessons learned from security incidents and feedback .
  hasRelatedRisk:
  - nist-information-integrity
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Domain Experts
  - Operation and Monitoring
  - TEVV
- id: MS-2.7-007
  name: MS-2.7-007
  description: 'Perform AI red-teaming to assess resilience against: Abuse to facilitate
    attacks on other systems (e.g., malicious code generation, enhanced phishing content),
    GAI attacks (e.g., prompt injection), ML attacks (e.g., adversarial examples/prompts,
    data poisoning, membership inference, model extraction, sponge examples).'
  hasRelatedRisk:
  - nist-dangerous-violent-or-hateful-content
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Domain Experts
  - Operation and Monitoring
  - TEVV
- id: MS-2.7-008
  name: MS-2.7-008
  description: Verify fine-tuning does not compromise safety and security controls.
  hasRelatedRisk:
  - nist-dangerous-violent-or-hateful-content
  - nist-information-integrity
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Domain Experts
  - Operation and Monitoring
  - TEVV
- id: MS-2.7-009
  name: MS-2.7-009
  description: Regularly assess and verify that security measures remain been compromised.
  hasRelatedRisk:
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Domain Experts
  - Operation and Monitoring
  - TEVV
- id: MS-2.8-001
  name: MS-2.8-001
  description: 'Compile statistics on actual policy violations, take-down requests,
    and intellectual property infringement for organizational GAI systems: Analyze
    transparency reports across demographic groups, languages groups .'
  hasRelatedRisk:
  - nist-intellectual-property
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Domain Experts
  - Operation and Monitoring
  - TEVV
- id: MS-2.8-002
  name: MS-2.8-002
  description: Document the instructions given to data annotators or AI red-teamers.
  hasRelatedRisk:
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Domain Experts
  - Operation and Monitoring
  - TEVV
- id: MS-2.8-003
  name: MS-2.8-003
  description: Use digital content transparency solutions to enable the documentation
    of each instance where content is generated, modified, or shared to provide a
    tamper-proof history of the content, promote transparency, and enable traceability.
    Robust version control systems can also be applied to track chang es across the
    AI lifecycle over time.
  hasRelatedRisk:
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Domain Experts
  - Operation and Monitoring
  - TEVV
- id: MS-2.8-004
  name: MS-2.8-004
  description: Verify adequacy of GAI system user instructions through user testing.
  hasRelatedRisk:
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Domain Experts
  - Operation and Monitoring
  - TEVV
- id: MS-2.9-001
  name: MS-2.9-001
  description: 'Apply and document ML explanation results such as: Analysis of embeddings,
    Counterfactual prompts, Gradient-based attributions, Model compression/surrogate
    models, Occlusion/term reduction.'
  hasRelatedRisk:
  - nist-confabulation
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Domain Experts
  - End-Users
  - Operation and Monitoring
  - TEVV
- id: MS-2.9-002
  name: MS-2.9-002
  description: 'Document GAI model details including: Proposed use and organizational
    value; Assumptions and limitations, Data collection methodologies; Data provenance;
    Data quality; Model architecture (e.g., convolutional neural network, transformers,
    etc.); Optimization objectives; Training algorithms; RLHF approaches; Fine-tuning
    or retrieval-augmented generation approaches; Evaluation data; Ethical considerations;
    Legal and regulatory requirements.'
  hasRelatedRisk:
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Domain Experts
  - End-Users
  - Operation and Monitoring
  - TEVV
- id: MS-2.10-001
  name: MS-2.10-001
  description: 'Conduct AI red-teaming to assess issues such as: Outputting of training
    data samples, and subsequent reverse engineering, model extraction, and membership
    inference risks; Revealing biometric, confidential, copyrighted, licensed, patented,
    personal, proprietary, sensitive, or trade-marked information ; Tracking or revealing
    location information of users or members of training datasets. Property'
  hasRelatedRisk:
  - nist-human-ai-configuration
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Domain Experts
  - End-Users
  - Operation and Monitoring
  - TEVV
- id: MS-2.10-002
  name: MS-2.10-002
  description: Engage directly with end-users and other stakeholders to understand
    their expectations and concerns regarding content provenance. Use this feedback
    to guide the design of provenance data-tracking techniques."
  hasRelatedRisk:
  - nist-human-ai-configuration
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Domain Experts
  - End-Users
  - Operation and Monitoring
  - TEVV
- id: MS-2.10-003
  name: MS-2.10-003
  description: Verify deduplication of GAI training data samples, particularly regarding
    synthetic data.
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Domain Experts
  - End-Users
  - Operation and Monitoring
  - TEVV
- id: MS-2.11-001
  name: MS-2.11-001
  description: Apply use-case appropriate benchmarks (e.g., Bias Benchmark Questions,
    Real Hateful or Harmful Prompts, Winogender Schemas 15 ) to quantify systemic
    bias, stereotyping, denigration, and hateful content in GAI system outputs; Document
    assumptions and limitations of benchmarks, including any actual or possible training/test
    data cross contamination, relative to in-context deployment environment.
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Affected Individuals and Communities
  - Domain Experts
  - End-Users
  - Operation and Monitoring
  - TEVV
- id: MS-2.11-002
  name: MS-2.11-002
  description: 'Conduct fairness assessments to measure systemic bias. Measure GAI
    system performance across demographic groups and subgroups, addressing both quality
    of service and any allocation of services and resources. Quantify harms using:
    field testing with sub-gro up populations to determine likelihood of exposure
    to generated content exhibiting harmful bias, AI red-teaming with counterfactual
    and low-context (e.g., ''leader,'' ''bad guys'') prompts. For ML pipelines or
    business processes with categorical or numeric out comes that rely on GAI, apply
    general fairness metrics (e.g., demographic parity, equalized odds, equal opportunity,
    statistical hypothesis tests), to the pipeline or business outcome where appropriate;
    Custom, context-specific metrics developed in collabo ration with domain experts
    and affected communities; Measurements of the prevalence of denigration in generated
    content in deployment (e.g., sub-sampling a fraction of traffic and manually annotating
    denigrating content) .'
  hasRelatedRisk:
  - nist-dangerous-violent-or-hateful-content
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Affected Individuals and Communities
  - Domain Experts
  - End-Users
  - Operation and Monitoring
  - TEVV
- id: MS-2.11-003
  name: MS-2.11-003
  description: Identify the classes of individuals, groups, or environmental ecosystems
    which might be impacted by GAI systems through direct engagement with potentially
    impacted communities.
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Affected Individuals and Communities
  - Domain Experts
  - End-Users
  - Operation and Monitoring
  - TEVV
- id: MS-2.11-004
  name: MS-2.11-004
  description: 'Review, document, and measure sources of bias in GAI training and
    TEVV data: Differences in distributions of outcomes across and within groups,
    including intersecting groups; Completeness, representativeness, and balance of
    data sources; demographic group and subgroup coverage in GAI system training data;
    Fo rms of latent systemic bias in images, text, audio, embeddings, or other complex
    or unstructured data; Input data features that may serve as proxies for demographic
    group membership (i.e., image metadata, language dialect) or otherwise give rise
    to emergent bias within GAI systems; The extent to which the digital divide may
    negatively impact representativeness in GAI system training and TEVV data; Filtering
    of hate speech or content in GAI system training data; Prevalence of GAI-generated
    data in GAI system training data.'
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Affected Individuals and Communities
  - Domain Experts
  - End-Users
  - Operation and Monitoring
  - TEVV
- id: MS-2.11-005
  name: MS-2.11-005
  description: Assess the proportion of synthetic to non-synthetic training data and
    verify training data is not overly homogenous or GAI-produced to mitigate concerns
    of model collapse.
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Affected Individuals and Communities
  - Domain Experts
  - End-Users
  - Operation and Monitoring
  - TEVV
- id: MS-2.12-001
  name: MS-2.12-001
  description: Assess safety to physical environments when deploying GAI systems.
  hasRelatedRisk:
  - nist-dangerous-violent-or-hateful-content
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Domain Experts
  - Operation and Monitoring
  - TEVV
- id: MS-2.12-002
  name: MS-2.12-002
  description: Document anticipated environmental impacts of model development, maintenance,
    and deployment in product design decisions.
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Domain Experts
  - Operation and Monitoring
  - TEVV
- id: MS-2.12-003
  name: MS-2.12-003
  description: 'Measure or estimate environmental impacts (e.g., energy and water
    consumption) for training, fine tuning, and deploying models: Verify tradeoffs
    between resources used at inference time versus additional resources required
    at training time.'
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Domain Experts
  - Operation and Monitoring
  - TEVV
- id: MS-2.12-004
  name: MS-2.12-004
  description: Verify effectiveness of carbon capture or offset programs for GAI training
    and applications, and address green-washing concerns.
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Domain Experts
  - Operation and Monitoring
  - TEVV
- id: MS-2.13-001
  name: MS-2.13-001
  description: 'Create measurement error models for pre-deployment metrics to demonstrate
    construct validity for each metric (i.e., does the metric effectively operationalize
    the desired concept): Measure or estimate, and document, biases or statistical
    variance in applie d metrics or structured human feedback processes; Leverage
    domain expertise when modeling complex societal constructs such as hateful content.'
  hasRelatedRisk:
  - nist-confabulation
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Operation and Monitoring
  - TEVV
- id: MS-3.2-001
  name: MS-3.2-001
  description: Establish processes for identifying emergent GAI system risks including
    consulting with external AI Actors.
  hasRelatedRisk:
  - nist-confabulation
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Impact Assessment
  - Domain Experts
  - Operation and Monitoring
  - TEVV
- id: MS-3.3-001
  name: MS-3.3-001
  description: Conduct impact assessments on how AI-generated content might affect
    different social, economic, and cultural groups.
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AÃ”Â¨Ã„ected Individuals and Communities
  - End-Users
  - Operation and Monitoring
  - TEVV
- id: MS-3.3-002
  name: MS-3.3-002
  description: Conduct studies to understand how end users perceive and interact with
    GAI content and accompanying content provenance within context of use. Assess
    whether the content aligns with their expectations and how they may act upon the
    information presented.
  hasRelatedRisk:
  - nist-human-ai-configuration
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AÃ”Â¨Ã„ected Individuals and Communities
  - End-Users
  - Operation and Monitoring
  - TEVV
- id: MS-3.3-003
  name: MS-3.3-003
  description: Evaluate potential biases and stereotypes that could emerge from the
    AI-generated content using appropriate methodologies including computational testing
    methods as well as evaluating structured feedback input."
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AÃ”Â¨Ã„ected Individuals and Communities
  - End-Users
  - Operation and Monitoring
  - TEVV
- id: MS-3.3-004
  name: MS-3.3-004
  description: Provide input for training materials about the capabilities and limitations
    of GAI systems related to digital content transparency for AI Actors, other professionals,
    and the public about the societal impacts of AI and the role of diverse and inclusive
    content generation.
  hasRelatedRisk:
  - nist-human-ai-configuration
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AÃ”Â¨Ã„ected Individuals and Communities
  - End-Users
  - Operation and Monitoring
  - TEVV
- id: MS-3.3-005
  name: MS-3.3-005
  description: Record and integrate structured feedback about content provenance from
    operators, users, and potentially impacted communities through the use of methods
    such as user research studies, focus groups, or community forums. Actively seek
    feedback on generated content quality and potential biases. Assess the general
    awareness among end users and impacted communities about the availability of these
    feedback channels.
  hasRelatedRisk:
  - nist-human-ai-configuration
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AÃ”Â¨Ã„ected Individuals and Communities
  - End-Users
  - Operation and Monitoring
  - TEVV
- id: MS-4.2-001
  name: MS-4.2-001
  description: Conduct adversarial testing at a regular cadence to map and measure
    GAI risks, including tests to address attempts to deceive or manipulate the application
    of provenance techniques or other misuses. Identify vulnerabilities and understand
    potential misuse scenarios and unintended outputs.
  hasRelatedRisk:
  - nist-information-integrity
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Domain Experts
  - End-Users
  - Operation and Monitoring
  - TEVV
- id: MS-4.2-002
  name: MS-4.2-002
  description: Evaluate GAI system performance in real-world scenarios to observe
    its behavior in practical environments and reveal issues that might not surface
    in controlled and optimized testing environments.
  hasRelatedRisk:
  - nist-confabulation
  - nist-human-ai-configuration
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Domain Experts
  - End-Users
  - Operation and Monitoring
  - TEVV
- id: MS-4.2-003
  name: MS-4.2-003
  description: Implement interpretability and explainability methods to evaluate GAI
    system decisions and verify alignment with intended purpose.
  hasRelatedRisk:
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Domain Experts
  - End-Users
  - Operation and Monitoring
  - TEVV
- id: MS-4.2-004
  name: MS-4.2-004
  description: Monitor and document instances where human operators or other systems
    override the GAI's decisions. Evaluate these cases to understand if the overrides
    are linked to issues related to content provenance."
  hasRelatedRisk:
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Domain Experts
  - End-Users
  - Operation and Monitoring
  - TEVV
- id: MS-4.2-005
  name: MS-4.2-005
  description: Verify and document the incorporation of results of structured public
    feedback exercises into design, implementation, deployment approval ('go'/'no-go'
    decisions), monitoring, and decommission decisions.
  hasRelatedRisk:
  - nist-human-ai-configuration
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Domain Experts
  - End-Users
  - Operation and Monitoring
  - TEVV
- id: MG-1.3-001
  name: MG-1.3-001
  description: 'Document trade-offs, decision processes, and relevant measurement
    and feedback results for risks that do not surpass organizational risk tolerance,
    for example, in the context of model release : Consider different approaches for
    model release, for example, leveraging a staged release approach. Consider release
    approaches in the context of the model and its projected use cases. Mitigate,
    transfer, or avoid risks that surpass organizational risk tolerances.'
  hasRelatedRisk:
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Operation and Monitoring
- id: MG-1.3-002
  name: MG-1.3-002
  description: Monitor the robustness and effectiveness of risk controls and mitigation
    plans (e.g., via red-teaming, field testing, participatory engagements, performance
    assessments, user feedback mechanisms).
  hasRelatedRisk:
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Operation and Monitoring
- id: MG-2.2-001
  name: MG-2.2-001
  description: Compare GAI system outputs against pre-defined organization risk tolerance,
    guidelines, and principles, and review and test AI-generated content against these
    guidelines.
  hasRelatedRisk:
  - nist-cbrn-information-or-capabilities
  - nist-dangerous-violent-or-hateful-content
  - nist-obscene-degrading-and-or-abusive-content
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Governance and Oversight
  - Operation and Monitoring
- id: MG-2.2-002
  name: MG-2.2-002
  description: Document training data sources to trace the origin and provenance of
    AI-generated content.
  hasRelatedRisk:
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Governance and Oversight
  - Operation and Monitoring
- id: MG-2.2-003
  name: MG-2.2-003
  description: Evaluate feedback loops between GAI system content provenance and human
    reviewers, and update where needed. Implement real-time monitoring systems to
    affirm that cont e nt provenance protocols remain effective.
  hasRelatedRisk:
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Governance and Oversight
  - Operation and Monitoring
- id: MG-2.2-004
  name: MG-2.2-004
  description: Evaluate GAI content and data for representational biases and employ
    techniques such as re-sampling, re-ranking, or adversarial training to mitigate
    biases in the generated content.
  hasRelatedRisk:
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Governance and Oversight
  - Operation and Monitoring
- id: MG-2.2-005
  name: MG-2.2-005
  description: Engage in due diligence to analyze GAI output for harmful content,
    potential misinformation, and CBRN-related or NCII content.
  hasRelatedRisk:
  - nist-cbrn-information-or-capabilities
  - nist-dangerous-violent-or-hateful-content
  - nist-obscene-degrading-and-or-abusive-content
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Governance and Oversight
  - Operation and Monitoring
- id: MG-2.2-006
  name: MG-2.2-006
  description: Use feedback from internal and external AI Actors, users, individuals,
    and communities, to assess impact of AI-generated content.
  hasRelatedRisk:
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Governance and Oversight
  - Operation and Monitoring
- id: MG-2.2-007
  name: MG-2.2-007
  description: Use real-time auditing tools where they can be demonstrated to aid
    in the tracking and validation of the lineage and authenticity of AI-generated
    data."
  hasRelatedRisk:
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Governance and Oversight
  - Operation and Monitoring
- id: MG-2.2-008
  name: MG-2.2-008
  description: Use structured feedback mechanisms to solicit and capture user input
    about AI-generated content to detect subtle shifts in quality or alignment with
    community and societal values."
  hasRelatedRisk:
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Governance and Oversight
  - Operation and Monitoring
- id: MG-2.2-009
  name: MG-2.2-009
  description: Consider opportunities to responsibly use synthetic data and other
    privacy enhancing techniques in GAI development, where appropriate and applicable,
    match the statistical properties of real-world data without disclosing personally
    identifiable information or contributing to homogenization .
  hasRelatedRisk:
  - nist-confabulation
  - nist-data-privacy
  - nist-information-integrity
  - nist-intellectual-property
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Impact Assessment
  - Governance and Oversight
  - Operation and Monitoring
- id: MG-2.3-001
  name: MG-2.3-001
  description: 'Develop and update GAI system incident response and recovery plans
    and procedures to address the following: Review and maintenance of policies and
    procedures to account for newly encountered uses; Review and maintenance of policies
    and procedures for detec tion of unanticipated uses; Verify response and recovery
    plans account for the GAI system value chain ; Verify response and recovery plans
    are updated for and include necessary details to communicate with downstream GAI
    system Actors: Points-of-Contact (POC), Contact information, notification format.'
  hasRelatedRisk:
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Operation and Monitoring
- id: MG-2.4-001
  name: MG-2.4-001
  description: Establish and maintain communication plans to inform AI stakeholders
    as part of the deactivation or disengagement process of a specific GAI system
    (including for open-source models) or context of use, including r easons, workarounds,
    user access removal, alternative processes, contact information, etc. Human-
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Governance and Oversight
  - Operation and Monitoring
- id: MG-2.4-002
  name: MG-2.4-002
  description: Establish and maintain procedures for escalating GAI system incidents
    to the organizational risk management authority when specific criteria for deactivation
    or disengagement is met for a particular context of use or for the GAI system
    as a whole.
  hasRelatedRisk:
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Governance and Oversight
  - Operation and Monitoring
- id: MG-2.4-003
  name: MG-2.4-003
  description: Establish and maintain procedures for the remediation of issues which
    trigger incident response processes for the use of a GAI system, and provide stakeholders
    timelines associated with the remediation plan.
  hasRelatedRisk:
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Governance and Oversight
  - Operation and Monitoring
- id: MG-2.4-004
  name: MG-2.4-004
  description: Establish and regularly review specific criteria that warrants the
    deactivation of GAI systems in accordance with set risk tolerances and appetites.
  hasRelatedRisk:
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Governance and Oversight
  - Operation and Monitoring
- id: MG-3.1-001
  name: MG-3.1-001
  description: 'Apply organizational risk tolerances and controls (e.g., acquisition
    and procurement processes; assessing personnel credentials and qualifications,
    performing background checks; filtering GAI input and outputs, grounding, fine
    tuning, retrieval-augmented generation) to third-party GAI resources: Apply organizational
    risk tolerance to the utilization of third-party datasets and other GAI resources;
    Apply organizational risk tolerances to fine-tuned third-party models; Apply organizational
    risk tolerance to existing t hird-party models adapted to a new domain; Reassess
    risk measurements after fine-tuning third-party GAI models.'
  hasRelatedRisk:
  - nist-intellectual-property
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Operation and Monitoring
  - Third-party entities
- id: MG-3.1-002
  name: MG-3.1-002
  description: Test GAI system value chain risks (e.g., data poisoning, malware, other
    software and hardware vulnerabilities; labor practices; data privacy and localization
    compliance; geopolitical alignment).
  hasRelatedRisk:
  - nist-data-privacy
  - nist-information-security
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Operation and Monitoring
  - Third-party entities
- id: MG-3.1-003
  name: MG-3.1-003
  description: Re-assess model risks after fine-tuning or retrieval-augmented generation
    implementation and for any third-party GAI models deployed for applications and/or
    use cases that were not evaluated in initial testing.
  hasRelatedRisk:
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Operation and Monitoring
  - Third-party entities
- id: MG-3.1-004
  name: MG-3.1-004
  description: Take reasonable measures to review training data for CBRN information,
    and intellectual property, and where appropriate, remove it. Implement reasonable
    measures to prevent, flag, or take other action in response to outputs that reproduce
    particular training data (e.g., plagiarized, trademarked, patented, licensed content
    or trade secret material ).
  hasRelatedRisk:
  - nist-cbrn-information-or-capabilities
  - nist-intellectual-property
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Operation and Monitoring
  - Third-party entities
- id: MG-3.1-005
  name: MG-3.1-005
  description: Review various transparency artifacts (e.g., system cards and model
    cards) for third-party models.
  hasRelatedRisk:
  - nist-information-integrity
  - nist-information-security
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Operation and Monitoring
  - Third-party entities
- id: MG-3.2-001
  name: MG-3.2-001
  description: Apply explainable AI (XAI) techniques (e.g., analysis of embeddings,
    model compression/distillation, gradient-based attributions, occlusion/term reduction,
    counterfactual prompts, word clouds) as part of ongoing continuous improvement
    processes to mitigate risks related to unexplainable GAI systems.
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Operation and Monitoring
  - Third-party entities
- id: MG-3.2-002
  name: MG-3.2-002
  description: Document how pre-trained models have been adapted (e.g., fine-tuned,
    or retrieval-augmented generation) for the specific generative task, including
    any data augmentations, parameter adjustments, or other modifications. Access
    to un-tuned (baseline) models support s debugging the relative influence of the
    pre-trained weights compared to the fine-tuned model weights or other system updates.
  hasRelatedRisk:
  - nist-data-privacy
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Operation and Monitoring
  - Third-party entities
- id: MG-3.2-003
  name: MG-3.2-003
  description: Document sources and types of training data and their origins, potential
    biases present in the data related to the GAI application and its content provenance,
    architecture, training process of the pre-trained model including information
    on hyperparameters, training duration, and any fine-tuning or retrieval-augmented
    generation processes applied.
  hasRelatedRisk:
  - nist-information-integrity
  - nist-intellectual-property
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Operation and Monitoring
  - Third-party entities
- id: MG-3.2-004
  name: MG-3.2-004
  description: Evaluate user reported problematic content and integrate feedback into
    system updates."
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Operation and Monitoring
  - Third-party entities
- id: MG-3.2-005
  name: MG-3.2-005
  description: Implement content filters to prevent the generation of inappropriate,
    harmful, false, illegal, or violent content related to the GAI application, including
    for CSAM and NCII. These filters can be rule-based or leverage additional machine
    learning models to flag problematic inputs and outputs.
  hasRelatedRisk:
  - nist-dangerous-violent-or-hateful-content
  - nist-information-integrity
  - nist-obscene-degrading-and-or-abusive-content
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Operation and Monitoring
  - Third-party entities
- id: MG-3.2-006
  name: MG-3.2-006
  description: Implement real-time monitoring processes for analyzing generated content
    performance and trustworthiness characteristics related to content provenance
    to identify deviations from the desired standards and trigger alerts for human
    intervention.
  hasRelatedRisk:
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Operation and Monitoring
  - Third-party entities
- id: MG-3.2-007
  name: MG-3.2-007
  description: Leverage feedback and recommendations from organizational boards or
    committees related to the deployment of GAI applications and content provenance
    when using third-party pre-trained models.
  hasRelatedRisk:
  - nist-information-integrity
  - nist-value-chain-and-component-integration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Operation and Monitoring
  - Third-party entities
- id: MG-3.2-008
  name: MG-3.2-008
  description: Use human moderation systems where appropriate to review generated
    content in accordance with human-AI configuration policies established in the
    Govern function, aligned with socio-cultural norms in the context of use, and
    for settings where AI models are demonstrated to perform poorly.
  hasRelatedRisk:
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Operation and Monitoring
  - Third-party entities
- id: MG-3.2-009
  name: MG-3.2-009
  description: Use organizational risk tolerance to evaluate acceptable risks and
    performance metrics and decommission or retrain pre-trained models that perform
    outside of defined limits.
  hasRelatedRisk:
  - nist-cbrn-information-or-capabilities
  - nist-confabulation
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Operation and Monitoring
  - Third-party entities
- id: MG-4.1-001
  name: MG-4.1-001
  description: Collaborate with external researchers, industry experts, and community
    representatives to maintain awareness of emerging best practices and technologies
    in measuring and managing identified risks.
  hasRelatedRisk:
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Affected Individuals and Communities
  - Domain Experts
  - End-Users
  - Human Factors
  - Operation and Monitoring
- id: MG-4.1-002
  name: MG-4.1-002
  description: Establish, maintain, and evaluate effectiveness of organizational processes
    and procedures for post-deployment monitoring of GAI systems, particularly for
    potential confabulation, CBRN, or cyber risks.
  hasRelatedRisk:
  - nist-cbrn-information-or-capabilities
  - nist-confabulation
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Affected Individuals and Communities
  - Domain Experts
  - End-Users
  - Human Factors
  - Operation and Monitoring
- id: MG-4.1-003
  name: MG-4.1-003
  description: Evaluate the use of sentiment analysis to gauge user sentiment regarding
    GAI content performance and impact, and work in collaboration with AI Actors experienced
    in user research and experience.
  hasRelatedRisk:
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Affected Individuals and Communities
  - Domain Experts
  - End-Users
  - Human Factors
  - Operation and Monitoring
- id: MG-4.1-004
  name: MG-4.1-004
  description: Implement active learning techniques to identify instances where the
    model fails or produces unexpected outputs.
  hasRelatedRisk:
  - nist-confabulation
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Affected Individuals and Communities
  - Domain Experts
  - End-Users
  - Human Factors
  - Operation and Monitoring
- id: MG-4.1-005
  name: MG-4.1-005
  description: Share transparency reports with internal and external stakeholders
    that detail steps taken to update the G AI system to enhance transparency and
    accountability.
  hasRelatedRisk:
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Affected Individuals and Communities
  - Domain Experts
  - End-Users
  - Human Factors
  - Operation and Monitoring
- id: MG-4.1-006
  name: MG-4.1-006
  description: Track dataset modifications for provenance by monitoring data deletions,
    rectification requests, and other changes that may impact the verifiability of
    content origins.
  hasRelatedRisk:
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Affected Individuals and Communities
  - Domain Experts
  - End-Users
  - Human Factors
  - Operation and Monitoring
- id: MG-4.1-007
  name: MG-4.1-007
  description: Verify that AI Actors responsible for monitoring reported issues can
    effectively evaluate GAI system performance including the application of content
    provenance data tracking techniques, and promptly escalate issues for response.
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Affected Individuals and Communities
  - Domain Experts
  - End-Users
  - Human Factors
  - Operation and Monitoring
- id: MG-4.2-001
  name: MG-4.2-001
  description: Conduct regular monitoring of GAI systems and publish reports detailing
    the performance, feedback received, and improvements made.
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Design
  - AI Development
  - AÃ”Â¨Ã„ected Individuals and Communities
  - End-Users
  - Operation and Monitoring
  - TEVV
- id: MG-4.2-002
  name: MG-4.2-002
  description: Practice and follow incident response plans for addressing the generation
    of inappropriate or harmful content and adapt processes based on findings to prevent
    future occurrences. Conduct post-mortem analyses of incidents with relevant AI
    Actors, to understand the root causes and implement preventive measures.
  hasRelatedRisk:
  - nist-dangerous-violent-or-hateful-content
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Design
  - AI Development
  - AÃ”Â¨Ã„ected Individuals and Communities
  - End-Users
  - Operation and Monitoring
  - TEVV
- id: MG-4.2-003
  name: MG-4.2-003
  description: Use visualizations or other methods to represent GAI model behavior
    to ease non-technical stakeholders understanding of GAI system functionality.
  hasRelatedRisk:
  - nist-human-ai-configuration
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - AI Design
  - AI Development
  - AÃ”Â¨Ã„ected Individuals and Communities
  - End-Users
  - Operation and Monitoring
  - TEVV
- id: MG-4.3-001
  name: MG-4.3-001
  description: Conduct after-action assessments for GAI system incidents to verify
    incident response and recovery processes are followed and effective, including
    to follow procedures for communicating incidents to relevant AI Actors and where
    applicable, relevant legal and regulatory bodies.
  hasRelatedRisk:
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Affected Individuals and Communities
  - Domain Experts
  - End-Users
  - Human Factors
  - Operation and Monitoring
- id: MG-4.3-002
  name: MG-4.3-002
  description: Establish and maintain policies and procedures to record and track
    GAI system reported errors, near-misses, and negative impacts.
  hasRelatedRisk:
  - nist-confabulation
  - nist-information-integrity
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Affected Individuals and Communities
  - Domain Experts
  - End-Users
  - Human Factors
  - Operation and Monitoring
- id: MG-4.3-003
  name: MG-4.3-003
  description: Report GAI incidents in compliance with legal and regulatory requirements
    (e.g., HIPAA breach reporting, e.g., OCR (2023) or NHTSA (2022) autonomous vehicle
    crash reporting requirements.
  hasRelatedRisk:
  - nist-data-privacy
  - nist-information-security
  hasDocumentation:
  - NIST.AI.600-1
  aiActorTask:
  - AI Deployment
  - Affected Individuals and Communities
  - Domain Experts
  - End-Users
  - Human Factors
  - Operation and Monitoring
evaluations:
- id: stanford-fmti
  name: The Foundation Model Transparency Index
  description: The Foundation Model Transparency Index is an ongoing initiative to
    comprehensively assess the transparency of foundation model developers.
  url: https://crfm.stanford.edu/fmti/
  hasDocumentation:
  - arxiv.org/2310.12941
  hasRelatedRisk:
  - atlas-lack-of-model-transparency
  - atlas-data-transparency
  - atlas-data-provenance
- id: cards.value_alignment.hallucinations.truthfulqa
  name: TruthfulQA
  description: TruthfulQA is a benchmark to measure whether a language model is truthful
    in generating answers to questions.
  url: https://github.com/sylinrl/TruthfulQA
  hasDocumentation:
  - arxiv.org/2109.07958
  hasDataset:
  - truthfulqa/truthful_qa
  hasUnitxtCard: cards.value_alignment.hallucinations.truthfulqa
  hasRelatedRisk:
  - atlas-hallucination
aimodelfamilies:
- id: ibm-granite
  name: IBM Granite
  description: IBM is building enterprise-focused foundation models to drive the future
    of business. The Granite family of foundation models span a variety of modalities,
    including language, code, and other modalities, such as time series.
  url: https://huggingface.co/ibm-granite
  hasDocumentation:
  - granite-3.0-paper
aimodels:
- id: granite-3.0-2b-base
  name: Granite-3.0-2B-Base
  description: Granite-3.0-2B-Base is a decoder-only language model to support a variety
    of text-to-text generation tasks.
  url: https://github.com/ibm-granite/granite-3.0-language-models
  dateCreated: 2024-10-21
  hasModelCard:
  - https://huggingface.co/ibm-granite/granite-3.0-2b-instruct
  - https://www.ibm.com/docs/en/watsonx/w-and-w/2.1.x?topic=models-granite-30-2b-instruct-model-card
  hasDocumentation:
  - granite-3.0-paper
  hasLicense: license-apache-2.0
  performsTask:
  - question-answering
  - summarization
  - text-classification
  - text-generation
  - code-generation
  - code-explanation
  - code-editing
  isProvidedBy: ibm
  hasEvaluation:
  - id: truthfulqa-granite-3-2b-instruct
    name: TruthfulQA result for Granite-3.0-2B-Instruct
    description: Result of the TruthfulQA evaluation for the IBM Granite-3.0-2B-Instruct
      model.
    dateCreated: 2024-10-21
    value: '53.37'
    evidence: https://github.com/ibm-granite/granite-3.0-language-models/blob/main/paper.pdf,
      Table 10
    isResultOf: cards.value_alignment.hallucinations.truthfulqa
  architecture: Decoder-only
  carbon_emitted: 68.1
  numParameters: 2500000000
  numTrainingTokens: 12000000000000
  contextWindowSize: 4094
  hasInputModality:
  - modality-text
  hasOutputModality:
  - modality-text
  isPartOf: ibm-granite
- id: granite-3.0-8b-base
  name: Granite-3.0-8B-Base
  description: Granite-3.0-8B-Base is a decoder-only language model to support a variety
    of text-to-text generation tasks.
  url: https://github.com/ibm-granite/granite-3.0-language-models
  dateCreated: 2024-10-21
  hasModelCard:
  - https://huggingface.co/ibm-granite/granite-3.0-8b-instruct
  - https://www.ibm.com/docs/en/watsonx/w-and-w/2.1.x?topic=models-granite-30-8b-instruct-model-card,
    https://build.nvidia.com/ibm/granite-3_0-8b-instruct
  hasDocumentation:
  - granite-3.0-paper
  hasLicense: license-apache-2.0
  performsTask:
  - question-answering
  - summarization
  - text-classification
  - text-generation
  - code-generation
  - code-explanation
  - code-editing
  isProvidedBy: ibm
  hasEvaluation:
  - id: truthfulqa-granite-3-8b-instruct
    name: TruthfulQA result for Granite-3.0-8B-Instruct
    description: Result of the TruthfulQA evaluation for the IBM Granite-3.0-8B-Instruct
      model.
    dateCreated: 2024-10-21
    value: '60.32'
    evidence: https://github.com/ibm-granite/granite-3.0-language-models/blob/main/paper.pdf,
      Table 10
    isResultOf: cards.value_alignment.hallucinations.truthfulqa
  architecture: Decoder-only
  carbon_emitted: 295.2
  numParameters: 8100000000
  numTrainingTokens: 12000000000000
  contextWindowSize: 4094
  hasInputModality:
  - modality-text
  hasOutputModality:
  - modality-text
  isPartOf: ibm-granite

