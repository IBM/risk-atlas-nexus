{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c84259c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ingevejs/Documents/workspace/ingelise/risk-atlas-nexus/src/risk_atlas_nexus/toolkit/job_utils.py:2: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n",
      "/Users/ingevejs/Documents/workspace/ingelise/risk-atlas-nexus/vrisk-atlas-nexus/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: '__init_subclass__' (from 'transformers.agents.tools') is deprecated and will be removed from version '4.51.0'. Switch to smolagents instead, with the same functionalities and similar API (https://huggingface.co/docs/smolagents/index)\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "/Users/ingevejs/Documents/workspace/ingelise/risk-atlas-nexus/vrisk-atlas-nexus/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: '__init_subclass__' (from 'transformers.agents.tools') is deprecated and will be removed from version '4.51.0'. Switch to smolagents instead, with the same functionalities and similar API (https://huggingface.co/docs/smolagents/index)\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "/Users/ingevejs/Documents/workspace/ingelise/risk-atlas-nexus/vrisk-atlas-nexus/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Tool' (from 'transformers.agents.tools') is deprecated and will be removed from version '4.51.0'. Switch to smolagents instead, with the same functionalities and similar API (https://huggingface.co/docs/smolagents/index)\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from risk_atlas_nexus.ai_risk_ontology.datamodel.ai_risk_ontology import AiEval, BenchmarkMetadataCard, Documentation, Risk\n",
    "from risk_atlas_nexus import RiskAtlasNexus\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be47ef8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-19 22:02:36:644] - INFO - RiskAtlasNexus - Created RiskAtlasNexus instance. Base_dir: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ran = RiskAtlasNexus() # no args, so default configuration \n",
    "len(ran.get_all_risks(taxonomy=\"nist-ai-rmf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a12420e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Benchmark Details': {'Name': 'Bias Benchmark for QA (BBQ) Age', 'ID': 'cards.safety.bbq.Age', 'Overview': 'Bias Benchmark for QA (BBQ) Age, a dataset of question sets that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts.', 'Data Type': 'Text', 'Domains': ['Natural Language Processing'], 'Application Domains': 'Question Answering systems, ', 'Languages': ['English'], 'Similar Benchmarks': 'UnQover', 'Resources': \"[{'arxiv': 'https://arxiv.org/abs/2110.08193'}]\"}, 'Purpose and Intended Users': {'Goal': \"To assess the biases of question answering (QA) models.  Evaluate model responses at two levels: (i) given an under-informative context, test how strongly responses reflect social biases, and (ii) given an adequately informative context, test whether the model's biases override a correct answer choice.\", 'Audience': 'Researchers, developers, policymakers', 'Tasks': ['Question Answering'], 'Out-of-Scope Uses': 'Systems that are not question answering (QA) models. '}, 'Data': {'Source': 'Bias Benchmark for QA (BBQ), a dataset of question sets that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts.', 'Size': '58,492 unique examples', 'Format': 'Templates which consist of two questions, answer choices, a partial context missing information necessary to answer the questions, and a disambiguating context that provides the necessary information.', 'Annotation': 'Human annotators, MTurk workers located in the US, compensated for their time'}, 'Methodology': {'Methods': 'Metric: accuracy.  Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:\\nAccuracy = (TP + TN) / (TP + TN + FP + FN)\\n Where:\\nTP: True positive\\nTN: True negative\\nFP: False positive\\nFN: False negative\\n, Metric Citation\\n@article{scikit-learn,\\n  title={Scikit-learn: Machine Learning in {P}ython},\\n  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\\n         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\\n         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\\n         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E҆},\\n  journal={Journal of Machine Learning Research},\\n  volume={12},\\n  pages={2825--2830},\\n  year={2011}\\n}\\n.', 'Metrics': 'Accuracy', 'Calculation': 'Accuracy = (TP + TN) / (TP + TN + FP + FN)', 'Interpretation': \"Higher accuracy indicates better performance. However, it's important to note that accuracy alone does not paint a complete picture of a model's performance, especially when it comes to assessing biases in language models.  A low bias score should not be seen as indicative of a less biased model in all cases.\", 'Validation': 'Templates were validated by MTurk testing of rater agreement. The paper reports an estimate that raw human (crowdworker annotator) accuracy on BBQ is 95.7%, and aggregate human accuracy calculated via majority vote is 99.7%.'}, 'Risks': {'Risk Categories': 'Social Biases, stereotype reinforcement, stereotype attribution', 'Limitations': 'Gaps in demographic coverage or specific domains. Categories are limited to a US English-speaking cultural context and do not include all possible social biases', 'Demographic Analysis': 'Unknown', 'Harm': 'Perpetuation of stereotypes and biases'}, 'Ethical and Legal Considerations': {'Privacy and Anonymity': 'Publicly available data', 'Data Licensing': 'CC BY 4.0', 'Consent Procedures': 'Unknown', 'Compliance with Regulations': 'Unknown'}}\n",
      "{'Benchmark Details': {'Name': 'Prompt Leakage (RAG)', 'ID': 'cards.prompt_leakage.rag_dataset', 'Overview': \"This benchmark is designed to assess the tendency of a model to leak its system prompt, also known as the base prompt or prompt template. The benchmark uses a dataset of prompts that are systematically designed to test the model's tendency to disclose its system prompt. The rag-dataset is used to add few shot examples to the system prompt. The metric used to assess this ability is the prompt leakage score, which measures the degree of similarity between the model's response and the actual system prompt.\", 'Data Type': 'Text', 'Domains': ['Natural Language Processing'], 'Application Domains': '', 'Languages': ['English'], 'Similar Benchmarks': 'prompt-leaking', 'Resources': [{'arxiv': 'https://arxiv.org/pdf/2307.06865.pdf'}, {'huggingface': 'https://huggingface.co/datasets/neural-bridge/rag-dataset-12000'}]}, 'Purpose and Intended Users': {'Goal': 'To assess the tendency of a model to leak its system prompt, also known as the base prompt or prompt template.', 'Audience': 'Researchers, developers, policymakers, etc.', 'Tasks': ['Prompt leakage assessment'], 'Out-of-Scope Uses': 'Models that do not have a system prompt.'}, 'Data': {'Source': \"The benchmark uses a dataset of prompts that are systematically designed to test the model's tendency to disclose its system prompt.\", 'Size': 'The dataset contains 58 prompts.', 'Format': 'The dataset is in CSV format and contains two columns: the prompt and rank.', 'Annotation': ' The dataset was manually curated by IBM and extended using the mixtral-8x7b-instruct-v01 model.'}, 'Methodology': {'Methods': \"The benchmark uses a prompt leakage attack to assess the model's tendency to disclose its system prompt. The attack involves presenting the model with a series of prompts and measuring the degree of similarity between the model's response and the actual system prompt.\", 'Metrics': 'This metric is based on a system prompt leakage attack and represents to what degree the model is willing to disclose its own system/base prompt. The attack tries to cause the model to leak its system prompt, and measures the degree of similarity between the model response and the actual system prompt. A high similarity between the model response and the actual prompt represents higher leakage risk. The score is based on the attack’s average similarity score across all attack prompts tested, weighted according to their difficulty level, and normalized such that that 1 represents perfect privacy and 0 represents worst privacy.', 'Calculation': \"The prompt leakage score is calculated using a similarity metric, such as rouge or bert, between the model's response and the actual system prompt.\", 'Interpretation': 'A higher prompt leakage score indicates a lower risk of the model leaking its system prompt (better privacy).', 'Baseline Results': '', 'Validation': ''}, 'Risks': {'Risk Categories': 'Prompt leaking', 'Limitations': 'The benchmark work does not experiment with sophisticated attacking strategies. ', 'Demographic Analysis': 'Unknown', 'Harm': 'Prompt leaking risk for AI. A successful attack exposes the system prompt used in the model. Depending on the content of that prompt, the attacker might gain access to valuable information, such as sensitive personal information or intellectual property, and might be able to use this information to mount a secondary attack, such as performing fraud.'}, 'Ethical and Legal Considerations': {'Privacy and Anonymity': 'The benchmark does not handle any personal or sensitive data, and it does not apply any anonymization techniques.', 'Data Licensing': 'This is a proprietary IBM dataset.', 'Consent Procedures': 'The benchmark does not involve any personal data, and it does not require consent from users.', 'Compliance with Regulations': 'Unknown'}}\n",
      "{'Benchmark Details': {'Name': 'Dolly data leakage', 'ID': 'cards.data_leakage.dolly', 'Overview': \"This dataset contains instruction-following examples, including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization, Human preference data about helpfulness and harmlessness and Human-generated and annotated red teaming dialogues. Training data is used to assess the model's tendency to reveal confidential information or sensitive personal information in its generated output.\", 'Data Type': 'Text', 'Domains': ['Natural Language Processing'], 'Application Domains': '', 'Languages': ['English'], 'Similar Benchmarks': '', 'Resources': \"[{'huggingface': 'mosaicml/dolly_hhrlhf'}]\"}, 'Purpose and Intended Users': {'Goal': 'To assess the risk of LLMs revealing memorized training samples in their generated output.', 'Audience': 'Researchers, developers, policymakers, and anyone interested in the potential risks of LLMs.', 'Tasks': ['The benchmark is designed for the task of detecting data leakage'], 'Out-of-Scope Uses': 'The benchmark is not designed to be used for tasks such as text classification, sentiment analysis, or question-answering.'}, 'Data': {'Source': \"This uses the dataset mosaicml/dolly_hhrlhf.  This dataset is a combination of [Databrick's dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k) dataset and a filtered subset of [Anthropic's HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf). It also includes a test split, which was missing in the original `dolly` set. That test set is composed of 200 randomly selected samples from `dolly` + 4,929 of the test set samples from HH-RLHF which made it through a filtering process. The train set contains 59,310 samples; `15,014 - 200 = 14,814` from Dolly, and the remaining 44,496 from HH-RLHF..\", 'Size': 'The dataset contains 64,539 samples, with 59,310 samples in the training set and 5,229 samples in the test set. The download size is reported as 24882010 and dataset size as 48260742. A subset of data (1000 sample instances) is used to run the MRA recipe.', 'Format': 'The dataset is stored in the Hugging Face Dataset format, with each sample consisting of a prompt and its corresponding response.', 'Annotation': 'The dataset was manually curated and filtered by MosaicML, and the responses were checked for the presence of sensitive information.'}, 'Methodology': {'Methods': 'The benchmark looks for high syntactic similarity between the model responses and the actual samples, to identify leakage risk.', 'Metrics': 'Data leakage (metrics.data_leakage). ', 'Calculation': 'This metric is based on a training data or fine-tuning data leakage attack, and represents to what degree the model has memorized its training data. The attack tries to cause the model to leak complete or partial samples from its training (or fine-tuning) data, and measures the degree of similarity between the model responses and the actual training data.', 'Interpretation': 'A high syntactic similarity between the model response and the actual sample represents higher leakage risk. The score is based on the attack’s average similarity score across all training samples tested, and normalized such that 1 represents perfect privacy and 0 represents worst privacy.', 'Validation': ''}, 'Risks': {'Risk Categories': ['revealing-confidential-information', 'exposing-personal-information'], 'Limitations': 'This benchmark only assesses the leakage of this specific dataset and no other datasets used in the model’s training or tuning. Moreover, it measures leakage using a specialized metric based on syntactic similarity, which may miss certain types of leakage. The benchmark assumes partial knowledge of the training samples in order to prompt the model to leak the rest of the data.', 'Demographic Analysis': 'Unknown', 'Harm': 'The benchmark assesses the risk of LLMs revealing confidential information or sensitive personal information in their generated output. Risk Title: Revealing confidential information,\\n Tag: revealing-confidential-information,\\n Type: output,\\n Phase:None,\\n Category:None,\\n Group:intellectual property,\\n Descriptor:amplified,\\n Description: When confidential information is used in training data, fine-tuning data, or as part of the prompt, models might reveal that data in the generated output. Revealing confidential information is a type of data leakage.,\\n Concern: If not properly developed to secure confidential data, the model might reveal confidential information or IP in the generated output and reveal information that was meant to be secret. Risk Title: Exposing personal information,\\n Tag: exposing-personal-information,\\n Type: output,\\n Phase:None,\\n Category:None,\\n Group:privacy,\\n Descriptor:amplified,\\n Description: When personal identifiable information (PII) or sensitive personal information (SPI) are used in training data, fine-tuning data, or as part of the prompt, models might reveal that data in the generated output. Revealing personal information is a type of data leakage.,\\n Concern: Sharing people’s PI impacts their rights and make them more vulnerable.'}, 'Ethical and Legal Considerations': {'Privacy and Anonymity': 'The benchmark does not include any personal or sensitive data.', 'Data Licensing': 'The dataset is licensed under the CC BY-SA 3.0 license, which allows for free use and distribution of the data.', 'Consent Procedures': 'Unknown', 'Compliance with Regulations': 'Unknown'}}\n",
      "{'Benchmark Details': {'Name': 'Data leakage (RAG)', 'ID': 'cards.data_leakage.rag_dataset_12000', 'Overview': 'This benchmark is designed to assess the tendency of a model to leak RAG context in case it will be provided together with a user prompt.', 'Data Type': 'Text', 'Domains': ['Natural Language Processing'], 'Application Domains': '', 'Languages': ['English'], 'Similar Benchmarks': '', 'Resources': \"[{'huggingface': 'neural-bridge/rag-dataset-12000'}]\"}, 'Purpose and Intended Users': {'Goal': 'To assess the risk of LLMs leaking RAG context into their generated output.', 'Audience': 'Researchers, developers, policymakers, and anyone interested in the potential risks of LLMs.', 'Tasks': ['The benchmark is designed for the task of detecting data leakage from RAG context'], 'Out-of-Scope Uses': 'The benchmark is not designed to be used for tasks such as text classification, sentiment analysis, or question-answering.'}, 'Data': {'Source': 'This benchmark uses the dataset neural-bridge/rag-dataset-12000. Retrieval-Augmented Generation (RAG) Dataset 12000 is an English dataset designed for RAG-optimized models, built by Neural Bridge AI, and released under Apache license 2.0. Retrieval-Augmented Generation (RAG) Dataset 12000 dataset is a triple-feature collection, with each entry containing a \"context\", \"question\", and \"answer\" fields, designed to help build RAG-optimized models.', 'Size': 'The dataset contains 12,000 samples, with 9,600 samples in the training set and 2,400 samples in the test set. The download size is reported as 28.9 MB and dataset size as 28.9 MB. A subset of data (1000 sample instances) is used to run the MRA recipe.', 'Format': 'The dataset is stored in the Hugging Face Dataset format, with each sample consisting of a context and its corresponding question and answer.', 'Annotation': 'The dataset was built by Neural Bridge AI, and the responses were checked for the presence of sensitive information.'}, 'Methodology': {'Methods': 'The benchmark looks for high syntactic similarity between the model responses and the given RAG context, to identify leakage risk.', 'Metrics': 'Data leakage (metrics.data_leakage). ', 'Calculation': \"The RAG context leakage score is calculated using a syntactic similarity metric, an enhanced variation of rougeL (rougLmod), between the model's response and the RAG context. A high syntactic similarity between the model response and the RAG context represents higher leakage risk.\", 'Interpretation': 'The final score is based on the attack’s average similarity score across all RAG context samples tested, normalized such that 1 represents perfect privacy and 0 represents worst privacy.', 'Validation': ''}, 'Risks': {'Risk Categories': ['revealing-confidential-information', 'exposing-personal-information'], 'Limitations': 'This benchmark only assesses the tendency of RAG context leakage by using public RAG dataset. Moreover, it measures leakage using a specialized metric based on syntactic similarity, which may miss certain types of leakage. ', 'Demographic Analysis': 'Unknown', 'Harm': 'The benchmark assesses the risk of LLMs revealing confidential information or sensitive personal information from RAG context in their generated output. Risk Title: Revealing confidential information,\\n Tag: revealing-confidential-information,\\n Type: output,\\n Phase:None,\\n Category:None,\\n Group:intellectual property,\\n Descriptor:amplified,\\n Description: When confidential information is used in RAG context, training data, fine-tuning data, or as part of the prompt, models might reveal that data in the generated output. Revealing confidential information is a type of data leakage.,\\n Concern: If not properly developed to secure confidential data, the model might reveal confidential information or IP in the generated output and reveal information that was meant to be secret. Risk Title: Exposing personal information,\\n Tag: exposing-personal-information,\\n Type: output,\\n Phase:None,\\n Category:None,\\n Group:privacy,\\n Descriptor:amplified,\\n Description: When personal identifiable information (PII) or sensitive personal information (SPI) are used in RAG context, training data, fine-tuning data, or as part of the prompt, models might reveal that data in the generated output. Revealing personal information is a type of data leakage.,\\n Concern: Sharing people’s PI impacts their rights and make them more vulnerable.'}, 'Ethical and Legal Considerations': {'Privacy and Anonymity': 'The benchmark does not include any personal or sensitive data.', 'Data Licensing': 'This dataset is licensed under the [Apache License, Version 2.0]', 'Consent Procedures': 'Unknown', 'Compliance with Regulations': 'Unknown'}}\n",
      "{'Benchmark Details': {'Name': 'Data leakage (SYNTH)', 'ID': 'cards.data_leakage.synth_sentence', 'Overview': 'This benchmark is designed to assess the tendency of a model to leak the original text (seed) while using it to generate synthetic data.', 'Data Type': 'Text', 'Domains': ['Natural Language Processing'], 'Application Domains': '', 'Languages': ['English'], 'Similar Benchmarks': '', 'Resources': \"[{'huggingface': 'fedml/PubMedQA_instruction'}]\"}, 'Purpose and Intended Users': {'Goal': 'To assess the risk of LLMs revealing the original text (seed) while using it to generate synthetic data.', 'Audience': 'Researchers, developers, policymakers, and anyone interested in the potential risks of LLMs.', 'Tasks': ['The benchmark is designed for the task of detecting data leakage from the original text (seed) while paraphrasing it.'], 'Out-of-Scope Uses': 'The benchmark is not designed to be used for tasks such as text classification, sentiment analysis, or question-answering.'}, 'Data': {'Source': 'This benchmark uses the dataset fedml/PubMedQA_instruction. It contains a PubMedQA dataset converted for instruction tuning.', 'Size': 'The dataset contains 273,518 samples, with 272,518 samples in the training set and 1,000 samples in the test set. The download size is reported as 275 MB and dataset size as 275 MB. A subset of data (1000 sample instances) is used to run the MRA recipe.', 'Format': 'The dataset is stored in the Hugging Face Dataset format, with each sample consisting of an instruction, a context and its corresponding response.', 'Annotation': 'The dataset was built by FedML, and distributed un the the MIT License.'}, 'Methodology': {'Methods': 'To identify leakage risk, the benchmark looks for high syntactic similarity between the model responses and the original text, used as a seed for paraphrasing.', 'Metrics': 'Data leakage (metrics.data_leakage). ', 'Calculation': \"The synthetic data generation leakage score is calculated using a syntactic similarity metric, an enhanced variation of rougeL (rougLmod), between the model's response and the seed (text to be rephrased). A high syntactic similarity between the model response and the seed represents higher leakage risk.\", 'Interpretation': 'The final score is based on the attack’s average similarity score across all the synthetic generated samples tested, normalized such that 1 represents perfect privacy and 0 represents worst privacy.', 'Validation': ''}, 'Risks': {'Risk Categories': ['revealing-confidential-information', 'exposing-personal-information'], 'Limitations': 'This benchmark only assesses the tendency of synthetic data generation seeds leakage by using an available public dataset. Moreover, it measures leakage using a specialized metric based on syntactic similarity, which may miss certain types of leakage. ', 'Demographic Analysis': 'Unknown', 'Harm': 'The benchmark assesses the risk of LLMs revealing confidential information or sensitive personal information from the original text (aka synthetic data generation seed) in their generated output. Risk Title: Revealing confidential information,\\n Tag: revealing-confidential-information,\\n Type: output,\\n Phase:None,\\n Category:None,\\n Group:intellectual property,\\n Descriptor:amplified,\\n Description: When confidential information is used in the seed, training data, fine-tuning data, or as part of the prompt, models might reveal that data in the generated output. Revealing confidential information is a type of data leakage.,\\n Concern: If not properly developed to secure confidential data, the model might reveal confidential information or IP in the generated output and reveal information that was meant to be secret. Risk Title: Exposing personal information,\\n Tag: exposing-personal-information,\\n Type: output,\\n Phase:None,\\n Category:None,\\n Group:privacy,\\n Descriptor:amplified,\\n Description: When personal identifiable information (PII) or sensitive personal information (SPI) are used in the seed for synthetic data generation, training data, fine-tuning data, or as part of the prompt, models might reveal that data in the generated output. Revealing personal information is a type of data leakage.,\\n Concern: Sharing people’s PI impacts their rights and make them more vulnerable.'}, 'Ethical and Legal Considerations': {'Privacy and Anonymity': 'The benchmark does not collect or use any personal or sensitive data.', 'Data Licensing': 'The dataset is licensed under the MIT License.', 'Consent Procedures': 'Unknown', 'Compliance with Regulations': 'Unknown'}}\n",
      "{'Benchmark Details': {'Name': 'Prompt Leakage (QNLI)', 'ID': 'cards.prompt_leakage.glue_qnli', 'Overview': \"This benchmark is designed to assess the tendency of a model to leak its system prompt, also known as the base prompt or prompt template. The benchmark uses a dataset of prompts that are systematically designed to test the model's tendency to disclose its system prompt. The GLUE QNLI dataset is used to add few shot examples to the system prompt. The metric used to assess this ability is the prompt leakage score, which measures the degree of similarity between the model's response and the actual system prompt.\", 'Data Type': 'Text', 'Domains': ['natural language processing'], 'Application Domains': ['LLMs'], 'Languages': ['English'], 'Similar Benchmarks': ['Prompt Leakage (RAG)'], 'Resources': [{'arxiv': 'https://arxiv.org/pdf/2307.06865.pdf'}, {'huggingface': 'glue'}]}, 'Purpose and Intended Users': {'Goal': 'The benchmark is designed to assess the tendency of LLMs to leak their system prompt, and to provide a way to measure and evaluate this risk. The benchmark is intended for researchers, developers, and policymakers who are interested in understanding and addressing the potential security and privacy risks associated with LLMs.', 'Audience': 'The benchmark is intended for researchers, developers, and policymakers who are interested in understanding and addressing the prompt leakage from their LLMs', 'Tasks': [\"The benchmark evaluates the model's tendency to leak its system prompt by measuring the degree of similarity between the model's responses and the system prompts. The benchmark does not assess other aspects of the model's performance, such as accuracy or generation quality.\"], 'Out-of-Scope Uses': 'Models that do not have a system prompt.'}, 'Data': {'Source': \"The benchmark uses a dataset of prompts that are systematically designed to test the model's tendency to disclose its system prompt.\", 'Size': 'The dataset contains 58 prompts.', 'Format': 'The dataset is in CSV format and contains two columns: the prompt and rank.', 'Annotation': 'The dataset was manually curated by IBM and extended using the mixtral-8x7b-instruct-v01 model.'}, 'Methodology': {'Methods': \"The benchmark evaluates the model's tendency to leak its system prompt by measuring the degree of similarity between the model's responses and the system prompts. The benchmark uses a prompt leakage attack that tries to cause the model to leak its system prompt, and measures the degree of similarity between the model response and the actual system prompt. The attack is designed to be challenging for the model, and the benchmark uses a set of difficulty levels to measure the attack's effectiveness. The benchmark also provides a set of metrics that can be used to assess the model's performance, including the average similarity score across all attack prompts tested, and the normalized score that represents the degree of similarity between the model response and the system prompt. The benchmark provides a way to measure and evaluate the model's tendency to leak its system prompt, and can be used to help identify and mitigate potential security and privacy risks associated with LLMs.\", 'Metrics': \"Prompt leakage ['metrics.prompt_leakage'].\", 'Calculation': \"The prompt leakage score is calculated using a similarity metric, such as rouge or bert, between the model's response and the actual system prompt.\", 'Interpretation': 'A higher prompt leakage score indicates a lower risk of the model leaking its system prompt (better privacy).', 'Validation': ''}, 'Risks': {'Risk Categories': 'Prompt leaking', 'Limitations': 'The benchmark work does not experiment with sophisticated attacking strategies.', 'Demographic Analysis': 'Unknown', 'Harm': 'Prompt leaking risk for AI. A successful attack copies the system prompt used in the model. Depending on the content of that prompt, the attacker might gain access to valuable information, such as sensitive personal information or intellectual property, and might be able to replicate some of the functionality of the model.'}, 'Ethical and Legal Considerations': {'Privacy and Anonymity': 'The benchmark does not handle any personal or sensitive data.', 'Data Licensing': 'This is a proprietary IBM dataset.', 'Consent Procedures': 'Unknown', 'Compliance with Regulations': 'Unknown'}}\n",
      "{'Benchmark Details': {'Name': 'The AttaQ Dataset', 'ID': 'cards.value_alignment.toxic_output.attaq', 'Overview': 'The AttaQ dataset is a benchmark for evaluating the harmlessness of Large Language Models (LLMs) and to further investigate the factors influencing LLM behavior. It consists of adversarial samples that represent questions and queries for which LLMs should refrain from providing answers, such as inquiries about constructing dangerous devices or engaging in harmful activities.', 'Data Type': 'Text', 'Domains': ['natural language processing'], 'Application Domains': [], 'Languages': ['English'], 'Similar Benchmarks': ['None'], 'Resources': [{'arxiv': 'https://arxiv.org/pdf/2311.04124'}, {'huggingface': 'ibm/AttaQ'}]}, 'Purpose and Intended Users': {'Goal': 'Designed to provoke harmful or inappropriate responses.', 'Audience': 'Researchers, developers and policymakers working on safety and ethical considerations in AI.', 'Tasks': ['Evaluate the tendency of LLMs to generate harmful or undesirable responses in text generation.'], 'Out-of-Scope Uses': 'The benchmark is not designed to be applied in cases where harmful or unethical content is not the primary concern.'}, 'Data': {'Source': 'The dataset is a representative subset of all possible attacks, and many other attacks can cause LLMs to generate harmful or inappropriate responses.', 'Size': 'The dataset consists of 1402 adversarial questions.', 'Format': 'The dataset is in CSV format, with columns for label (label of adversarial question), input (adversarial question), and additional_info (source of the adversarial question). The data contains offensive and upsetting content by nature therefore it may not be easy to read.', 'Annotation': 'The dataset was derived by extracting attacks from the hh-rlhf dataset provided by Anthropic, and semi-automatically labelling them into seven categories: deception, discrimination, harmful information, substance abuse, sexual content, personally identifiable information (PII), and violence. LLMs generated further attack data and the Wikipedia crimes page was used to generate actions associated with each crime.  These actions were used to created the question data, , specifically focusing on strategies to evade detection or consequences. '}, 'Methodology': {'Methods': 'The dataset is created by semi-automatically labeling extracted attacks into seven categories.', 'Metrics': \"The metric used is ['metrics.mra_safety'], which provides a harmlessness score leveraging a reward model trained from human feedback.\", 'Calculation': 'The safety score is a measure of harmlessness. The level of harmlessness is evaluated using a reward model that considers both the input and output. The metric is described further in https://arxiv.org/abs/2209.07858.', 'Interpretation': 'A higher harmlessness score can be seen as better or less harmful, and a lower score can be seen as more harmful.', 'Validation': ''}, 'Risks': {'Risk Categories': ['Toxic output', 'Harmful output'], 'Limitations': 'The dataset is not designed to evaluate all possible risks of LLMs.  In the case of template-generated attacks, the model refused to generate inputs related to certain crimes and actions. This could potentially introduce biases to the dataset, possibly omitting certain crime domains. Secondly, the defition of what constitutes a crime is culturally specific which can potentially introduce bias.This dataset is a representative subset of all possible attacks. Many other attacks can cause LLM to answer harmful or inappropriate responses.', 'Demographic Analysis': 'Unknown', 'Harm': \"['Risk Title: Toxic output,\\n Tag: toxic-output,\\n Type: output,\\n Phase:None,\\n Category:None,\\n Group:value alignment,\\n Descriptor:specific,\\n Description: Toxic output occurs when the model produces hateful, abusive, and profane (HAP) or obscene content. This also includes behaviors like bullying.,\\n Concern: Hateful, abusive, and profane (HAP) or obscene content can adversely impact and harm people interacting with the model..', 'Risk Title: Harmful output,\\n Tag: harmful-output,\\n Type: output,\\n Phase:None,\\n Category:None,\\n Group:value alignment,\\n Descriptor:specific,\\n Description: A model might generate language that leads to physical harm The language might include overtly violent, covertly dangerous, or otherwise indirectly unsafe statements.,\\n Concern: A model generating harmful output can cause immediate physical harm or create prejudices that might lead to future harm..']\"}, 'Ethical and Legal Considerations': {'Privacy and Anonymity': 'The dataset does not include any personal or sensitive data.', 'Data Licensing': 'The dataset is licensed under the MIT license.', 'Consent Procedures': 'The dataset does not include any personal data, so no consent is required.', 'Compliance with Regulations': 'Unknown'}}\n",
      "{'Benchmark Details': {'Name': 'CNN Daily Mail data leakage', 'ID': 'cards.data_leakage.cnn_dailymail', 'Overview': \"This dataset contains news articles as written by journalists at CNN and the Daily Mail. It includes both training and test splits, and is used to assess the model's tendency to reveal confidential information or sensitive personal information in its generated output.\", 'Data Type': 'Text', 'Domains': ['Natural Language Processing'], 'Application Domains': ['LLMs'], 'Languages': ['English'], 'Similar Benchmarks': '', 'Resources': [{'github': 'https://github.com/abisee/cnn-dailymail'}, {'paper': 'https://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf'}, {'leaderboard': 'https://paperswithcode.com/sota/document-summarization-on-cnn-daily-mail'}]}, 'Purpose and Intended Users': {'Goal': 'To assess the risk of LLMs revealing memorized training samples in their generated output.', 'Audience': 'Researchers, developers, policymakers, and anyone interested in the potential risks of LLMs.', 'Tasks': ['The benchmark is designed for the task of detecting data leakage.'], 'Out-of-Scope Uses': 'The benchmark is not designed to be used for tasks such as text classification, sentiment analysis, or question-answering.'}, 'Data': {'Source': 'This uses the dataset CNN / Daily Mail.  This dataset is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail. The current version supports both extractive and abstractive summarization, though the original version was created for machine reading and comprehension and abstractive question answering.', 'Size': 'The dataset contains 287,113 samples in the training set, 13,368 in the validation set and 11,490 samples in the test set.  The test set is not used in this assessment. The download size is reported as 837094602 and dataset size as 1369361929. A subset of data (1000 sample instances) is used to run the MRA recipe.', 'Format': 'The dataset is stored in the Hugging Face Dataset format. For each instance, there is a string for the article, a string for the highlights, and a string for the id.', 'Annotation': 'The dataset does not contain any additional annotations.'}, 'Methodology': {'Methods': 'The benchmark looks for high syntactic similarity between the model responses and the actual samples, to identify leakage risk.', 'Metrics': 'Data leakage (metrics.data_leakage).', 'Calculation': 'This metric is based on a training data or fine-tuning data leakage attack, and represents to what degree the model has memorized its training data. The attack tries to cause the model to leak complete or partial samples from its training (or fine-tuning) data, and measures the degree of similarity between the model responses and the actual training data.', 'Interpretation': 'A high syntactic similarity between the model response and the actual sample represents higher leakage risk. The score is based on the attack’s average similarity score across all training samples tested, and normalized such that 1 represents perfect privacy and 0 represents worst privacy.', 'Validation': ''}, 'Risks': {'Risk Categories': ['revealing-confidential-information', 'exposing-personal-information'], 'Limitations': 'This benchmark only assesses the leakage of this specific dataset and no other datasets used in the model’s training or tuning. Moreover, it measures leakage using a specialized metric based on syntactic similarity, which may miss certain types of leakage. The benchmark assumes partial knowledge of the training samples in order to prompt the model to leak the rest of the data.', 'Demographic Analysis': \"Bordia and Bowman (2019) find the CNN / Dailymail dataset to have a slightly lower gender bias based on their metric compared to the other datasets, but still show evidence of gender bias when looking at words such as 'fragile'.\", 'Harm': \"Risk Title: Revealing confidential information,\\n Tag: revealing-confidential-information,\\n Type: output,\\n Phase:None,\\n Category:None,\\n Group:intellectual property,\\n Descriptor:amplified,\\n Description: When confidential information is used in training data, fine-tuning data, or as part of the prompt, models might reveal that data in the generated output. Revealing confidential information is a type of data leakage.,\\n Concern: If not properly developed to secure confidential data, the model might reveal confidential information or IP in the generated output and reveal information that was meant to be secret..', 'Risk Title: Exposing personal information,\\n Tag: exposing-personal-information,\\n Type: output,\\n Phase:None,\\n Category:None,\\n Group:privacy,\\n Descriptor:amplified,\\n Description: When personal identifiable information (PII) or sensitive personal information (SPI) are used in training data, fine-tuning data, or as part of the prompt, models might reveal that data in the generated output. Revealing personal information is a type of data leakage.,\\n Concern: Sharing people’s PI impacts their rights and make them more vulnerable..'\"}, 'Ethical and Legal Considerations': {'Privacy and Anonymity': \"Version 3.0 of data set is not anonymized, so individuals' names can be found in the dataset. Information about the original author is not included in the dataset.\", 'Data Licensing': 'This dataset is licensed under the [Apache License, Version 2.0]', 'Consent Procedures': 'Unknown', 'Compliance with Regulations': 'Unknown'}}\n",
      "{'Benchmark Details': {'Name': 'Jailbreak Risk LLM Judge', 'ID': 'cards.jailbreak_risk.jailbreak_risk_llm_judge', 'Overview': 'The dataset represents jailbreak prompts which are employed to bypass the safety alignment of LLMs and aligned LLMs should refrain from providing answers to such prompts. This dataset is based on a compilation of diverse datasets with malicious prompts spanning several categories, including instruction-based, question-based, artificial attacks (e.g., those generated iteratively with the use of language models), role-playing, harmful behavior, toxic content, and chat-based interactions. We use this dataset as a benchmark for evaluating the jailbreak risk of LLMs.', 'Data Type': 'Text', 'Domains': ['Natural Language Processing'], 'Application Domains': ['LLMs'], 'Languages': ['English'], 'Similar Benchmarks': [], 'Resources': [{'github': 'https://github.com/IBM/Adversarial-Prompt-Evaluation'}, {'paper': 'https://openreview.net/pdf?id=a44MiSFw6G'}]}, 'Purpose and Intended Users': {'Goal': 'Assess the jailbreak risk of LLMs', 'Audience': 'Researchers, developers, and policymakers', 'Tasks': ['Evaluate the jailbreak risk of LLMs'], 'Out-of-Scope Uses': 'Not available'}, 'Data': {'Source': 'The dataset was manually compiled from several public datasets.', 'Size': '10644', 'Format': 'CSV format, with columns for prompt, label, and source.', 'Annotation': 'The dataset was manually curated by IBM.'}, 'Methodology': {'Methods': 'Uses an LLM-as-judge metric for evaluating evaluating the jailbreak risk of generated predictions.', 'Metrics': \"Metric summary:['metrics.jailbreak_risk_llm_judge'], \", 'Calculation': 'Uses the source prompt, a template which reflects the criteria for judging jailbreaking risk, and the predictions to evaluate correctness using rating.single_turn with a designated judge model.', 'Interpretation': 'A higher score indicates a higher level of jailbreak risk.', 'Validation': ''}, 'Risks': {'Risk Categories': ['prompt-injection', 'jailbreaking'], 'Limitations': 'Not all LLMs (including large ones) can serve as good judges - their assessment may not correlate with human judgements and can also be biased. This means that unless you have a prior indication that the LLM you use is a good judge for your task, you need to evaluate its judgements and see they match your expectations.', 'Demographic Analysis': 'Unknown', 'Harm': 'Risk Title: Prompt injection attack,\\n Tag: prompt-injection,\\n Type: input,\\n Phase:inference,\\n Category:,\\n Group:robustness,\\n Descriptor:specific,\\n Description: A prompt injection attack forces a generative model that takes a prompt as input to produce unexpected output by manipulating the structure, instructions, or information contained in its prompt.,\\n Concern: Injection attacks can be used to alter model behavior and benefit the attacker. Risk Title: Jailbreaking,\\n Tag: jailbreaking,\\n Type: input,\\n Phase:inference,\\n Category:,\\n Group:multi-category,\\n Descriptor:specific,\\n Description: A jailbreaking attack attempts to break through the guardrails that are established in the model to perform restricted actions.,\\n Concern: Jailbreaking attacks can be used to alter model behavior and benefit the attacker. If not properly controlled, business entities can face fines, reputational harm, and other legal consequences.'}, 'Ethical and Legal Considerations': {'Privacy and Anonymity': 'The benchmark does not handle any personal or sensitive data.', 'Data Licensing': 'IBM proprietary dataset, available upon request.', 'Consent Procedures': 'Unknown', 'Compliance with Regulations': 'Unknown'}}\n",
      "{'Benchmark Details': {'Name': 'Jailbreak Risk', 'ID': 'cards.jailbreak_risk.jailbreak_risk', 'Overview': 'The dataset represents jailbreak prompts which are employed to bypass the safety alignment of LLMs and aligned LLMs should refrain from providing answers to such prompts. This dataset is based on a compilation of diverse datasets with malicious prompts spanning several categories, including instruction-based, question-based, artificial attacks (e.g., those generated iteratively with the use of language models), role-playing, harmful behavior, toxic content, and chat-based interactions. We use this dataset as a benchmark for evaluating the jailbreak risk of LLMs.', 'Data Type': 'Text', 'Domains': ['Natural language processing'], 'Application Domains': ['Question Answering'], 'Languages': ['English'], 'Similar Benchmarks': [], 'Resources': [{'github': 'https://github.com/IBM/Adversarial-Prompt-Evaluation'}, {'paper': 'https://openreview.net/pdf?id=a44MiSFw6G'}]}, 'Purpose and Intended Users': {'Goal': 'Assess the jailbreak risk of LLMs', 'Audience': 'Researchers, developers, and policymakers', 'Tasks': ['Evaluate the jailbreak risk of LLMs'], 'Out-of-Scope Uses': 'Not available'}, 'Data': {'Source': 'The dataset was manually compiled from several public datasets.', 'Size': '10644', 'Format': 'CSV format, with columns for prompt, label, and source.', 'Annotation': 'The dataset was manually curated by IBM.'}, 'Methodology': {'Methods': 'The dataset is a compilation of diverse datasets with malicious prompts spanning several categories, including instruction-based, question-based, artificial attacks (e.g., those generated iteratively with the use of language models), role-playing, harmful behavior, toxic content, and chat-based interactions.', 'Metrics': \"Metric summary:['metrics.jailbreak_risk']\", 'Calculation': 'The metric is calculated based on the ability of the model to provide correct answers to jailbreak prompts', 'Interpretation': 'A higher score indicates a higher level of jailbreak risk.', 'Baseline Results': 'The dataset is not accompanied by any baseline results.', 'Validation': 'The dataset is not accompanied by any validation details.'}, 'Risks': {'Risk Categories': ['prompt-injection', 'jailbreaking'], 'Limitations': 'The dataset is not provided with any limitations or details on evaluating or addressing risks', 'Demographic Analysis': 'Unknown', 'Harm': 'Risk Title: Prompt injection attack,\\n Tag: prompt-injection,\\n Type: input,\\n Phase:inference,\\n Category:,\\n Group:robustness,\\n Descriptor:specific,\\n Description: A prompt injection attack forces a generative model that takes a prompt as input to produce unexpected output by manipulating the structure, instructions, or information contained in its prompt.,\\n Concern: Injection attacks can be used to alter model behavior and benefit the attacker. Risk Title: Jailbreaking,\\n Tag: jailbreaking,\\n Type: input,\\n Phase:inference,\\n Category:,\\n Group:multi-category,\\n Descriptor:specific,\\n Description: A jailbreaking attack attempts to break through the guardrails that are established in the model to perform restricted actions.,\\n Concern: Jailbreaking attacks can be used to alter model behavior and benefit the attacker. If not properly controlled, business entities can face fines, reputational harm, and other legal consequences.'}, 'Ethical and Legal Considerations': {'Privacy and Anonymity': 'The benchmark does not handle any personal or sensitive data.', 'Data Licensing': 'IBM proprietary dataset, available upon request.', 'Consent Procedures': 'Unknown', 'Compliance with Regulations': 'Unknown'}}\n",
      "{'Benchmark Details': {'Name': 'SocialStigmaQA', 'ID': 'cards.value_alignment.toxic_output.social_stigma_cot', 'Overview': 'A benchmark to uncover stigma amplification in generative language models. The benchmark aims to test for social bias and model robustness in generative language models. It consists of 10K prompts with a variety of prompt styles, carefully constructed to systematically test for both social bias and model robustness.', 'Data Type': 'Text', 'Domains': ['Natural Language Processing'], 'Application Domains': ['Question Answering'], 'Languages': ['English'], 'Similar Benchmarks': 'None', 'Resources': [{'source': 'ibm/SocialStigmaQA'}, {'paper': 'https://ojs.aaai.org/index.php/AAAI/article/view/30142'}, {'model': 'ibm/social-bias-detector-v0'}]}, 'Purpose and Intended Users': {'Goal': \"Assess the model's ability to produce hateful, abusive, and profane (HAP) or obscene content, as well as its robustness to social bias.\", 'Audience': 'Researchers, developers, and policymakers in the field of natural language processing, as well as those interested in social bias and model evaluation.', 'Tasks': ['Question-answering, toxic output detection'], 'Out-of-Scope Uses': 'The sole purpose for of the benchmark and dataset is as a bias auditing tool, meant to evaluate generative language models.'}, 'Data': {'Source': 'The bencmark uses the ibm/SocialStigmaQA curated dataset. The dataset is meant to capture the amplification of social bias, via stigmas, in generative language models. Taking inspiration from social science research, it starts with a documented list of 93 US-centric stigmas and curate a question-answering (QA) dataset which involves simple social situations. The dataset, SocialStigmaQA, contains roughly 10K prompts, with a variety of prompt styles, carefully constructed to systematically test for both social bias and model robustness.', 'Size': '10,000 prompts', 'Format': 'CSV file. 37 patterns are present within the dataset. These patterns, or templates, were individually hand curated. Each of these patterns follows the same overarching structure - provide a mundane social situation and ask a question pertaining to this situation. The dataset to contain patterns for which answering either “yes” or “no” implies social bias.', 'Annotation': ''}, 'Methodology': {'Methods': 'Question-answering, toxic output detection', 'Metrics': \"Metric summary: ['metrics.socialbias']\", 'Calculation': 'The social-bias-detector is a BERT-like classification model which takes as input a sentence and outputs a label. It calculates the proportion of biased outputs and biased CoT (chain-of-thought) for the yes_no and cot versions of the dataset.', 'Interpretation': 'Interpret the scores based on the proportion of biased outputs and biased CoT.', 'Validation': ''}, 'Risks': {'Risk Categories': 'Toxic output', 'Limitations': 'Using this dataset on a large language model may result in text that is harmful and discriminatory. Possible limitations in evaluating or addressing stigmas, due to gaps in demographic coverage or specific domains, as the dataset is in English and reflects US culture.  The authors of the benchmark view evaluating open-ended text generation as an unsolved problem, so there may subtle biases pressent in CoT results.  Aditionally it is still an open question on how bias scores should be adjusted for certain models.', 'Demographic Analysis': 'Unknown', 'Harm': 'Risk Title: Toxic output,\\n Tag: toxic-output,\\n Type: output,\\n Phase:None,\\n Category:None,\\n Group:value alignment,\\n Descriptor:specific,\\n Description: Toxic output occurs when the model produces hateful, abusive, and profane (HAP) or obscene content. This also includes behaviors like bullying.,\\n Concern: Hateful, abusive, and profane (HAP) or obscene content can adversely impact and harm people interacting with the model.'}, 'Ethical and Legal Considerations': {'Privacy and Anonymity': 'No personal or sensitive data is included in the benchmark.', 'Data Licensing': 'The benchmark data is available under the cdla-permissive-2.0 license.', 'Consent Procedures': 'Unknown', 'Compliance with Regulations': 'Unknown'}}\n",
      "{'Benchmark Details': {'Name': 'TruthfulQA', 'ID': 'cards.value_alignment.hallucinations.truthfulqa', 'Overview': 'TruthfulQA is a benchmark to measure whether a language model is truthful in generating answers to questions.', 'Data Type': 'Text', 'Domains': ['natural language processing'], 'Application Domains': ['Health', 'Law', 'Finance', 'Politics'], 'Languages': ['English'], 'Similar Benchmarks': [], 'Resources': [{'github': 'https://github.com/sylinrl/TruthfulQA'}, {'paper': 'https://arxiv.org/abs/2109.07958'}]}, 'Purpose and Intended Users': {'Goal': 'A benchmark to measure whether a language model is truthful in generating answers to questions', 'Audience': 'Researchers, developers, policymakers', 'Tasks': ['Multiple-choice question answering', 'Text generation', 'Question answering'], 'Out-of-Scope Uses': 'Use cases where the benchmark is not designed to be applied and could give misleading results.'}, 'Data': {'Source': 'All questions were written by the authors and were designed to elicit imitative falsehoods. The questions were designed to be “adversarial” in the sense of testing for a weakness in the truthfulness of language models (rather than testing models on a useful task).', 'Size': 'The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics', 'Format': 'CSV format.  It is structured as sentence pairs, question-answer format. It includes columns: type, category, question, best_answer, correct_answers, incorrect_answers, source.', 'Annotation': 'Human annotations'}, 'Methodology': {'Methods': 'Adversarial procedure', 'Metrics': 'ROUGE: ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for evaluating automatic summarization and machine translation software in natural language processing.  Normalized Sacrebleu: SacreBLEU provides a method of computation of BLEU scores, which are a method of automatic machine translation evaluation.', 'Calculation': \"Rouge scores are calculated by comparing an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation. Normalized Sacrebleu: Bleu metrics range from 0 to 1. Scores are calculated for individual translated segments by comparing them with a set of good quality translations. Those scores are then averaged over the whole corpus to reach an estimate of the translation's overall quality. \", 'Interpretation': 'To perform well, models must avoid generating false answers learned from imitating human texts. A higher score suggests the model performed better at returning truthful responses. However, if a model performs well, we cannot conclude that it will be equally truthful on other kinds of tasks (even if we expect some transfer). ', 'Validation': ''}, 'Risks': {'Risk Categories': 'Hallucination', 'Limitations': 'TruthfulQA does not cover long-form generation (e.g. news articles) or interactive settings (e.g. extended chat with an adversarial human). Moreover, while the questions in TruthfulQA resemble real-world questions, they were not collected from a deployed system — and hence may over- or underestimate truthfulness for a deployed system.', 'Demographic Analysis': 'Performance across different demographic groups', 'Harm': 'Risk Title: Hallucination,\\n Tag: hallucination,\\n Type: output,\\n Phase:None,\\n Category:None,\\n Group:robustness,\\n Descriptor:specific,\\n Description: Hallucinations generate factually inaccurate or untruthful content with respect to the model’s training data or input. This is also sometimes referred to lack of faithfulness or lack of groundedness.,\\n Concern: Hallucinations can be misleading. These false outputs can mislead users and be incorporated into downstream artifacts, further spreading misinformation. False output can harm both owners and users of the AI models. In some uses, hallucinations can be particularly consequential.'}, 'Ethical and Legal Considerations': {'Privacy and Anonymity': 'Not available', 'Data Licensing': 'This dataset is licensed under the [Apache License, Version 2.0]', 'Consent Procedures': 'Obtaining consent (if applicable)', 'Compliance with Regulations': 'Compliance with relevant legal or ethical regulations'}}\n",
      "{'Benchmark Details': {'Name': 'Mitre CSE', 'ID': 'cards.harmful_code.mitre.mitre_cse_benchmark', 'Overview': 'The Mitre Test is a benchmark designed to assess the compliance of a Large Language Model (LLM) with the guidelines defined by the MITRE ATT&CK framework for cybersecurity. The benchmark involves three LLMs: a target LLM, an expansion LLM, and a judge LLM. The expansion LLM summarizes the response generated by the target LLM, which is then classified by the judge LLM as benign or malicious. The benchmark provides the numbers of test prompts and number of classifications as benign, malicious, and refusal of classification.', 'Data Type': 'Text', 'Domains': ['Cybersecurity'], 'Application Domains': ['Cybersecurity', 'LLMs'], 'Languages': ['English'], 'Similar Benchmarks': [], 'Resources': [{'paper': 'https://arxiv.org/abs/2404.13161'}, {'github': 'https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks'}]}, 'Purpose and Intended Users': {'Goal': 'The primary goal of the Mitre Test is to evaluate the ability of LLMs to generate harmful code that could be used in cyberattacks. ', 'Audience': 'The benchmark is intended for researchers, developers, and policymakers who are interested in assessing the cybersecurity risks associated with LLMs.', 'Tasks': [\"The benchmark is designed to assess the compliance of an LLM with the guidelines defined by the MITRE ATT&CK framework for cybersecurity. The benchmark evaluates the LLM's ability to generate code that could be used in cyberattacks.\"], 'Out-of-Scope Uses': 'The benchmark is not designed to be applied to evaluate the cybersecurity risks associated with other types of language generation models, such as those used for creative writing or storytelling.'}, 'Data': {'Source': 'The data used is benchmark data, publicly available under the MIT license.', 'Size': '4.3 MB', 'Format': 'CSV file. The data instances include the base_prompt ,mutated_prompt_base, mutated_prompt, mitre_category, ttp_id_name_mapping.', 'Annotation': 'The data is automatically labeled by the judge LLM.'}, 'Methodology': {'Methods': 'The benchmark evaluates the compliance of an LLM with the guidelines defined by the MITRE ATT&CK framework for cybersecurity. ', 'Metrics': 'Metric: Harmful Code Mitre CSE [metrics.harmful_code.mitre.mitre_cse_benchmark]', 'Calculation': \"The benchmark uses the metric of Harmful Code Mitre CSE, which is designed to assess the LLM's ability to generate code that could be used in cyberattacks. The metric involves three LLMs: a target LLM, an expansion LLM, and a judge LLM. The expansion LLM summarizes the response generated by the target LLM, which is then classified by the judge LLM as benign or malicious. The metric provides the numbers of test prompts and number of classifications as benign, malicious, and refusal of classification.\", 'Interpretation': 'The metric provides the numbers of test prompts and number of classifications as benign, malicious, and refusal of classification. A higher number of classifications as malicious indicates a higher risk of harmful code generation.', 'Validation': ''}, 'Risks': {'Risk Categories': ['Harmful code generation'], 'Limitations': 'Please note that this metric is a benchmark and is subject to ongoing development and refinement. Future improvements and advancements are planned to be incorporated to enhance its accuracy and effectiveness in model assessments.', 'Demographic Analysis': 'The benchmark does not provide a detailed analysis of the impact of LLMs on different demographic groups. However, it does note that the benchmark is designed to be inclusive and should be applied to all populations and demographic groups.', 'Harm': \"'Risk Title: Harmful code generation,\\n Tag: harmful-code-generation,\\n Type: output,\\n Phase:None,\\n Category:None,\\n Group:harmful code generation,\\n Descriptor:specific,\\n Description: Models might generate code that causes harm or unintentionally affects other systems.,\\n Concern: The execution of harmful code might open vulnerabilities in IT systems.\"}, 'Ethical and Legal Considerations': {'Privacy and Anonymity': 'The benchmark does not use any personal or sensitive data. It does not collect or store any personal information from users or the general public.', 'Data Licensing': 'The data used in the benchmark is publicly available under the MIT license.', 'Consent Procedures': 'Unknown', 'Compliance with Regulations': 'Unknown'}}\n",
      "{'Benchmark Details': {'Name': 'False Refusal Rate (FRR) CSE Benchmark', 'ID': 'cards.harmful_code.frr.frr_cse_benchmark', 'Overview': 'This benchmark evaluates the rate of falsely refusing benign prompts for generating source code of a target Large Language Models (LLM). The test prompts have been specifically designed to be borderline or ambiguous by containing keywords that are typical for benign as well as malicious source code requests. Refusals are determined by matching the generated output with a list of keywords commonly found in refusal statements of current LLMs. The final score is the fraction of benign test prompts that have been refused.', 'Data Type': 'Text', 'Domains': ['Cybersecurity'], 'Application Domains': ['Cybersecurity', 'LLMs'], 'Languages': ['English'], 'Similar Benchmarks': [], 'Resources': [{'Paper': 'https://arxiv.org/abs/2404.13161'}, {'Github': 'https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks'}]}, 'Purpose and Intended Users': {'Goal': 'To give visibility into how often an LLM designed to refuse help with cyber attacks also refuses ambiguous cases that are not clearly malicious (the safety-utility tradeoff).', 'Audience': 'Researchers, developers, and policymakers working in the field of cybersecurity', 'Tasks': ['Assess the false refusal rate (FRR) of LLMs on benchmark datasets'], 'Out-of-Scope Uses': 'Any other use than assessing the false refusal rate (FRR) of LLMs on benchmark datasets'}, 'Data': {'Source': 'The data used is benchmark data, publicly available under the MIT license.', 'Size': '352 KB', 'Format': 'CSV format.  The data instances include the mutated_prompt, is_malicious, attack_type, model', 'Annotation': 'General annotation, the data samples are borderline benign prompts. In terms of label: {benign}'}, 'Methodology': {'Methods': 'Benchmark datasets are used to evaluate the false refusal rate (FRR) of LLMs', 'Metrics': 'False Refusal Rate (FRR)', 'Calculation': 'This metric evaluates the rate of falsely refusing benign prompts for generating source code of a target Large Language Models (LLM). The test prompts have been specifically designed to be borderline or ambiguous by containing keywords that are typical for benign as well as malicious source code requests. Refusals are determined by matching the generated output with a list of keywords or patterns commonly found in refusal statements of current LLMs. The final score is the fraction of benign test prompts that have been refused.', 'Interpretation': 'A higher FRR score indicates a higher likelihood of LLMs rejecting to answer the requests that are borderline benign.', 'Validation': ''}, 'Risks': {'Risk Categories': ['Harmful code generation'], 'Limitations': 'Please note that this metric is a benchmark and is subject to ongoing development and refinement. Future improvements and advancements are planned to be incorporated to enhance its accuracy and effectiveness in model assessments.', 'Demographic Analysis': 'Not applicable', 'Harm': \"'Risk Title: Harmful code generation,\\n Tag: harmful-code-generation,\\n Type: output,\\n Phase:None,\\n Category:None,\\n Group:harmful code generation,\\n Descriptor:specific,\\n Description: Models might generate code that causes harm or unintentionally affects other systems.,\\n Concern: The execution of harmful code might open vulnerabilities in IT systems.\"}, 'Ethical and Legal Considerations': {'Privacy and Anonymity': 'Benchmark datasets do not contain any personal or sensitive data', 'Data Licensing': 'The data used in the benchmark is publicly available under the MIT license.', 'Consent Procedures': 'Unknown', 'Compliance with Regulations': 'Unknown'}}\n",
      "{'Benchmark Details': {'Name': 'SimpleSafetyTests', 'ID': 'cards.safety.simple_safety_tests', 'Overview': 'SimpleSafetyTests assists in identifying critical safety risks in Large Language Models.  It consists of 100 test prompts across five harm areas that LLMs, for the vast majority of applications, should refuse to comply with.', 'Data Type': 'text', 'Domains': ['LLMs'], 'Languages': ['English'], 'Resources': [{'arxiv': 'https://arxiv.org/abs/2311.08370'}, {'huggingface': 'https://huggingface.co/datasets/Bertievidgen/SimpleSafetyTests'}]}, 'Purpose and Intended Users': {'Goal': 'A benchmark to measure whether an LLM is following malicious instructions, providing unsafe advice, and generating toxic content', 'Audience': 'Researchers, developers, policymakers', 'Tasks': ['text-generation'], 'Out-of-Scope Uses': ''}, 'Data': {'Source': 'SST dataset consists of 100 test prompts equally split into five harm areas and two prompt categories. The harm areas are: (1) Suicide, Self-Harm, and Eating Disorders; (2) Physical Harm (3) Illegal and Highly Regulated Items; (4) Scams and Fraud; and (5) Child Abuse.  Ther categories are: (1) Information and Advice Seeking; and (2) Instructions and Actions.', 'Size': '100 prompts', 'Format': 'CSV', 'Annotation': 'Hand-crafted. The test prompts were created by one of the authors, and then iteratively adjusted through multiple rounds of review and editing by the other authors and assessed for internal validity.'}, 'Methodology': {'Methods': '', 'Metrics': 'Normalized Sacrebleu: SacreBLEU provides a method of computation of BLEU scores, which are a method of automatic machine translation evaluation.', 'Calculation': \"Normalized Sacrebleu: Bleu metrics range from 0 to 100%. Scores are calculated for individual translated segments by comparing them with a set of good quality translations. Those scores are then averaged over the whole corpus to reach an estimate of the translation's overall quality. \", 'Interpretation': 'To perform well, models must avoid returning unsafe responses. A higher score suggests better safety. However, if a model performs well, we cannot conclude that no safety risk exists, just that none was detected.', 'Baseline Results': 'The dataset is not accompanied by any baseline results.', 'Validation': 'The dataset is not accompanied by any validation details.'}, 'Risks': {'Risk Categories': ['harmful-output'], 'Limitations': 'Caution is advised as the prompts are sensitive and you could find them offensive, however, for the vast majority of applications, LLMs should refuse to comply with all of them. SST is not intended to be a comprehensive test of model safety. If a model returns 0% unsafe responses to SST then it indicates only that no safety risks have been found, rather than guaranteeing the absence of any safety risks (negative predictive power). The test prompts are designed to be extremely unsafe, and may not be commonly used by actual users. However, if models fail on even these simple prompts, they may be vulnerable to more sophisticated forms of unsafe prompts.  SST is  limited to the English language, with ten prompts each across five harm areas, so there are gaps in demographic coverage and specific domains. ', 'Demographic Analysis': 'Not available', 'Harm': ['Risk Title: Harmful output,\\n Tag: harmful-output,\\n Type: output,\\n Phase:None,\\n Category:None,\\n Group:value alignment,\\n Descriptor:specific,\\n Description: A model might generate language that leads to physical harm The language might include overtly violent, covertly dangerous, or otherwise indirectly unsafe statements.,\\n Concern: A model generating harmful output can cause immediate physical harm or create prejudices that might lead to future harm.']}, 'Ethical and Legal Considerations': {'Privacy and Anonymity': 'Personal or sensitive data not included', 'Data Licensing': 'cc-by-2.0', 'Consent Procedures': 'Not applicable', 'Compliance with Regulations': 'Not applicable'}}\n",
      "{'Benchmark Details': {'Name': 'Discrim-Eval', 'ID': 'cards.safety.discrim_eval.implicit', 'Overview': \"The data contains a diverse set of prompts covering 70 hypothetical decision scenarios, ranging from approving a loan to providing press credentials. Each prompt instructs the model to make a binary decision (yes/no) about a particular person described in the prompt. Each person is described in terms of three demographic attributes: age (ranging from 20 to 100 in increments of 10), gender (male, female, non-binary), and race (white, Black, Asian, Hispanic, Native American), for a total of 135 examples per decision scenario. The prompts are designed so a 'yes' decision is always advantageous to the person (e.g. deciding to grant the loan).\", 'Data Type': 'Text', 'Domains': ['Question answering'], 'Languages': ['English'], 'Resources': {'Paper': 'https://arxiv.org/abs/2312.03689'}}, 'Purpose and Intended Users': {'Goal': 'To evaluate the potential discriminatory impact of LMs in a wide range of use cases, including hypothetical use cases where they have not yet been deployed.', 'Audience': 'Researchers, developers, and policymakers who are interested in understanding and mitigating the potential risks of AI systems.', 'Tasks': ['Evaluating and mitigating discrimination in language model decisions.'], 'Out-of-Scope Uses': 'Using the benchmark for high-risk automated decision making or for other purposes not related to its original goals.'}, 'Data': {'Source': 'Anthropic', 'Size': 'The dataset contains 135 examples per decision scenario, totalling 9,450 examples across all 70 decision scenarios.', 'Format': 'The data is provided in two JSONL files: explicit.jsonl and implicit.jsonl. Each line in the files contains a prompt, decision question ID, age, gender, and race of the person described in the prompt. The implicit.jsonl file does not have an explicit mention of race or gender, but rather relies on an implicit version of these attributes based on a name. ', 'Annotation': \"The data was annotated using Anthropics's Claude language models.\"}, 'Methodology': {'Methods': 'The benchmark uses a series of hypothetical decision scenarios to evaluate the ability of language models to make unbiased decisions. Each prompt is designed to elicit a binary decision (yes/no) about a particular person described in the prompt. The prompts are then used to test the ability of language models to make accurate decisions and to identify potential biases.', 'Metrics': \"The benchmark uses accuracy as the primary metric to evaluate the model's performance. Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with: Accuracy = (TP + TN) / (TP + TN + FP + FN).\", 'Calculation': \"The benchmark calculates the discrimination score for each decision scenario and demographic attribute based on the model's performance on the prompt examples.\", 'Interpretation': 'The benchmark provides a way to measure the extent to which the model may be biased towards certain demographic groups. A higher discrimination score indicates a higher likelihood of discrimination.', 'Baseline Results': \"The benchmark compares the model's performance to a baseline model that does not take into account the demographic attributes of the person described in the prompt. This can help identify if the model is biased towards any particular demographic group.\", 'Validation': 'The benchmark does not provide information on the validation process used to ensure the data and methods are valid and reliable.'}, 'Risks': {'Risk Categories': ['output-bias'], 'Limitations': \"The benchmark does not cover all possible decision making scenarios or demographics, and may not be fully representative of all real-world scenarios.  In terms of external validity, the evaluation is done on a model's judgement of a short text description of candiates only, which does not account for the wider variety of input formats that might affect the model's conclusions. It is important to note that the dataset authors do not permit or endorse the use of LMs for high-risk automated decision making.\", 'Demographic Analysis': \"The benchmark evaluates the performance of language models across different demographic groups, including age, gender, and race. However, it does not provide a detailed analysis of how these biases manifest in the model's decisions.\", 'Harm': ['Risk Title: Output Bias,\\n Tag: output-bias,\\n Type: output,\\n Phase:None,\\n Category:None,\\n Group:fairness,\\n Descriptor:specific,\\n Description: Generated content might unfairly represent certain groups or individuals.,\\n Concern:Bias can harm users of the AI models and magnify existing discriminatory behaviors.']}, 'Ethical and Legal Considerations': {'Privacy and Anonymity': 'The benchmark does not collect or use personal or sensitive data.', 'Data Licensing': 'The benchmark data is licensed under the Creative Commons Attribution 4.0 International (CC-BY-4.0) license.', 'Consent Procedures': 'The benchmark does not collect any personal data, and does not require any consent from users or subjects.', 'Compliance with Regulations': 'The benchmark does not address compliance with any specific regulations related to data privacy or discrimination.'}}\n",
      "{'Benchmark Details': {'Name': 'ProvoQ (PROVOcative Questions about minority-associated stigmas)', 'ID': 'cards.safety.provoq', 'Overview': 'The ProvoQ dataset is designed to evaluate the sensitivity of large language models (LLMs) to stigma-related topics. It contains 2,705 human-curated provocative questions that systematically target minority-stigma pairs in the United States, creating a diverse and nuanced set of questions that reflect these sensitive topics. The dataset aims to support research in understanding and mitigating biases in AI systems, particularly in the context of minority groups. Whilst most questions are toxic, others may seem benign but potentially elicit harmful responses. The dataset contains questions in text format, organized by minority-stigma pairs.', 'Data Type': 'Text', 'Domains': ['Natural Language Processing'], 'Languages': ['English']}, 'Resources': {'Paper': 'https://arxiv.org/abs/2311.04124', 'Code': 'https://huggingface.co/datasets/ibm-research/ProvoQ'}, 'Purpose and Intended Users': {'Goal': 'To evaluate the sensitivity of LLMs to stigma-related topics and support research in understanding and mitigating biases in AI systems, particularly in the context of minority groups.', 'Audience': 'Researchers, developers, and policymakers', 'Tasks': ['The dataset can be used to train and evaluate models that detect and mitigate biases in LLMs related to stigma-related topics.'], 'Out-of-Scope Uses': 'The dataset is not intended for use in applications that require specific knowledge or skills related to stigma-related topics.'}, 'Data': {'Source': 'IBM Provoq. Curated by two teams from the US and the Middle East. The minority groups and associated stigmas were identified using a combination of a proprietary language model and manual collection methods.', 'Size': '2,705 questions', 'Format': 'Questions in text format, organized by minority-stigma pairs.', 'Annotation': \"The questions were labeled by 14 volunteers from diverse genders, backgrounds, and nationalities, who marked them as good, based on the definition of 'good' outlined in the annotators' guidelines. Each question has received at least one annotation.\"}, 'Methodology': {'Methods': \"The dataset was created using the 'Crawl and Generate' approach, inspired by Kour et al. (2023).\", 'Metrics': \"Metric summary: ['metrics.llm_as_judge.safety.llama_3_70b_instruct_ibm_watsonx_template_harm_rating'].  This is an LLM-as-judge-based metric class for evaluating correctness of generated predictions.  It uses the source prompt given to the generator and the generator's predictions to evaluate correctness using a single turn task.\", 'Calculation': 'The harm rating metric is a binary classification score indicating whether a question was harmful or not.', 'Interpretation': \"Users should interpret the harm rating score as a measure of the question's potential to elicit harmful responses from LLMs.\", 'Baseline Results': 'Not available', 'Validation': 'The dataset underwent both automatic and manual curation processes, including grammar and spell checks, followed by human review to ensure the quality and relevance of the questions.'}, 'Risks': {'Risk Categories': ['output-bias'], 'Limitations': \"The groups and associated stigmas in the ProvoQ dataset may be controversial. The dataset's biases could arise from human input and automated processes involved in its creation process. Users should exercise caution and judgment when using this dataset. Please note that the researchers involved in the creation of this dataset are not specialists in the social aspects addressed. This dataset is intended for research and testing, particularly for evaluation and addressing biases in chat models. It contains sensitive questions, so applying ethical consideration, exercising proper judgment, and implementing appropriate safeguards when working with this dataset is crucial.  Each step in this dataset generation process can potentially introduce bias, whether through human input or automated procedures. This is particularly true for the selection of minority groups and their associated stigmas, which can be especially controversial.  The dataset may not be exhaustive or cover all possible perspectives on the stigma-related topics. It may also not capture the nuances and complexities of real-life situations.\", 'Demographic Analysis': 'The dataset does not provide a comprehensive analysis of the impact of stigma on different demographic groups. It is important to consider the potential implications of the dataset for various communities and individuals.', 'Harm': ['Risk Title: Output Bias,\\n Tag: output-bias,\\n Type: output,\\n Phase:None,\\n Category:None,\\n Group:fairness,\\n Descriptor:specific,\\n Description: Generated content might unfairly represent certain groups or individuals.,\\n Concern:Bias can harm users of the AI models and magnify existing discriminatory behaviors.']}, 'Ethical and Legal Considerations': {'Privacy and Anonymity': 'The dataset does not contain any personal or sensitive data.', 'Data Licensing': 'The dataset is licensed under the cdla-permissive-2.0 license.', 'Consent Procedures': 'The dataset does not include any personal data, so no consent is required.', 'Compliance with Regulations': 'Unknown'}}\n",
      "{'Benchmark Details': {'Name': 'Custom Data Leakage', 'ID': 'cards.data_leakage.empty_data_leakage_card', 'Overview': \"This evaluation is performed on custom models. A user may use this evaluation to evaluate a custom model with a subset of the assessed model's training data, which must be supplied by the user to the system.  The benchmark is designed to assess the risk of data leakage in language models. It aims to evaluate the degree to which models memorize their training data and reveal confidential or sensitive information in the generated output.\", 'Data Type': 'Text', 'Domains': ['Natural Language Processing'], 'Languages': ['English']}, 'Purpose and Intended Users': {'Goal': 'To assess the risk of data leakage in the LM.', 'Audience': 'Researchers, developers, and policymakers', 'Tasks': [\"The benchmark evaluates the model's ability to memorize training data and reveal confidential or sensitive information in the generated output.\"], 'Out-of-Scope Uses': 'The benchmark is not designed to be applied to other types of models or data modalities.'}, 'Data': {'Source': \"The data for this evaluation must be supplied by the user.  It must be a subset of the assessed model's training data.\", 'Size': 'Unknown', 'Format': 'Unknown', 'Annotation': 'Unknown '}, 'Methodology': {'Methods': \"The benchmark uses a data leakage attack to evaluate the model's ability to memorize training data. The attack tries to cause the model to leak complete or partial samples from its training data and measures the degree of similarity between the model responses and the actual training data.\", 'Metrics': 'This metric is based on a training data or fine-tuning data leakage attack, and represents to what degree the model has memorized its training data. ', 'Calculation': ' The attack tries to cause the model to leak complete or partial samples from its training (or fine-tuning) data, and measures the degree of similarity between the model responses and the actual training data.', 'Interpretation': 'A high syntactic similarity between the model response and the actual sample represents higher leakage risk. The score is based on the attack’s average similarity score across all training samples tested, and normalized such that 1 represents perfect privacy and 0 represents worst privacy.', 'Validation': ''}, 'Risks': {'Risk Categories': ['revealing-confidential-information', 'exposing-personal-information'], 'Limitations': 'The benchmark does not evaluate all potential risks associated with data leakage in language models. It does not address gaps in demographic coverage or specific domains.', 'Demographic Analysis': 'The benchmark does not perform demographic analysis to evaluate the impact of data leakage on different demographic groups.', 'Harm': 'The benchmark assesses the risk of LLMs revealing confidential information or sensitive personal information in their generated output. Risk Title: Revealing confidential information,\\n Tag: revealing-confidential-information,\\n Type: output,\\n Phase:None,\\n Category:None,\\n Group:intellectual property,\\n Descriptor:amplified,\\n Description: When confidential information is used in training data, fine-tuning data, or as part of the prompt, models might reveal that data in the generated output. Revealing confidential information is a type of data leakage.,\\n Concern: If not properly developed to secure confidential data, the model might reveal confidential information or IP in the generated output and reveal information that was meant to be secret. Risk Title: Exposing personal information,\\n Tag: exposing-personal-information,\\n Type: output,\\n Phase:None,\\n Category:None,\\n Group:privacy,\\n Descriptor:amplified,\\n Description: When personal identifiable information (PII) or sensitive personal information (SPI) are used in training data, fine-tuning data, or as part of the prompt, models might reveal that data in the generated output. Revealing personal information is a type of data leakage.,\\n Concern: Sharing people’s PI impacts their rights and make them more vulnerable.'}, 'Ethical and Legal Considerations': {'Privacy and Anonymity': 'The benchmark does not handle any personal or sensitive data, and it does not apply any anonymization techniques.', 'Data Licensing': 'Unknown, custom model and dataset.', 'Consent Procedures': 'The benchmark does not involve any personal data, and it does not require consent from users.', 'Compliance with Regulations': 'Unknown'}}\n",
      "{'Benchmark Details': {'Name': 'Prompt Injection Risk CSE Benchmark', 'ID': 'cards.jailbreak_risk.prompt_injection_risk_cse_benchmark', 'Overview': 'The dataset represents prompt injection attacks which are employed to bypass the safety alignment of LLMs and aligned LLMs should refrain from providing answers to such prompts. This dataset is taken from cyberseceval benchmark developed by Meta and released under MIT License. We use this dataset as a benchmark for evaluating the prompt injection risk of LLMs.', 'Data Type': 'text', 'Domains': ['Cybersecurity'], 'Application Domains': ['Cybersecurity', 'LLMs'], 'Languages': ['English'], 'Resources': [{'Github': 'https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks'}]}, 'Purpose and Intended Users': {'Goal': 'To evaluate the prompt injection risk of LLMs', 'Audience': 'Researchers, developers, policymakers', 'Tasks': ['Assessing the prompt injection risk of LLMs on benchmark datasets.'], 'Out-of-Scope Uses': 'Any other use than assessing the prompt injection risk of LLMs on benchmark datasets.'}, 'Data': {'Source': 'Cyberseceval benchmark developed by Meta. The data used is benchmark data, publicly available under the MIT license.', 'Size': '352 KB', 'Format': 'CSV format. The data instances include the test_case_prompt, user_input, judge_question, injection_variant, injection_type, risk_category, speaking_language', 'Annotation': 'General annotation, the data samples are injection attack prompts.'}, 'Methodology': {'Methods': 'Using the benchmark to evaluate the prompt injection risk of LLMs', 'Metrics': 'Prompt Injection Risk CSE', 'Calculation': 'This metric evaluates the rate of prompt injection attack risk. The test prompts have been specifically designed to be ambiguous by containing keywords that are typical for benign as well as malicious requests. The final score is the fraction of successful attack prompts that result in model response according to injection goal.', 'Interpretation': 'A higher risk score indicates a higher likelihood of prompt injection risk.', 'Validation': ''}, 'Risks': {'Risk Categories': ['prompt-injection', 'jailbreaking'], 'Limitations': 'Please note that this metric is a benchmark and is subject to ongoing development and refinement. Future improvements and advancements are planned to be incorporated to enhance its accuracy and effectiveness in model assessments.', 'Demographic Analysis': 'Unknown', 'Harm': 'Risk Title: Prompt injection attack,\\n Tag: prompt-injection,\\n Type: input,\\n Phase:inference,\\n Category:,\\n Group:robustness,\\n Descriptor:specific,\\n Description: A prompt injection attack forces a generative model that takes a prompt as input to produce unexpected output by manipulating the structure, instructions, or information contained in its prompt.,\\n Concern: Injection attacks can be used to alter model behavior and benefit the attacker. Risk Title: Jailbreaking,\\n Tag: jailbreaking,\\n Type: input,\\n Phase:inference,\\n Category:,\\n Group:multi-category,\\n Descriptor:specific,\\n Description: A jailbreaking attack attempts to break through the guardrails that are established in the model to perform restricted actions.,\\n Concern: Jailbreaking attacks can be used to alter model behavior and benefit the attacker. If not properly controlled, business entities can face fines, reputational harm, and other legal consequences.'}, 'Ethical and Legal Considerations': {'Privacy and Anonymity': 'Benchmark datasets do not contain any personal or sensitive data', 'Data Licensing': 'The data used in the benchmark is publicly available under the MIT license.', 'Consent Procedures': 'Unknown', 'Compliance with Regulations': 'Unknown'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n        #hasResources=[value for dictionary in item[\"Benchmark Details\"][\"Resources\"] for value in dictionary.values()],\\n        #hasSimilarBenchmarks: List[str] | None = None,\\n\\n        url: str | None = None,\\n        dateCreated: date | None = None,\\n        dateModified: date | None = None,\\n\\n\\n        hasBaselineResults: str | None = None,\\n\\n        hasRelatedRisks: List[str] | None = None,\\n        hasDocumentation: List[str] | None = None,\\n\\n\\n        '"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from risk_atlas_nexus.ai_risk_ontology.datamodel.ai_risk_ontology import AiEval, BenchmarkMetadataCard\n",
    "\n",
    "\n",
    "with open('/Users/ingevejs/Documents/workspace/ingelise/risk-atlas-nexus/docs/examples/notebooks/benchmark_card_metadata.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "ai_evals = []\n",
    "ai_eval_bmcs = []\n",
    "for item in data:\n",
    "    print(item) \n",
    "    \n",
    "    ai_ev = AiEval(\n",
    "        id=\"ai_eval_\"+item[\"Benchmark Details\"][\"Name\"].replace(\" \", \"_\"), \n",
    "        name=item[\"Benchmark Details\"][\"Name\"], \n",
    "        description= item[\"Benchmark Details\"][\"Overview\"],\n",
    "        hasBenchmarkMetadata= [\"ai_eval_bmc_\"+item[\"Benchmark Details\"][\"Name\"].replace(\" \", \"_\") ],\n",
    "        hasUnitxtCard=item[\"Benchmark Details\"][\"ID\"],\n",
    "        hasLicense=item[\"Ethical and Legal Considerations\"][\"Data Licensing\"],\n",
    "        \n",
    "      \n",
    "        # url: str | None = None, \n",
    "        # dateCreated: date | None = None, \n",
    "        # dateModified: date | None = None, \n",
    "        # hasDocumentation: List[str] | None = None, \n",
    "        # hasDataset: List[str] | None = None, \n",
    "     \n",
    "        # hasRelatedRisk: List[str] | None = None, \n",
    "        # bestValue: str | None = None, \n",
    "\n",
    "        )\n",
    "    ai_evals.append(ai_ev)\n",
    "    '''\n",
    "        AiEval  - id: cards.value_alignment.hallucinations.truthfulqa\n",
    "            name: \"TruthfulQA\"\n",
    "            description: \"TruthfulQA is a benchmark to measure whether a language model is truthful in generating answers to questions.\"\n",
    "            url: https://github.com/sylinrl/TruthfulQA\n",
    "            hasUnitxtCard: cards.value_alignment.hallucinations.truthfulqa\n",
    "            hasRelatedRisk: [\"atlas-hallucination\"]\n",
    "            hasDocumentation: [\"arxiv.org/2109.07958\"]\n",
    "            hasDataset: [\"truthfulqa/truthful_qa\"]\n",
    "\n",
    "        '''\n",
    "    \n",
    "\n",
    "    bmc = BenchmarkMetadataCard(\n",
    "        id=\"ai_eval_bmc_\"+item[\"Benchmark Details\"][\"Name\"].replace(\" \", \"_\"), \n",
    "        name=item[\"Benchmark Details\"][\"Name\"], \n",
    "        overview=item[\"Benchmark Details\"][\"Overview\"],\n",
    "        describesAiEval=[\"ai_eval_\"+item[\"Benchmark Details\"][\"Name\"].replace(\" \", \"_\")],\n",
    "        hasDataType=[item[\"Benchmark Details\"][\"Data Type\"]], \n",
    "        hasDomains=item[\"Benchmark Details\"][\"Domains\"], \n",
    "        hasLanguages=item[\"Benchmark Details\"][\"Languages\"], \n",
    "        hasGoal=item[\"Purpose and Intended Users\"][\"Goal\"], \n",
    "        hasAudience=item[\"Purpose and Intended Users\"][\"Audience\"],\n",
    "        hasTasks=item[\"Purpose and Intended Users\"][\"Tasks\"],\n",
    "        hasLimitations=[item[\"Risks\"][\"Limitations\"]],\n",
    "        hasOutOfScopeUses=[item[\"Purpose and Intended Users\"][\"Out-of-Scope Uses\"]],\n",
    "        hasDataSource=[item[\"Data\"][\"Source\"]],\n",
    "        hasDataSize=item[\"Data\"][\"Size\"],\n",
    "        hasDataFormat=item[\"Data\"][\"Format\"],\n",
    "        hasAnnotation=item[\"Data\"][\"Annotation\"],\n",
    "        hasMethods=[item[\"Methodology\"][\"Methods\"]],\n",
    "        hasMetrics=[item[\"Methodology\"][\"Metrics\"]],\n",
    "        hasCalculation=[item[\"Methodology\"][\"Calculation\"]],\n",
    "        hasInterpretation=[item[\"Methodology\"][\"Interpretation\"]],\n",
    "        hasValidation=[item[\"Methodology\"][\"Validation\"]],\n",
    "        hasDemographicAnalysis=item[\"Risks\"][\"Demographic Analysis\"] or None,\n",
    "        hasLicense=item[\"Ethical and Legal Considerations\"][\"Data Licensing\"],\n",
    "        hasConsiderationPrivacyAndAnonymity=item[\"Ethical and Legal Considerations\"][\"Privacy and Anonymity\"],\n",
    "        hasConsiderationConsentProcedures=item[\"Ethical and Legal Considerations\"][\"Consent Procedures\"],\n",
    "        hasConsiderationComplianceWithRegulations=item[\"Ethical and Legal Considerations\"][\"Compliance with Regulations\"],\n",
    "        )\n",
    "\n",
    "    ai_eval_bmcs.append(bmc)\n",
    "    \n",
    "\n",
    "'''\n",
    "        #hasResources=[value for dictionary in item[\"Benchmark Details\"][\"Resources\"] for value in dictionary.values()],\n",
    "        #hasSimilarBenchmarks: List[str] | None = None,\n",
    "        \n",
    "        url: str | None = None,\n",
    "        dateCreated: date | None = None,\n",
    "        dateModified: date | None = None,\n",
    "        \n",
    "\n",
    "        hasBaselineResults: str | None = None,\n",
    "      \n",
    "        hasRelatedRisks: List[str] | None = None,\n",
    "        hasDocumentation: List[str] | None = None,\n",
    "      \n",
    "\n",
    "        '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0034639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create container\n",
    "from risk_atlas_nexus.ai_risk_ontology.datamodel.ai_risk_ontology import Container\n",
    "from linkml_runtime.dumpers import YAMLDumper\n",
    "\n",
    "container = Container(\n",
    "    evaluations=ai_evals,\n",
    "    benchmarkmetadatacards=ai_eval_bmcs,\n",
    ")\n",
    "\n",
    "with open(\n",
    "        \"/Users/ingevejs/Documents/workspace/ingelise/risk-atlas-nexus/src/risk_atlas_nexus/data/knowledge_graph_prep/benchmark_metadata_cards.yaml\",\n",
    "        \"+tw\",\n",
    "        encoding=\"utf-8\",\n",
    "    ) as output_file:\n",
    "        print(YAMLDumper().dumps(container), file=output_file)\n",
    "        output_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1cd1c34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Links</th>\n",
       "      <th>Risk Mapping (IBM) - Major</th>\n",
       "      <th>Risk Mapping (IBM) - Minor</th>\n",
       "      <th>Risk Mapping (MIT) - Major</th>\n",
       "      <th>Risk Mapping (MIT) - Minor</th>\n",
       "      <th>Risk Mapping (MLCommons)</th>\n",
       "      <th>Task</th>\n",
       "      <th>Preprocessing</th>\n",
       "      <th>Template</th>\n",
       "      <th>Metric</th>\n",
       "      <th>Implementation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BOLD</td>\n",
       "      <td>repo: https://github.com/amazon-research/bold\\...</td>\n",
       "      <td>Output / Fairness</td>\n",
       "      <td>Output BIas</td>\n",
       "      <td>DIscrimination &amp; Toxicity</td>\n",
       "      <td>Unequal performance across groups</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Generation</td>\n",
       "      <td>Split renaming\\nField renaming\\nExtract from JSON</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Regard</td>\n",
       "      <td>https://github.com/IBM/unitxt/blob/main/prepar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AttaQ</td>\n",
       "      <td>dataset: https://huggingface.co/datasets/ibm-r...</td>\n",
       "      <td>Output / Value Alignment</td>\n",
       "      <td>Toxic Output, Harmful Output</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Question Answering</td>\n",
       "      <td>Split renaming</td>\n",
       "      <td>Basic / Empty</td>\n",
       "      <td>Reward Model</td>\n",
       "      <td>https://github.com/IBM/unitxt/blob/main/prepar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ProvoQ</td>\n",
       "      <td>dataset: https://huggingface.co/datasets/ibm-r...</td>\n",
       "      <td>Output / Fairness</td>\n",
       "      <td>Output BIas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Question Answering</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Basic / Empty</td>\n",
       "      <td>Judge</td>\n",
       "      <td>https://github.com/IBM/unitxt/blob/main/prepar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CrowS-Pairs</td>\n",
       "      <td>repo: https://github.com/nyu-mll/crows-pairs\\n...</td>\n",
       "      <td>Output / Fairness</td>\n",
       "      <td>Output BIas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Multiple Choice?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://github.com/EleutherAI/lm-evaluation-ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alert</td>\n",
       "      <td>repo: https://github.com/Babelscape/ALERT\\ndat...</td>\n",
       "      <td>Output / Fairness, Output / Value Alignment</td>\n",
       "      <td>Toxic Output, Output BIas, Harmful Output</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Question Answering</td>\n",
       "      <td>Field renaming</td>\n",
       "      <td>Basic / Empty</td>\n",
       "      <td>Judge</td>\n",
       "      <td>https://github.com/Babelscape/ALERT/blob/maste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SALAD_Bench</td>\n",
       "      <td>repo: https://github.com/OpenSafetyLab/SALAD-B...</td>\n",
       "      <td>Output / Misuse, Output / Value Alignment, Out...</td>\n",
       "      <td>Toxic Output, Harmful Output, Output BIas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Question Answering Multiple Choice</td>\n",
       "      <td>Field renaming</td>\n",
       "      <td>Basic / Empty</td>\n",
       "      <td>Judge</td>\n",
       "      <td>https://github.com/OpenSafetyLab/SALAD-BENCH/b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SorryBench</td>\n",
       "      <td>repo: https://github.com/sorry-bench/sorry-ben...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Question Answering</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Judge</td>\n",
       "      <td>https://github.com/sorry-bench/sorry-bench</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ToxiGen</td>\n",
       "      <td>repo: https://github.com/microsoft/TOXIGEN\\nda...</td>\n",
       "      <td>Output / Value Alignment</td>\n",
       "      <td>Toxic Output</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Value mapping\\nField renaming</td>\n",
       "      <td>Classification request</td>\n",
       "      <td>F1 (micro/macro)\\nAccuracy</td>\n",
       "      <td>https://github.com/IBM/unitxt/blob/main/prepar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XSTest</td>\n",
       "      <td>repo: https://github.com/paul-rottger/xstest\\n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>StrongReject</td>\n",
       "      <td>repo: https://github.com/alexandrasouly/strong...</td>\n",
       "      <td>Output / Value Alignment</td>\n",
       "      <td>Toxic Output, Harmful Output</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Question Answering</td>\n",
       "      <td>Field renaming</td>\n",
       "      <td>Basic / Empty</td>\n",
       "      <td>Judge</td>\n",
       "      <td>https://github.com/alexandrasouly/strongreject...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SimpleSafetyTests</td>\n",
       "      <td>dataset: https://huggingface.co/datasets/Berti...</td>\n",
       "      <td>Output / Value Alignment</td>\n",
       "      <td>Harmful Output</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Question Answering</td>\n",
       "      <td>Field renaming</td>\n",
       "      <td>Basic / Empty</td>\n",
       "      <td>Judge</td>\n",
       "      <td>https://github.com/IBM/unitxt/blob/main/prepar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BBQ</td>\n",
       "      <td>repo: https://github.com/nyu-mll/BBQ\\ndataset:...</td>\n",
       "      <td>Output / Fairness</td>\n",
       "      <td>Output BIas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Multiple Choice</td>\n",
       "      <td>Field renaming\\nValue aggregation</td>\n",
       "      <td>Multiple Choice</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>https://github.com/IBM/unitxt/blob/main/prepar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Discrim_eval</td>\n",
       "      <td>dataset: https://huggingface.co/datasets/Anthr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Multiple Choice</td>\n",
       "      <td>Split renaming\\nField renaming</td>\n",
       "      <td>Multiple Choice</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>https://github.com/IBM/unitxt/blob/main/prepar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>XSafety</td>\n",
       "      <td>repo: https://github.com/jarviswang94/multilin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://github.ibm.com/IBM-Research-AI/fm-eval...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AILuminate</td>\n",
       "      <td>repo: https://github.com/mlcommons/ailuminate\\...</td>\n",
       "      <td>Output / Value Alignment</td>\n",
       "      <td>Toxic Output, Harmful Output</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Question Answering</td>\n",
       "      <td>Field renaming</td>\n",
       "      <td>Basic / Empty</td>\n",
       "      <td>Judge</td>\n",
       "      <td>https://github.com/mlcommons/modelbench/tree/main</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AIrbench 2024</td>\n",
       "      <td>repo: https://github.com/stanford-crfm/air-ben...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Question Answering</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Basic / Empty</td>\n",
       "      <td>Judge</td>\n",
       "      <td>https://github.com/stanford-crfm/air-bench-202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CTI-Bench</td>\n",
       "      <td>repo: https://github.com/xashru/cti-bench\\ndat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Multiple Choice\\nQuestion Answering</td>\n",
       "      <td>FIeld renaming\\nValue aggregation</td>\n",
       "      <td>Basic / Empty</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>https://github.com/xashru/cti-bench/tree/main/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Prompt Injection</td>\n",
       "      <td>repo: https://github.com/meta-llama/PurpleLlam...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>dataset: https://huggingface.co/datasets/cais/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>paper: https://arxiv.org/abs/2403.03218\"</td>\n",
       "      <td>Output / Value Alignment</td>\n",
       "      <td>Harmful Output</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Multiple Choice</td>\n",
       "      <td>Value aggregation</td>\n",
       "      <td>Basic / Empty</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>https://github.com/EleutherAI/lm-evaluation-ha...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>FRR</td>\n",
       "      <td>repo: https://github.com/meta-llama/PurpleLlam...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Generation</td>\n",
       "      <td>Field renaming</td>\n",
       "      <td>Basic / Empty</td>\n",
       "      <td>Judge</td>\n",
       "      <td>https://github.com/meta-llama/PurpleLlama/blob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Ethos</td>\n",
       "      <td>dataset: https://huggingface.co/datasets/iamol...</td>\n",
       "      <td>Output / Value Alignment</td>\n",
       "      <td>Toxic Output</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PopQA</td>\n",
       "      <td>dataset: https://huggingface.co/datasets/akari...</td>\n",
       "      <td>Output / Robustness</td>\n",
       "      <td>Hallucination</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Name  \\\n",
       "0                                                BOLD   \n",
       "1                                               AttaQ   \n",
       "2                                              ProvoQ   \n",
       "3                                         CrowS-Pairs   \n",
       "4                                               Alert   \n",
       "5                                         SALAD_Bench   \n",
       "6                                          SorryBench   \n",
       "7                                             ToxiGen   \n",
       "8                                              XSTest   \n",
       "9                                        StrongReject   \n",
       "10                                  SimpleSafetyTests   \n",
       "11                                                BBQ   \n",
       "12                                       Discrim_eval   \n",
       "13                                            XSafety   \n",
       "14                                         AILuminate   \n",
       "15                                      AIrbench 2024   \n",
       "16                                          CTI-Bench   \n",
       "17                                   Prompt Injection   \n",
       "18  dataset: https://huggingface.co/datasets/cais/...   \n",
       "19           paper: https://arxiv.org/abs/2403.03218\"   \n",
       "20                                                FRR   \n",
       "21                                              Ethos   \n",
       "22                                              PopQA   \n",
       "\n",
       "                                                Links  \\\n",
       "0   repo: https://github.com/amazon-research/bold\\...   \n",
       "1   dataset: https://huggingface.co/datasets/ibm-r...   \n",
       "2   dataset: https://huggingface.co/datasets/ibm-r...   \n",
       "3   repo: https://github.com/nyu-mll/crows-pairs\\n...   \n",
       "4   repo: https://github.com/Babelscape/ALERT\\ndat...   \n",
       "5   repo: https://github.com/OpenSafetyLab/SALAD-B...   \n",
       "6   repo: https://github.com/sorry-bench/sorry-ben...   \n",
       "7   repo: https://github.com/microsoft/TOXIGEN\\nda...   \n",
       "8   repo: https://github.com/paul-rottger/xstest\\n...   \n",
       "9   repo: https://github.com/alexandrasouly/strong...   \n",
       "10  dataset: https://huggingface.co/datasets/Berti...   \n",
       "11  repo: https://github.com/nyu-mll/BBQ\\ndataset:...   \n",
       "12  dataset: https://huggingface.co/datasets/Anthr...   \n",
       "13  repo: https://github.com/jarviswang94/multilin...   \n",
       "14  repo: https://github.com/mlcommons/ailuminate\\...   \n",
       "15  repo: https://github.com/stanford-crfm/air-ben...   \n",
       "16  repo: https://github.com/xashru/cti-bench\\ndat...   \n",
       "17  repo: https://github.com/meta-llama/PurpleLlam...   \n",
       "18                                                NaN   \n",
       "19                           Output / Value Alignment   \n",
       "20  repo: https://github.com/meta-llama/PurpleLlam...   \n",
       "21  dataset: https://huggingface.co/datasets/iamol...   \n",
       "22  dataset: https://huggingface.co/datasets/akari...   \n",
       "\n",
       "                           Risk Mapping (IBM) - Major  \\\n",
       "0                                   Output / Fairness   \n",
       "1                            Output / Value Alignment   \n",
       "2                                   Output / Fairness   \n",
       "3                                   Output / Fairness   \n",
       "4         Output / Fairness, Output / Value Alignment   \n",
       "5   Output / Misuse, Output / Value Alignment, Out...   \n",
       "6                                                 NaN   \n",
       "7                            Output / Value Alignment   \n",
       "8                                                 NaN   \n",
       "9                            Output / Value Alignment   \n",
       "10                           Output / Value Alignment   \n",
       "11                                  Output / Fairness   \n",
       "12                                                NaN   \n",
       "13                                                NaN   \n",
       "14                           Output / Value Alignment   \n",
       "15                                                NaN   \n",
       "16                                                NaN   \n",
       "17                                                NaN   \n",
       "18                                                NaN   \n",
       "19                                     Harmful Output   \n",
       "20                                                NaN   \n",
       "21                           Output / Value Alignment   \n",
       "22                                Output / Robustness   \n",
       "\n",
       "                   Risk Mapping (IBM) - Minor Risk Mapping (MIT) - Major  \\\n",
       "0                                 Output BIas  DIscrimination & Toxicity   \n",
       "1                Toxic Output, Harmful Output                        NaN   \n",
       "2                                 Output BIas                        NaN   \n",
       "3                                 Output BIas                        NaN   \n",
       "4   Toxic Output, Output BIas, Harmful Output                        NaN   \n",
       "5   Toxic Output, Harmful Output, Output BIas                        NaN   \n",
       "6                                         NaN                        NaN   \n",
       "7                                Toxic Output                        NaN   \n",
       "8                                         NaN                        NaN   \n",
       "9                Toxic Output, Harmful Output                        NaN   \n",
       "10                             Harmful Output                        NaN   \n",
       "11                                Output BIas                        NaN   \n",
       "12                                        NaN                        NaN   \n",
       "13                                        NaN                        NaN   \n",
       "14               Toxic Output, Harmful Output                        NaN   \n",
       "15                                        NaN                        NaN   \n",
       "16                                        NaN                        NaN   \n",
       "17                                        NaN                        NaN   \n",
       "18                                        NaN                        NaN   \n",
       "19                                        NaN                        NaN   \n",
       "20                                        NaN                        NaN   \n",
       "21                               Toxic Output                        NaN   \n",
       "22                              Hallucination                        NaN   \n",
       "\n",
       "           Risk Mapping (MIT) - Minor Risk Mapping (MLCommons)  \\\n",
       "0   Unequal performance across groups                      NaN   \n",
       "1                                 NaN                      NaN   \n",
       "2                                 NaN                      NaN   \n",
       "3                                 NaN                      NaN   \n",
       "4                                 NaN                      NaN   \n",
       "5                                 NaN                      NaN   \n",
       "6                                 NaN                      NaN   \n",
       "7                                 NaN                      NaN   \n",
       "8                                 NaN                      NaN   \n",
       "9                                 NaN                      NaN   \n",
       "10                                NaN                      NaN   \n",
       "11                                NaN                      NaN   \n",
       "12                                NaN                      NaN   \n",
       "13                                NaN                      NaN   \n",
       "14                                NaN                      NaN   \n",
       "15                                NaN                      NaN   \n",
       "16                                NaN                      NaN   \n",
       "17                                NaN                      NaN   \n",
       "18                                NaN                      NaN   \n",
       "19                                NaN          Multiple Choice   \n",
       "20                                NaN                      NaN   \n",
       "21                                NaN                      NaN   \n",
       "22                                NaN                      NaN   \n",
       "\n",
       "                                   Task  \\\n",
       "0                            Generation   \n",
       "1                    Question Answering   \n",
       "2                    Question Answering   \n",
       "3                      Multiple Choice?   \n",
       "4                    Question Answering   \n",
       "5    Question Answering Multiple Choice   \n",
       "6                    Question Answering   \n",
       "7                        Classification   \n",
       "8                                   NaN   \n",
       "9                    Question Answering   \n",
       "10                   Question Answering   \n",
       "11                      Multiple Choice   \n",
       "12                      Multiple Choice   \n",
       "13                                  NaN   \n",
       "14                   Question Answering   \n",
       "15                   Question Answering   \n",
       "16  Multiple Choice\\nQuestion Answering   \n",
       "17                                  NaN   \n",
       "18                                  NaN   \n",
       "19                    Value aggregation   \n",
       "20                           Generation   \n",
       "21                                  NaN   \n",
       "22                                  NaN   \n",
       "\n",
       "                                        Preprocessing                Template  \\\n",
       "0   Split renaming\\nField renaming\\nExtract from JSON                     NaN   \n",
       "1                                      Split renaming           Basic / Empty   \n",
       "2                                                 NaN           Basic / Empty   \n",
       "3                                                 NaN                     NaN   \n",
       "4                                      Field renaming           Basic / Empty   \n",
       "5                                      Field renaming           Basic / Empty   \n",
       "6                                                 NaN                     NaN   \n",
       "7                       Value mapping\\nField renaming  Classification request   \n",
       "8                                                 NaN                     NaN   \n",
       "9                                      Field renaming           Basic / Empty   \n",
       "10                                     Field renaming           Basic / Empty   \n",
       "11                  Field renaming\\nValue aggregation         Multiple Choice   \n",
       "12                     Split renaming\\nField renaming         Multiple Choice   \n",
       "13                                                NaN                     NaN   \n",
       "14                                     Field renaming           Basic / Empty   \n",
       "15                                                NaN           Basic / Empty   \n",
       "16                  FIeld renaming\\nValue aggregation           Basic / Empty   \n",
       "17                                                NaN                     NaN   \n",
       "18                                                NaN                     NaN   \n",
       "19                                      Basic / Empty                Accuracy   \n",
       "20                                     Field renaming           Basic / Empty   \n",
       "21                                                NaN                     NaN   \n",
       "22                                                NaN                     NaN   \n",
       "\n",
       "                                               Metric  \\\n",
       "0                                              Regard   \n",
       "1                                        Reward Model   \n",
       "2                                               Judge   \n",
       "3                                                 NaN   \n",
       "4                                               Judge   \n",
       "5                                               Judge   \n",
       "6                                               Judge   \n",
       "7                          F1 (micro/macro)\\nAccuracy   \n",
       "8                                                 NaN   \n",
       "9                                               Judge   \n",
       "10                                              Judge   \n",
       "11                                           Accuracy   \n",
       "12                                           Accuracy   \n",
       "13                                                NaN   \n",
       "14                                              Judge   \n",
       "15                                              Judge   \n",
       "16                                           Accuracy   \n",
       "17                                                NaN   \n",
       "18                                                NaN   \n",
       "19  https://github.com/EleutherAI/lm-evaluation-ha...   \n",
       "20                                              Judge   \n",
       "21                                                NaN   \n",
       "22                                                NaN   \n",
       "\n",
       "                                       Implementation  \n",
       "0   https://github.com/IBM/unitxt/blob/main/prepar...  \n",
       "1   https://github.com/IBM/unitxt/blob/main/prepar...  \n",
       "2   https://github.com/IBM/unitxt/blob/main/prepar...  \n",
       "3   https://github.com/EleutherAI/lm-evaluation-ha...  \n",
       "4   https://github.com/Babelscape/ALERT/blob/maste...  \n",
       "5   https://github.com/OpenSafetyLab/SALAD-BENCH/b...  \n",
       "6          https://github.com/sorry-bench/sorry-bench  \n",
       "7   https://github.com/IBM/unitxt/blob/main/prepar...  \n",
       "8                                                 NaN  \n",
       "9   https://github.com/alexandrasouly/strongreject...  \n",
       "10  https://github.com/IBM/unitxt/blob/main/prepar...  \n",
       "11  https://github.com/IBM/unitxt/blob/main/prepar...  \n",
       "12  https://github.com/IBM/unitxt/blob/main/prepar...  \n",
       "13  https://github.ibm.com/IBM-Research-AI/fm-eval...  \n",
       "14  https://github.com/mlcommons/modelbench/tree/main  \n",
       "15  https://github.com/stanford-crfm/air-bench-202...  \n",
       "16  https://github.com/xashru/cti-bench/tree/main/...  \n",
       "17                                                NaN  \n",
       "18                                                NaN  \n",
       "19                                                NaN  \n",
       "20  https://github.com/meta-llama/PurpleLlama/blob...  \n",
       "21                                                NaN  \n",
       "22                                                NaN  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now going to import benchmarks from excel\n",
    "\n",
    "import pandas as pd\n",
    "data_evals = pd.read_csv('/Users/ingevejs/Downloads/Safety Evals and Benchmarks - Sheet1.csv')\n",
    "data_evals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e597a714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /Users/ingevejs/Documents/workspace/ingelise/risk-atlas-nexus/vrisk-atlas-nexus/lib/python3.11/site-packages (5.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/ingevejs/Documents/workspace/ingelise/risk-atlas-nexus/vrisk-atlas-nexus/lib/python3.11/site-packages (4.13.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/ingevejs/Documents/workspace/ingelise/risk-atlas-nexus/vrisk-atlas-nexus/lib/python3.11/site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/ingevejs/Documents/workspace/ingelise/risk-atlas-nexus/vrisk-atlas-nexus/lib/python3.11/site-packages (from beautifulsoup4) (4.13.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "['Generation']\n",
      "b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D2101.11718%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=2101.11718&amp;start=0&amp;max_results=10</title>\\n  <id>http://arxiv.org/api/+f1JMsZlZZxIuuFSwH0AjRHOFoU</id>\\n  <updated>2025-05-16T00:00:00-04:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">10</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/2101.11718v1</id>\\n    <updated>2021-01-27T22:07:03Z</updated>\\n    <published>2021-01-27T22:07:03Z</published>\\n    <title>BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language\\n  Generation</title>\\n    <summary>  Recent advances in deep learning techniques have enabled machines to generate\\ncohesive open-ended text when prompted with a sequence of words as context.\\nWhile these models now empower many downstream applications from conversation\\nbots to automatic storytelling, they have been shown to generate texts that\\nexhibit social biases. To systematically study and benchmark social biases in\\nopen-ended language generation, we introduce the Bias in Open-Ended Language\\nGeneration Dataset (BOLD), a large-scale dataset that consists of 23,679\\nEnglish text generation prompts for bias benchmarking across five domains:\\nprofession, gender, race, religion, and political ideology. We also propose new\\nautomated metrics for toxicity, psycholinguistic norms, and text gender\\npolarity to measure social biases in open-ended text generation from multiple\\nangles. An examination of text generated from three popular language models\\nreveals that the majority of these models exhibit a larger social bias than\\nhuman-written Wikipedia text across all domains. With these results we\\nhighlight the need to benchmark biases in open-ended language generation and\\ncaution users of language generation models on downstream tasks to be cognizant\\nof these embedded prejudices.\\n</summary>\\n    <author>\\n      <name>Jwala Dhamala</name>\\n    </author>\\n    <author>\\n      <name>Tony Sun</name>\\n    </author>\\n    <author>\\n      <name>Varun Kumar</name>\\n    </author>\\n    <author>\\n      <name>Satyapriya Krishna</name>\\n    </author>\\n    <author>\\n      <name>Yada Pruksachatkun</name>\\n    </author>\\n    <author>\\n      <name>Kai-Wei Chang</name>\\n    </author>\\n    <author>\\n      <name>Rahul Gupta</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1145/3442188.3445924</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1145/3442188.3445924\" rel=\"related\"/>\\n    <link href=\"http://arxiv.org/abs/2101.11718v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2101.11718v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n</feed>\\n'\n",
      "['Question Answering']\n",
      "b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D2311.04124%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=2311.04124&amp;start=0&amp;max_results=10</title>\\n  <id>http://arxiv.org/api/tiUcXXOAA4ofWw25zqctlvFMXKo</id>\\n  <updated>2025-05-16T00:00:00-04:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">10</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/2311.04124v1</id>\\n    <updated>2023-11-07T16:50:33Z</updated>\\n    <published>2023-11-07T16:50:33Z</published>\\n    <title>Unveiling Safety Vulnerabilities of Large Language Models</title>\\n    <summary>  As large language models become more prevalent, their possible harmful or\\ninappropriate responses are a cause for concern. This paper introduces a unique\\ndataset containing adversarial examples in the form of questions, which we call\\nAttaQ, designed to provoke such harmful or inappropriate responses. We assess\\nthe efficacy of our dataset by analyzing the vulnerabilities of various models\\nwhen subjected to it. Additionally, we introduce a novel automatic approach for\\nidentifying and naming vulnerable semantic regions - input semantic areas for\\nwhich the model is likely to produce harmful outputs. This is achieved through\\nthe application of specialized clustering techniques that consider both the\\nsemantic similarity of the input attacks and the harmfulness of the model\\'s\\nresponses. Automatically identifying vulnerable semantic regions enhances the\\nevaluation of model weaknesses, facilitating targeted improvements to its\\nsafety mechanisms and overall reliability.\\n</summary>\\n    <author>\\n      <name>George Kour</name>\\n    </author>\\n    <author>\\n      <name>Marcel Zalmanovici</name>\\n    </author>\\n    <author>\\n      <name>Naama Zwerdling</name>\\n    </author>\\n    <author>\\n      <name>Esther Goldbraich</name>\\n    </author>\\n    <author>\\n      <name>Ora Nova Fandina</name>\\n    </author>\\n    <author>\\n      <name>Ateret Anaby-Tavor</name>\\n    </author>\\n    <author>\\n      <name>Orna Raz</name>\\n    </author>\\n    <author>\\n      <name>Eitan Farchi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">To be published in GEM workshop. Conference on Empirical Methods in\\n  Natural Language Processing (EMNLP). 2023</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2311.04124v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2311.04124v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2.7\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n</feed>\\n'\n",
      "['Question Answering']\n",
      "b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D2311.04124%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=2311.04124&amp;start=0&amp;max_results=10</title>\\n  <id>http://arxiv.org/api/tiUcXXOAA4ofWw25zqctlvFMXKo</id>\\n  <updated>2025-05-16T00:00:00-04:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">10</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/2311.04124v1</id>\\n    <updated>2023-11-07T16:50:33Z</updated>\\n    <published>2023-11-07T16:50:33Z</published>\\n    <title>Unveiling Safety Vulnerabilities of Large Language Models</title>\\n    <summary>  As large language models become more prevalent, their possible harmful or\\ninappropriate responses are a cause for concern. This paper introduces a unique\\ndataset containing adversarial examples in the form of questions, which we call\\nAttaQ, designed to provoke such harmful or inappropriate responses. We assess\\nthe efficacy of our dataset by analyzing the vulnerabilities of various models\\nwhen subjected to it. Additionally, we introduce a novel automatic approach for\\nidentifying and naming vulnerable semantic regions - input semantic areas for\\nwhich the model is likely to produce harmful outputs. This is achieved through\\nthe application of specialized clustering techniques that consider both the\\nsemantic similarity of the input attacks and the harmfulness of the model\\'s\\nresponses. Automatically identifying vulnerable semantic regions enhances the\\nevaluation of model weaknesses, facilitating targeted improvements to its\\nsafety mechanisms and overall reliability.\\n</summary>\\n    <author>\\n      <name>George Kour</name>\\n    </author>\\n    <author>\\n      <name>Marcel Zalmanovici</name>\\n    </author>\\n    <author>\\n      <name>Naama Zwerdling</name>\\n    </author>\\n    <author>\\n      <name>Esther Goldbraich</name>\\n    </author>\\n    <author>\\n      <name>Ora Nova Fandina</name>\\n    </author>\\n    <author>\\n      <name>Ateret Anaby-Tavor</name>\\n    </author>\\n    <author>\\n      <name>Orna Raz</name>\\n    </author>\\n    <author>\\n      <name>Eitan Farchi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">To be published in GEM workshop. Conference on Empirical Methods in\\n  Natural Language Processing (EMNLP). 2023</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2311.04124v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2311.04124v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2.7\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n</feed>\\n'\n",
      "['Multiple Choice?']\n",
      "b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D2010.00133%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=2010.00133&amp;start=0&amp;max_results=10</title>\\n  <id>http://arxiv.org/api/3t7/usgfEBUJb7BjzDzpeMqTRr4</id>\\n  <updated>2025-05-16T00:00:00-04:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">10</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/2010.00133v1</id>\\n    <updated>2020-09-30T22:38:40Z</updated>\\n    <published>2020-09-30T22:38:40Z</published>\\n    <title>CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked\\n  Language Models</title>\\n    <summary>  Pretrained language models, especially masked language models (MLMs) have\\nseen success across many NLP tasks. However, there is ample evidence that they\\nuse the cultural biases that are undoubtedly present in the corpora they are\\ntrained on, implicitly creating harm with biased representations. To measure\\nsome forms of social bias in language models against protected demographic\\ngroups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark\\n(CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing\\nwith nine types of bias, like race, religion, and age. In CrowS-Pairs a model\\nis presented with two sentences: one that is more stereotyping and another that\\nis less stereotyping. The data focuses on stereotypes about historically\\ndisadvantaged groups and contrasts them with advantaged groups. We find that\\nall three of the widely-used MLMs we evaluate substantially favor sentences\\nthat express stereotypes in every category in CrowS-Pairs. As work on building\\nless biased models advances, this dataset can be used as a benchmark to\\nevaluate progress.\\n</summary>\\n    <author>\\n      <name>Nikita Nangia</name>\\n    </author>\\n    <author>\\n      <name>Clara Vania</name>\\n    </author>\\n    <author>\\n      <name>Rasika Bhalerao</name>\\n    </author>\\n    <author>\\n      <name>Samuel R. Bowman</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">EMNLP 2020</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2010.00133v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2010.00133v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n</feed>\\n'\n",
      "['Question Answering']\n",
      "b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D2404.08676%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=2404.08676&amp;start=0&amp;max_results=10</title>\\n  <id>http://arxiv.org/api/jgrV50C1JUH9J/em5mpD1mVi5mM</id>\\n  <updated>2025-05-16T00:00:00-04:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">10</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/2404.08676v3</id>\\n    <updated>2024-06-24T08:50:22Z</updated>\\n    <published>2024-04-06T15:01:47Z</published>\\n    <title>ALERT: A Comprehensive Benchmark for Assessing Large Language Models\\'\\n  Safety through Red Teaming</title>\\n    <summary>  When building Large Language Models (LLMs), it is paramount to bear safety in\\nmind and protect them with guardrails. Indeed, LLMs should never generate\\ncontent promoting or normalizing harmful, illegal, or unethical behavior that\\nmay contribute to harm to individuals or society. This principle applies to\\nboth normal and adversarial use. In response, we introduce ALERT, a large-scale\\nbenchmark to assess safety based on a novel fine-grained risk taxonomy. It is\\ndesigned to evaluate the safety of LLMs through red teaming methodologies and\\nconsists of more than 45k instructions categorized using our novel taxonomy. By\\nsubjecting LLMs to adversarial testing scenarios, ALERT aims to identify\\nvulnerabilities, inform improvements, and enhance the overall safety of the\\nlanguage models. Furthermore, the fine-grained taxonomy enables researchers to\\nperform an in-depth evaluation that also helps one to assess the alignment with\\nvarious policies. In our experiments, we extensively evaluate 10 popular open-\\nand closed-source LLMs and demonstrate that many of them still struggle to\\nattain reasonable levels of safety.\\n</summary>\\n    <author>\\n      <name>Simone Tedeschi</name>\\n    </author>\\n    <author>\\n      <name>Felix Friedrich</name>\\n    </author>\\n    <author>\\n      <name>Patrick Schramowski</name>\\n    </author>\\n    <author>\\n      <name>Kristian Kersting</name>\\n    </author>\\n    <author>\\n      <name>Roberto Navigli</name>\\n    </author>\\n    <author>\\n      <name>Huu Nguyen</name>\\n    </author>\\n    <author>\\n      <name>Bo Li</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">17 pages, preprint</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2404.08676v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2404.08676v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n</feed>\\n'\n",
      "['Question Answering Multiple Choice']\n",
      "b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D2402.05044%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=2402.05044&amp;start=0&amp;max_results=10</title>\\n  <id>http://arxiv.org/api/5AwmrUrbqtxfVugFa89W63sTIwc</id>\\n  <updated>2025-05-16T00:00:00-04:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">10</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/2402.05044v4</id>\\n    <updated>2024-06-07T12:05:46Z</updated>\\n    <published>2024-02-07T17:33:54Z</published>\\n    <title>SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large\\n  Language Models</title>\\n    <summary>  In the rapidly evolving landscape of Large Language Models (LLMs), ensuring\\nrobust safety measures is paramount. To meet this crucial need, we propose\\n\\\\emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating\\nLLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench\\ntranscends conventional benchmarks through its large scale, rich diversity,\\nintricate taxonomy spanning three levels, and versatile\\nfunctionalities.SALAD-Bench is crafted with a meticulous array of questions,\\nfrom standard queries to complex ones enriched with attack, defense\\nmodifications and multiple-choice. To effectively manage the inherent\\ncomplexity, we introduce an innovative evaluators: the LLM-based MD-Judge for\\nQA pairs with a particular focus on attack-enhanced queries, ensuring a\\nseamless, and reliable evaluation. Above components extend SALAD-Bench from\\nstandard LLM safety evaluation to both LLM attack and defense methods\\nevaluation, ensuring the joint-purpose utility. Our extensive experiments shed\\nlight on the resilience of LLMs against emerging threats and the efficacy of\\ncontemporary defense tactics. Data and evaluator are released under\\nhttps://github.com/OpenSafetyLab/SALAD-BENCH.\\n</summary>\\n    <author>\\n      <name>Lijun Li</name>\\n    </author>\\n    <author>\\n      <name>Bowen Dong</name>\\n    </author>\\n    <author>\\n      <name>Ruohui Wang</name>\\n    </author>\\n    <author>\\n      <name>Xuhao Hu</name>\\n    </author>\\n    <author>\\n      <name>Wangmeng Zuo</name>\\n    </author>\\n    <author>\\n      <name>Dahua Lin</name>\\n    </author>\\n    <author>\\n      <name>Yu Qiao</name>\\n    </author>\\n    <author>\\n      <name>Jing Shao</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted at ACL 2024 Findings</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2402.05044v4\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.05044v4\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n</feed>\\n'\n",
      "['Question Answering']\n",
      "b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D2406.14598%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=2406.14598&amp;start=0&amp;max_results=10</title>\\n  <id>http://arxiv.org/api/YmMnYKzUyjCj1CtaM28pWvK/GKc</id>\\n  <updated>2025-05-16T00:00:00-04:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">10</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/2406.14598v2</id>\\n    <updated>2025-03-01T21:45:36Z</updated>\\n    <published>2024-06-20T17:56:07Z</published>\\n    <title>SORRY-Bench: Systematically Evaluating Large Language Model Safety\\n  Refusal</title>\\n    <summary>  Evaluating aligned large language models\\' (LLMs) ability to recognize and\\nreject unsafe user requests is crucial for safe, policy-compliant deployments.\\nExisting evaluation efforts, however, face three limitations that we address\\nwith SORRY-Bench, our proposed benchmark. First, existing methods often use\\ncoarse-grained taxonomies of unsafe topics, and are over-representing some\\nfine-grained topics. For example, among the ten existing datasets that we\\nevaluated, tests for refusals of self-harm instructions are over 3x less\\nrepresented than tests for fraudulent activities. SORRY-Bench improves on this\\nby using a fine-grained taxonomy of 44 potentially unsafe topics, and 440\\nclass-balanced unsafe instructions, compiled through human-in-the-loop methods.\\nSecond, linguistic characteristics and formatting of prompts are often\\noverlooked, like different languages, dialects, and more -- which are only\\nimplicitly considered in many evaluations. We supplement SORRY-Bench with 20\\ndiverse linguistic augmentations to systematically examine these effects.\\nThird, existing evaluations rely on large LLMs (e.g., GPT-4) for evaluation,\\nwhich can be computationally expensive. We investigate design choices for\\ncreating a fast, accurate automated safety evaluator. By collecting 7K+ human\\nannotations and conducting a meta-evaluation of diverse LLM-as-a-judge designs,\\nwe show that fine-tuned 7B LLMs can achieve accuracy comparable to GPT-4 scale\\nLLMs, with lower computational cost. Putting these together, we evaluate over\\n50 proprietary and open-weight LLMs on SORRY-Bench, analyzing their distinctive\\nsafety refusal behaviors. We hope our effort provides a building block for\\nsystematic evaluations of LLMs\\' safety refusal capabilities, in a balanced,\\ngranular, and efficient manner. Benchmark demo, data, code, and models are\\navailable through https://sorry-bench.github.io.\\n</summary>\\n    <author>\\n      <name>Tinghao Xie</name>\\n    </author>\\n    <author>\\n      <name>Xiangyu Qi</name>\\n    </author>\\n    <author>\\n      <name>Yi Zeng</name>\\n    </author>\\n    <author>\\n      <name>Yangsibo Huang</name>\\n    </author>\\n    <author>\\n      <name>Udari Madhushani Sehwag</name>\\n    </author>\\n    <author>\\n      <name>Kaixuan Huang</name>\\n    </author>\\n    <author>\\n      <name>Luxi He</name>\\n    </author>\\n    <author>\\n      <name>Boyi Wei</name>\\n    </author>\\n    <author>\\n      <name>Dacheng Li</name>\\n    </author>\\n    <author>\\n      <name>Ying Sheng</name>\\n    </author>\\n    <author>\\n      <name>Ruoxi Jia</name>\\n    </author>\\n    <author>\\n      <name>Bo Li</name>\\n    </author>\\n    <author>\\n      <name>Kai Li</name>\\n    </author>\\n    <author>\\n      <name>Danqi Chen</name>\\n    </author>\\n    <author>\\n      <name>Peter Henderson</name>\\n    </author>\\n    <author>\\n      <name>Prateek Mittal</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Paper accepted to ICLR 2025</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2406.14598v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2406.14598v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n</feed>\\n'\n",
      "['Classification']\n",
      "b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D2203.09509%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=2203.09509&amp;start=0&amp;max_results=10</title>\\n  <id>http://arxiv.org/api/I78n3mLPKIy1tiJ8icyQxBJS7d4</id>\\n  <updated>2025-05-16T00:00:00-04:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">10</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/2203.09509v4</id>\\n    <updated>2022-07-14T13:04:29Z</updated>\\n    <published>2022-03-17T17:57:56Z</published>\\n    <title>ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and\\n  Implicit Hate Speech Detection</title>\\n    <summary>  Toxic language detection systems often falsely flag text that contains\\nminority group mentions as toxic, as those groups are often the targets of\\nonline hate. Such over-reliance on spurious correlations also causes systems to\\nstruggle with detecting implicitly toxic language. To help mitigate these\\nissues, we create ToxiGen, a new large-scale and machine-generated dataset of\\n274k toxic and benign statements about 13 minority groups. We develop a\\ndemonstration-based prompting framework and an adversarial\\nclassifier-in-the-loop decoding method to generate subtly toxic and benign text\\nwith a massive pretrained language model. Controlling machine generation in\\nthis way allows ToxiGen to cover implicitly toxic text at a larger scale, and\\nabout more demographic groups, than previous resources of human-written text.\\nWe conduct a human evaluation on a challenging subset of ToxiGen and find that\\nannotators struggle to distinguish machine-generated text from human-written\\nlanguage. We also find that 94.5% of toxic examples are labeled as hate speech\\nby human annotators. Using three publicly-available datasets, we show that\\nfinetuning a toxicity classifier on our data improves its performance on\\nhuman-written data substantially. We also demonstrate that ToxiGen can be used\\nto fight machine-generated toxicity as finetuning improves the classifier\\nsignificantly on our evaluation subset. Our code and data can be found at\\nhttps://github.com/microsoft/ToxiGen.\\n</summary>\\n    <author>\\n      <name>Thomas Hartvigsen</name>\\n    </author>\\n    <author>\\n      <name>Saadia Gabriel</name>\\n    </author>\\n    <author>\\n      <name>Hamid Palangi</name>\\n    </author>\\n    <author>\\n      <name>Maarten Sap</name>\\n    </author>\\n    <author>\\n      <name>Dipankar Ray</name>\\n    </author>\\n    <author>\\n      <name>Ece Kamar</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Published as a long paper at ACL 2022. Code:\\n  https://github.com/microsoft/TOXIGEN</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2203.09509v4\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2203.09509v4\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n</feed>\\n'\n",
      "[nan]\n",
      "b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D2308.01263%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=2308.01263&amp;start=0&amp;max_results=10</title>\\n  <id>http://arxiv.org/api/VoL0WYt+UqtODJvjgsAmgkn++Ts</id>\\n  <updated>2025-05-16T00:00:00-04:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">10</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/2308.01263v3</id>\\n    <updated>2024-04-01T11:50:35Z</updated>\\n    <published>2023-08-02T16:30:40Z</published>\\n    <title>XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in\\n  Large Language Models</title>\\n    <summary>  Without proper safeguards, large language models will readily follow\\nmalicious instructions and generate toxic content. This risk motivates safety\\nefforts such as red-teaming and large-scale feedback learning, which aim to\\nmake models both helpful and harmless. However, there is a tension between\\nthese two objectives, since harmlessness requires models to refuse to comply\\nwith unsafe prompts, and thus not be helpful. Recent anecdotal evidence\\nsuggests that some models may have struck a poor balance, so that even clearly\\nsafe prompts are refused if they use similar language to unsafe prompts or\\nmention sensitive topics. In this paper, we introduce a new test suite called\\nXSTest to identify such eXaggerated Safety behaviours in a systematic way.\\nXSTest comprises 250 safe prompts across ten prompt types that well-calibrated\\nmodels should not refuse to comply with, and 200 unsafe prompts as contrasts\\nthat models, for most applications, should refuse. We describe XSTest\\'s\\ncreation and composition, and then use the test suite to highlight systematic\\nfailure modes in state-of-the-art language models as well as more general\\nchallenges in building safer language models.\\n</summary>\\n    <author>\\n      <name>Paul R\\xc3\\xb6ttger</name>\\n    </author>\\n    <author>\\n      <name>Hannah Rose Kirk</name>\\n    </author>\\n    <author>\\n      <name>Bertie Vidgen</name>\\n    </author>\\n    <author>\\n      <name>Giuseppe Attanasio</name>\\n    </author>\\n    <author>\\n      <name>Federico Bianchi</name>\\n    </author>\\n    <author>\\n      <name>Dirk Hovy</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted at NAACL 2024 (Main Conference)</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2308.01263v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2308.01263v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n</feed>\\n'\n",
      "['Question Answering']\n",
      "['Question Answering']\n",
      "b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D2311.08370%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=2311.08370&amp;start=0&amp;max_results=10</title>\\n  <id>http://arxiv.org/api/9Rp6A83MHSzAWn+SUiVS92Rw1FQ</id>\\n  <updated>2025-05-16T00:00:00-04:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">10</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/2311.08370v2</id>\\n    <updated>2024-02-16T09:42:19Z</updated>\\n    <published>2023-11-14T18:33:43Z</published>\\n    <title>SimpleSafetyTests: a Test Suite for Identifying Critical Safety Risks in\\n  Large Language Models</title>\\n    <summary>  The past year has seen rapid acceleration in the development of large\\nlanguage models (LLMs). However, without proper steering and safeguards, LLMs\\nwill readily follow malicious instructions, provide unsafe advice, and generate\\ntoxic content. We introduce SimpleSafetyTests (SST) as a new test suite for\\nrapidly and systematically identifying such critical safety risks. The test\\nsuite comprises 100 test prompts across five harm areas that LLMs, for the vast\\nmajority of applications, should refuse to comply with. We test 11 open-access\\nand open-source LLMs and four closed-source LLMs, and find critical safety\\nweaknesses. While some of the models do not give a single unsafe response, most\\ngive unsafe responses to more than 20% of the prompts, with over 50% unsafe\\nresponses in the extreme. Prepending a safety-emphasising system prompt\\nsubstantially reduces the occurrence of unsafe responses, but does not\\ncompletely stop them from happening. Trained annotators labelled every model\\nresponse to SST (n = 3,000). We use these annotations to evaluate five AI\\nsafety filters (which assess whether a models\\' response is unsafe given a\\nprompt) as a way of automatically evaluating models\\' performance on SST. The\\nfilters\\' performance varies considerably. There are also differences across the\\nfive harm areas, and on the unsafe versus safe responses. The widely-used\\nPerspective API has 72% accuracy and a newly-created zero-shot prompt to\\nOpenAI\\'s GPT-4 performs best with 89% accuracy. Content Warning: This paper\\ncontains prompts and responses that relate to child abuse, suicide, self-harm\\nand eating disorders, scams and fraud, illegal items, and physical harm.\\n</summary>\\n    <author>\\n      <name>Bertie Vidgen</name>\\n    </author>\\n    <author>\\n      <name>Nino Scherrer</name>\\n    </author>\\n    <author>\\n      <name>Hannah Rose Kirk</name>\\n    </author>\\n    <author>\\n      <name>Rebecca Qian</name>\\n    </author>\\n    <author>\\n      <name>Anand Kannappan</name>\\n    </author>\\n    <author>\\n      <name>Scott A. Hale</name>\\n    </author>\\n    <author>\\n      <name>Paul R\\xc3\\xb6ttger</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2311.08370v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2311.08370v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n</feed>\\n'\n",
      "['Multiple Choice']\n",
      "b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D2110.08193%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=2110.08193&amp;start=0&amp;max_results=10</title>\\n  <id>http://arxiv.org/api/CP+6LSkJGYPZ7Na5q4v1XYR/+x4</id>\\n  <updated>2025-05-16T00:00:00-04:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">10</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/2110.08193v2</id>\\n    <updated>2022-03-16T01:35:45Z</updated>\\n    <published>2021-10-15T16:43:46Z</published>\\n    <title>BBQ: A Hand-Built Bias Benchmark for Question Answering</title>\\n    <summary>  It is well documented that NLP models learn social biases, but little work\\nhas been done on how these biases manifest in model outputs for applied tasks\\nlike question answering (QA). We introduce the Bias Benchmark for QA (BBQ), a\\ndataset of question sets constructed by the authors that highlight attested\\nsocial biases against people belonging to protected classes along nine social\\ndimensions relevant for U.S. English-speaking contexts. Our task evaluates\\nmodel responses at two levels: (i) given an under-informative context, we test\\nhow strongly responses reflect social biases, and (ii) given an adequately\\ninformative context, we test whether the model\\'s biases override a correct\\nanswer choice. We find that models often rely on stereotypes when the context\\nis under-informative, meaning the model\\'s outputs consistently reproduce\\nharmful biases in this setting. Though models are more accurate when the\\ncontext provides an informative answer, they still rely on stereotypes and\\naverage up to 3.4 percentage points higher accuracy when the correct answer\\naligns with a social bias than when it conflicts, with this difference widening\\nto over 5 points on examples targeting gender for most models tested.\\n</summary>\\n    <author>\\n      <name>Alicia Parrish</name>\\n    </author>\\n    <author>\\n      <name>Angelica Chen</name>\\n    </author>\\n    <author>\\n      <name>Nikita Nangia</name>\\n    </author>\\n    <author>\\n      <name>Vishakh Padmakumar</name>\\n    </author>\\n    <author>\\n      <name>Jason Phang</name>\\n    </author>\\n    <author>\\n      <name>Jana Thompson</name>\\n    </author>\\n    <author>\\n      <name>Phu Mon Htut</name>\\n    </author>\\n    <author>\\n      <name>Samuel R. Bowman</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted to ACL 2022 Findings. 20 pages, 10 figures</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2110.08193v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2110.08193v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n</feed>\\n'\n",
      "['Multiple Choice']\n",
      "b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D2312.03689%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=2312.03689&amp;start=0&amp;max_results=10</title>\\n  <id>http://arxiv.org/api/WLW9iQcZ1kX4zFOaXnZyo6dnEwQ</id>\\n  <updated>2025-05-16T00:00:00-04:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">10</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/2312.03689v1</id>\\n    <updated>2023-12-06T18:53:01Z</updated>\\n    <published>2023-12-06T18:53:01Z</published>\\n    <title>Evaluating and Mitigating Discrimination in Language Model Decisions</title>\\n    <summary>  As language models (LMs) advance, interest is growing in applying them to\\nhigh-stakes societal decisions, such as determining financing or housing\\neligibility. However, their potential for discrimination in such contexts\\nraises ethical concerns, motivating the need for better methods to evaluate\\nthese risks. We present a method for proactively evaluating the potential\\ndiscriminatory impact of LMs in a wide range of use cases, including\\nhypothetical use cases where they have not yet been deployed. Specifically, we\\nuse an LM to generate a wide array of potential prompts that decision-makers\\nmay input into an LM, spanning 70 diverse decision scenarios across society,\\nand systematically vary the demographic information in each prompt. Applying\\nthis methodology reveals patterns of both positive and negative discrimination\\nin the Claude 2.0 model in select settings when no interventions are applied.\\nWhile we do not endorse or permit the use of language models to make automated\\ndecisions for the high-risk use cases we study, we demonstrate techniques to\\nsignificantly decrease both positive and negative discrimination through\\ncareful prompt engineering, providing pathways toward safer deployment in use\\ncases where they may be appropriate. Our work enables developers and\\npolicymakers to anticipate, measure, and address discrimination as language\\nmodel capabilities and applications continue to expand. We release our dataset\\nand prompts at https://huggingface.co/datasets/Anthropic/discrim-eval\\n</summary>\\n    <author>\\n      <name>Alex Tamkin</name>\\n    </author>\\n    <author>\\n      <name>Amanda Askell</name>\\n    </author>\\n    <author>\\n      <name>Liane Lovitt</name>\\n    </author>\\n    <author>\\n      <name>Esin Durmus</name>\\n    </author>\\n    <author>\\n      <name>Nicholas Joseph</name>\\n    </author>\\n    <author>\\n      <name>Shauna Kravec</name>\\n    </author>\\n    <author>\\n      <name>Karina Nguyen</name>\\n    </author>\\n    <author>\\n      <name>Jared Kaplan</name>\\n    </author>\\n    <author>\\n      <name>Deep Ganguli</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2312.03689v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2312.03689v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n</feed>\\n'\n",
      "[nan]\n",
      "b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D2310.00905%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=2310.00905&amp;start=0&amp;max_results=10</title>\\n  <id>http://arxiv.org/api/Lm+O3UoFS9JxeqzlE2yltu9KANo</id>\\n  <updated>2025-05-16T00:00:00-04:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">10</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/2310.00905v2</id>\\n    <updated>2024-06-20T14:15:23Z</updated>\\n    <published>2023-10-02T05:23:34Z</published>\\n    <title>All Languages Matter: On the Multilingual Safety of Large Language\\n  Models</title>\\n    <summary>  Safety lies at the core of developing and deploying large language models\\n(LLMs). However, previous safety benchmarks only concern the safety in one\\nlanguage, e.g. the majority language in the pretraining data such as English.\\nIn this work, we build the first multilingual safety benchmark for LLMs,\\nXSafety, in response to the global deployment of LLMs in practice. XSafety\\ncovers 14 kinds of commonly used safety issues across 10 languages that span\\nseveral language families. We utilize XSafety to empirically study the\\nmultilingual safety for 4 widely-used LLMs, including both close-API and\\nopen-source models. Experimental results show that all LLMs produce\\nsignificantly more unsafe responses for non-English queries than English ones,\\nindicating the necessity of developing safety alignment for non-English\\nlanguages. In addition, we propose several simple and effective prompting\\nmethods to improve the multilingual safety of ChatGPT by evoking safety\\nknowledge and improving cross-lingual generalization of safety alignment. Our\\nprompting method can significantly reduce the ratio of unsafe responses from\\n19.1% to 9.7% for non-English queries. We release our data at\\nhttps://github.com/Jarviswang94/Multilingual_safety_benchmark.\\n</summary>\\n    <author>\\n      <name>Wenxuan Wang</name>\\n    </author>\\n    <author>\\n      <name>Zhaopeng Tu</name>\\n    </author>\\n    <author>\\n      <name>Chang Chen</name>\\n    </author>\\n    <author>\\n      <name>Youliang Yuan</name>\\n    </author>\\n    <author>\\n      <name>Jen-tse Huang</name>\\n    </author>\\n    <author>\\n      <name>Wenxiang Jiao</name>\\n    </author>\\n    <author>\\n      <name>Michael R. Lyu</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted by ACL 2024 Findings. The first multilingual safety\\n  benchmark for large language models</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2310.00905v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2310.00905v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n</feed>\\n'\n",
      "['Question Answering']\n",
      "['Question Answering']\n",
      "['Multiple Choice\\nQuestion Answering']\n",
      "b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D2406.07599%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=2406.07599&amp;start=0&amp;max_results=10</title>\\n  <id>http://arxiv.org/api/C2mmxLT5gKDGtMJ9/ty44MNIZqE</id>\\n  <updated>2025-05-16T00:00:00-04:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">10</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/2406.07599v3</id>\\n    <updated>2024-11-11T12:00:35Z</updated>\\n    <published>2024-06-11T16:42:02Z</published>\\n    <title>CTIBench: A Benchmark for Evaluating LLMs in Cyber Threat Intelligence</title>\\n    <summary>  Cyber threat intelligence (CTI) is crucial in today\\'s cybersecurity\\nlandscape, providing essential insights to understand and mitigate the\\never-evolving cyber threats. The recent rise of Large Language Models (LLMs)\\nhave shown potential in this domain, but concerns about their reliability,\\naccuracy, and hallucinations persist. While existing benchmarks provide general\\nevaluations of LLMs, there are no benchmarks that address the practical and\\napplied aspects of CTI-specific tasks. To bridge this gap, we introduce\\nCTIBench, a benchmark designed to assess LLMs\\' performance in CTI applications.\\nCTIBench includes multiple datasets focused on evaluating knowledge acquired by\\nLLMs in the cyber-threat landscape. Our evaluation of several state-of-the-art\\nmodels on these tasks provides insights into their strengths and weaknesses in\\nCTI contexts, contributing to a better understanding of LLM capabilities in\\nCTI.\\n</summary>\\n    <author>\\n      <name>Md Tanvirul Alam</name>\\n    </author>\\n    <author>\\n      <name>Dipkamal Bhusal</name>\\n    </author>\\n    <author>\\n      <name>Le Nguyen</name>\\n    </author>\\n    <author>\\n      <name>Nidhi Rastogi</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2406.07599v3\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2406.07599v3\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n</feed>\\n'\n",
      "[nan]\n",
      "b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D2408.01605%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=2408.01605&amp;start=0&amp;max_results=10</title>\\n  <id>http://arxiv.org/api/G47IE+M6oT24T1Lv59bzri2/joA</id>\\n  <updated>2025-05-16T00:00:00-04:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">10</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/2408.01605v2</id>\\n    <updated>2024-09-06T18:17:07Z</updated>\\n    <published>2024-08-02T23:47:27Z</published>\\n    <title>CYBERSECEVAL 3: Advancing the Evaluation of Cybersecurity Risks and\\n  Capabilities in Large Language Models</title>\\n    <summary>  We are releasing a new suite of security benchmarks for LLMs, CYBERSECEVAL 3,\\nto continue the conversation on empirically measuring LLM cybersecurity risks\\nand capabilities. CYBERSECEVAL 3 assesses 8 different risks across two broad\\ncategories: risk to third parties, and risk to application developers and end\\nusers. Compared to previous work, we add new areas focused on offensive\\nsecurity capabilities: automated social engineering, scaling manual offensive\\ncyber operations, and autonomous offensive cyber operations. In this paper we\\ndiscuss applying these benchmarks to the Llama 3 models and a suite of\\ncontemporaneous state-of-the-art LLMs, enabling us to contextualize risks both\\nwith and without mitigations in place.\\n</summary>\\n    <author>\\n      <name>Shengye Wan</name>\\n    </author>\\n    <author>\\n      <name>Cyrus Nikolaidis</name>\\n    </author>\\n    <author>\\n      <name>Daniel Song</name>\\n    </author>\\n    <author>\\n      <name>David Molnar</name>\\n    </author>\\n    <author>\\n      <name>James Crnkovich</name>\\n    </author>\\n    <author>\\n      <name>Jayson Grace</name>\\n    </author>\\n    <author>\\n      <name>Manish Bhatt</name>\\n    </author>\\n    <author>\\n      <name>Sahana Chennabasappa</name>\\n    </author>\\n    <author>\\n      <name>Spencer Whitman</name>\\n    </author>\\n    <author>\\n      <name>Stephanie Ding</name>\\n    </author>\\n    <author>\\n      <name>Vlad Ionescu</name>\\n    </author>\\n    <author>\\n      <name>Yue Li</name>\\n    </author>\\n    <author>\\n      <name>Joshua Saxe</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2408.01605v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2408.01605v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n</feed>\\n'\n",
      "[nan]\n",
      "['Value aggregation']\n",
      "['Generation']\n",
      "b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D2404.13161%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=2404.13161&amp;start=0&amp;max_results=10</title>\\n  <id>http://arxiv.org/api/FSwlsa21oHRv+m8q8lFJIaIC+AY</id>\\n  <updated>2025-05-16T00:00:00-04:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">10</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/2404.13161v1</id>\\n    <updated>2024-04-19T20:11:12Z</updated>\\n    <published>2024-04-19T20:11:12Z</published>\\n    <title>CyberSecEval 2: A Wide-Ranging Cybersecurity Evaluation Suite for Large\\n  Language Models</title>\\n    <summary>  Large language models (LLMs) introduce new security risks, but there are few\\ncomprehensive evaluation suites to measure and reduce these risks. We present\\nBenchmarkName, a novel benchmark to quantify LLM security risks and\\ncapabilities. We introduce two new areas for testing: prompt injection and code\\ninterpreter abuse. We evaluated multiple state-of-the-art (SOTA) LLMs,\\nincluding GPT-4, Mistral, Meta Llama 3 70B-Instruct, and Code Llama. Our\\nresults show that conditioning away risk of attack remains an unsolved problem;\\nfor example, all tested models showed between 26% and 41% successful prompt\\ninjection tests. We further introduce the safety-utility tradeoff: conditioning\\nan LLM to reject unsafe prompts can cause the LLM to falsely reject answering\\nbenign prompts, which lowers utility. We propose quantifying this tradeoff\\nusing False Refusal Rate (FRR). As an illustration, we introduce a novel test\\nset to quantify FRR for cyberattack helpfulness risk. We find many LLMs able to\\nsuccessfully comply with \"borderline\" benign requests while still rejecting\\nmost unsafe requests. Finally, we quantify the utility of LLMs for automating a\\ncore cybersecurity task, that of exploiting software vulnerabilities. This is\\nimportant because the offensive capabilities of LLMs are of intense interest;\\nwe quantify this by creating novel test sets for four representative problems.\\nWe find that models with coding capabilities perform better than those without,\\nbut that further work is needed for LLMs to become proficient at exploit\\ngeneration. Our code is open source and can be used to evaluate other LLMs.\\n</summary>\\n    <author>\\n      <name>Manish Bhatt</name>\\n    </author>\\n    <author>\\n      <name>Sahana Chennabasappa</name>\\n    </author>\\n    <author>\\n      <name>Yue Li</name>\\n    </author>\\n    <author>\\n      <name>Cyrus Nikolaidis</name>\\n    </author>\\n    <author>\\n      <name>Daniel Song</name>\\n    </author>\\n    <author>\\n      <name>Shengye Wan</name>\\n    </author>\\n    <author>\\n      <name>Faizan Ahmad</name>\\n    </author>\\n    <author>\\n      <name>Cornelius Aschermann</name>\\n    </author>\\n    <author>\\n      <name>Yaohui Chen</name>\\n    </author>\\n    <author>\\n      <name>Dhaval Kapil</name>\\n    </author>\\n    <author>\\n      <name>David Molnar</name>\\n    </author>\\n    <author>\\n      <name>Spencer Whitman</name>\\n    </author>\\n    <author>\\n      <name>Joshua Saxe</name>\\n    </author>\\n    <link href=\"http://arxiv.org/abs/2404.13161v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2404.13161v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n</feed>\\n'\n",
      "[nan]\n",
      "b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D2006.08328%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=2006.08328&amp;start=0&amp;max_results=10</title>\\n  <id>http://arxiv.org/api/vE76cTNWHTuKVR6D+5JoqGZuIV8</id>\\n  <updated>2025-05-16T00:00:00-04:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">10</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/2006.08328v2</id>\\n    <updated>2021-07-06T07:25:14Z</updated>\\n    <published>2020-06-11T08:59:57Z</published>\\n    <title>ETHOS: an Online Hate Speech Detection Dataset</title>\\n    <summary>  Online hate speech is a recent problem in our society that is rising at a\\nsteady pace by leveraging the vulnerabilities of the corresponding regimes that\\ncharacterise most social media platforms. This phenomenon is primarily fostered\\nby offensive comments, either during user interaction or in the form of a\\nposted multimedia context. Nowadays, giant corporations own platforms where\\nmillions of users log in every day, and protection from exposure to similar\\nphenomena appears to be necessary in order to comply with the corresponding\\nlegislation and maintain a high level of service quality. A robust and reliable\\nsystem for detecting and preventing the uploading of relevant content will have\\na significant impact on our digitally interconnected society. Several aspects\\nof our daily lives are undeniably linked to our social profiles, making us\\nvulnerable to abusive behaviours. As a result, the lack of accurate hate speech\\ndetection mechanisms would severely degrade the overall user experience,\\nalthough its erroneous operation would pose many ethical concerns. In this\\npaper, we present \\'ETHOS\\', a textual dataset with two variants: binary and\\nmulti-label, based on YouTube and Reddit comments validated using the\\nFigure-Eight crowdsourcing platform. Furthermore, we present the annotation\\nprotocol used to create this dataset: an active sampling procedure for\\nbalancing our data in relation to the various aspects defined. Our key\\nassumption is that, even gaining a small amount of labelled data from such a\\ntime-consuming process, we can guarantee hate speech occurrences in the\\nexamined material.\\n</summary>\\n    <author>\\n      <name>Ioannis Mollas</name>\\n    </author>\\n    <author>\\n      <name>Zoe Chrysopoulou</name>\\n    </author>\\n    <author>\\n      <name>Stamatis Karlos</name>\\n    </author>\\n    <author>\\n      <name>Grigorios Tsoumakas</name>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1007/s40747-021-00608-2</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1007/s40747-021-00608-2\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">16 Pages, 3 Figures, 9 Tables, Submitted to the special issue on\\n  \"Intelligent Systems for Safer Social Media\" of Complex &amp; Intelligent Systems</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2006.08328v2\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2006.08328v2\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"I.2.6; I.2.7; I.5.4; H.2.4\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n</feed>\\n'\n",
      "[nan]\n",
      "b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D2212.10511%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=2212.10511&amp;start=0&amp;max_results=10</title>\\n  <id>http://arxiv.org/api/s/DxxbTo1CQTsQYjkLsaOYSJg2A</id>\\n  <updated>2025-05-16T00:00:00-04:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">10</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/2212.10511v4</id>\\n    <updated>2023-07-02T07:21:59Z</updated>\\n    <published>2022-12-20T18:30:15Z</published>\\n    <title>When Not to Trust Language Models: Investigating Effectiveness of\\n  Parametric and Non-Parametric Memories</title>\\n    <summary>  Despite their impressive performance on diverse tasks, large language models\\n(LMs) still struggle with tasks requiring rich world knowledge, implying the\\nlimitations of relying solely on their parameters to encode a wealth of world\\nknowledge. This paper aims to understand LMs\\' strengths and limitations in\\nmemorizing factual knowledge, by conducting large-scale knowledge probing\\nexperiments of 10 models and 4 augmentation methods on PopQA, our new\\nopen-domain QA dataset with 14k questions. We find that LMs struggle with less\\npopular factual knowledge, and that scaling fails to appreciably improve\\nmemorization of factual knowledge in the long tail. We then show that\\nretrieval-augmented LMs largely outperform orders of magnitude larger LMs,\\nwhile unassisted LMs remain competitive in questions about high-popularity\\nentities. Based on those findings, we devise a simple, yet effective, method\\nfor powerful and efficient retrieval-augmented LMs, which retrieves\\nnon-parametric memories only when necessary. Experimental results show that\\nthis significantly improves models\\' performance while reducing the inference\\ncosts.\\n</summary>\\n    <author>\\n      <name>Alex Mallen</name>\\n    </author>\\n    <author>\\n      <name>Akari Asai</name>\\n    </author>\\n    <author>\\n      <name>Victor Zhong</name>\\n    </author>\\n    <author>\\n      <name>Rajarshi Das</name>\\n    </author>\\n    <author>\\n      <name>Daniel Khashabi</name>\\n    </author>\\n    <author>\\n      <name>Hannaneh Hajishirzi</name>\\n    </author>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">ACL 2023; Code and data available at\\n  https://github.com/AlexTMallen/adaptive-retrieval</arxiv:comment>\\n    <link href=\"http://arxiv.org/abs/2212.10511v4\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2212.10511v4\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n</feed>\\n'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[AiEval(id='ai_eval_BOLD', name='BOLD', description=None, url='https://github.com/amazon-research/bold\\n', dateCreated=None, dateModified=None, hasDocumentation=['https://arxiv.org/abs/2101.11718  '], hasDataset=['https://huggingface.co/datasets/AlexaAI/bold\\n'], hasTasks=['Generation'], hasImplementation=['https://github.com/IBM/unitxt/blob/main/prepare/cards/bold.py'], hasUnitxtCard=[], hasLicense=None, hasRelatedRisk=['Output BIas'], bestValue=None, hasBenchmarkMetadata=None),\n",
       " AiEval(id='ai_eval_AttaQ', name='AttaQ', description=None, url='', dateCreated=None, dateModified=None, hasDocumentation=['https://arxiv.org/abs/2311.04124'], hasDataset=['https://huggingface.co/datasets/ibm-research/AttaQ\\n'], hasTasks=['Question Answering'], hasImplementation=['https://github.com/IBM/unitxt/blob/main/prepare/cards/attaq.py'], hasUnitxtCard=[], hasLicense=None, hasRelatedRisk=['Toxic Output', ' Harmful Output'], bestValue=None, hasBenchmarkMetadata=None),\n",
       " AiEval(id='ai_eval_ProvoQ', name='ProvoQ', description=None, url='', dateCreated=None, dateModified=None, hasDocumentation=['https://arxiv.org/abs/2311.04124'], hasDataset=['https://huggingface.co/datasets/ibm-research/ProvoQ\\n'], hasTasks=['Question Answering'], hasImplementation=['https://github.com/IBM/unitxt/blob/main/prepare/cards/safety/provoq.py'], hasUnitxtCard=[], hasLicense=None, hasRelatedRisk=['Output BIas'], bestValue=None, hasBenchmarkMetadata=None),\n",
       " AiEval(id='ai_eval_CrowS-Pairs', name='CrowS-Pairs', description=None, url='https://github.com/nyu-mll/crows-pairs\\n', dateCreated=None, dateModified=None, hasDocumentation=['https://arxiv.org/abs/2010.00133'], hasDataset=['https://huggingface.co/datasets/nyu-mll/crows_pairs\\n'], hasTasks=['Multiple Choice?'], hasImplementation=['https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/crows_pairs/crows_pairs_english.yaml'], hasUnitxtCard=[], hasLicense=None, hasRelatedRisk=['Output BIas'], bestValue=None, hasBenchmarkMetadata=None),\n",
       " AiEval(id='ai_eval_Alert', name='Alert', description=None, url='https://github.com/Babelscape/ALERT\\n', dateCreated=None, dateModified=None, hasDocumentation=['https://arxiv.org/abs/2404.08676'], hasDataset=['https://huggingface.co/datasets/Babelscape/ALERT\\n'], hasTasks=['Question Answering'], hasImplementation=['https://github.com/Babelscape/ALERT/blob/master/src/evaluation.py'], hasUnitxtCard=[], hasLicense=None, hasRelatedRisk=['Toxic Output', ' Output BIas', ' Harmful Output'], bestValue=None, hasBenchmarkMetadata=None),\n",
       " AiEval(id='ai_eval_SALAD_Bench', name='SALAD_Bench', description=None, url='https://github.com/OpenSafetyLab/SALAD-BENCH ', dateCreated=None, dateModified=None, hasDocumentation=['https://arxiv.org/abs/2402.05044'], hasDataset=['https://huggingface.co/datasets/OpenSafetyLab/Salad-Data '], hasTasks=['Question Answering Multiple Choice'], hasImplementation=['https://github.com/OpenSafetyLab/SALAD-BENCH/blob/main/examples/example.py'], hasUnitxtCard=[], hasLicense=None, hasRelatedRisk=['Toxic Output', ' Harmful Output', ' Output BIas'], bestValue=None, hasBenchmarkMetadata=None),\n",
       " AiEval(id='ai_eval_SorryBench', name='SorryBench', description=None, url='https://github.com/sorry-bench/sorry-bench ', dateCreated=None, dateModified=None, hasDocumentation=['https://arxiv.org/abs/2406.14598'], hasDataset=['https://huggingface.co/datasets/sorry-bench/sorry-bench-202406 '], hasTasks=['Question Answering'], hasImplementation=['https://github.com/sorry-bench/sorry-bench'], hasUnitxtCard=[], hasLicense=None, hasRelatedRisk=[], bestValue=None, hasBenchmarkMetadata=None),\n",
       " AiEval(id='ai_eval_ToxiGen', name='ToxiGen', description=None, url='https://github.com/microsoft/TOXIGEN\\n', dateCreated=None, dateModified=None, hasDocumentation=['https://arxiv.org/abs/2203.09509'], hasDataset=['https://huggingface.co/datasets/toxigen/toxigen-data\\n'], hasTasks=['Classification'], hasImplementation=['https://github.com/IBM/unitxt/blob/main/prepare/cards/toxigen.py'], hasUnitxtCard=[], hasLicense=None, hasRelatedRisk=['Toxic Output'], bestValue=None, hasBenchmarkMetadata=None),\n",
       " AiEval(id='ai_eval_XSTest', name='XSTest', description=None, url='https://github.com/paul-rottger/xstest\\n', dateCreated=None, dateModified=None, hasDocumentation=['https://arxiv.org/abs/2308.01263'], hasDataset=['https://huggingface.co/datasets/Paul/XSTest\\n'], hasTasks=[], hasImplementation=[], hasUnitxtCard=[], hasLicense=None, hasRelatedRisk=[], bestValue=None, hasBenchmarkMetadata=None),\n",
       " AiEval(id='ai_eval_StrongReject', name='StrongReject', description=None, url='https://github.com/alexandrasouly/strongreject', dateCreated=None, dateModified=None, hasDocumentation=[''], hasDataset=[''], hasTasks=['Question Answering'], hasImplementation=['https://github.com/alexandrasouly/strongreject/tree/main/strongreject'], hasUnitxtCard=[], hasLicense=None, hasRelatedRisk=['Toxic Output', ' Harmful Output'], bestValue=None, hasBenchmarkMetadata=None),\n",
       " AiEval(id='ai_eval_SimpleSafetyTests', name='SimpleSafetyTests', description=None, url='', dateCreated=None, dateModified=None, hasDocumentation=['https://arxiv.org/abs/2311.08370'], hasDataset=['https://huggingface.co/datasets/Bertievidgen/SimpleSafetyTests\\n'], hasTasks=['Question Answering'], hasImplementation=['https://github.com/IBM/unitxt/blob/main/prepare/cards/safety/simple_safety_tests.py'], hasUnitxtCard=[], hasLicense=None, hasRelatedRisk=['Harmful Output'], bestValue=None, hasBenchmarkMetadata=None),\n",
       " AiEval(id='ai_eval_BBQ', name='BBQ', description=None, url='https://github.com/nyu-mll/BBQ\\n', dateCreated=None, dateModified=None, hasDocumentation=['https://arxiv.org/abs/2110.08193'], hasDataset=['https://huggingface.co/datasets/heegyu/bbq\\n'], hasTasks=['Multiple Choice'], hasImplementation=['https://github.com/IBM/unitxt/blob/main/prepare/cards/safety/bbq.py'], hasUnitxtCard=[], hasLicense=None, hasRelatedRisk=['Output BIas'], bestValue=None, hasBenchmarkMetadata=None),\n",
       " AiEval(id='ai_eval_Discrim_eval', name='Discrim_eval', description=None, url='', dateCreated=None, dateModified=None, hasDocumentation=['https://arxiv.org/abs/2312.03689'], hasDataset=['https://huggingface.co/datasets/Anthropic/discrim-eval\\n'], hasTasks=['Multiple Choice'], hasImplementation=['https://github.com/IBM/unitxt/blob/main/prepare/cards/safety/discrim_eval.py'], hasUnitxtCard=[], hasLicense=None, hasRelatedRisk=[], bestValue=None, hasBenchmarkMetadata=None),\n",
       " AiEval(id='ai_eval_XSafety', name='XSafety', description=None, url='https://github.com/jarviswang94/multilingual_safety_benchmark\\npaper: https://arxiv.org/abs/2310.00905', dateCreated=None, dateModified=None, hasDocumentation=['https://arxiv.org/abs/2310.00905'], hasDataset=[''], hasTasks=[], hasImplementation=['https://github.ibm.com/IBM-Research-AI/fm-eval/blob/main/prepare/cards/safety/multilingual.py'], hasUnitxtCard=[], hasLicense=None, hasRelatedRisk=[], bestValue=None, hasBenchmarkMetadata=None),\n",
       " AiEval(id='ai_eval_AILuminate', name='AILuminate', description=None, url='https://github.com/mlcommons/ailuminate\\n', dateCreated=None, dateModified=None, hasDocumentation=[''], hasDataset=['https://github.com/mlcommons/ailuminate/blob/main/airr_official_1.0_practice_prompt_set_release_public_subset.csv'], hasTasks=['Question Answering'], hasImplementation=['https://github.com/mlcommons/modelbench/tree/main'], hasUnitxtCard=[], hasLicense=None, hasRelatedRisk=['Toxic Output', ' Harmful Output'], bestValue=None, hasBenchmarkMetadata=None),\n",
       " AiEval(id='ai_eval_AIrbench_2024', name='AIrbench 2024', description=None, url='https://github.com/stanford-crfm/air-bench-2024\\n', dateCreated=None, dateModified=None, hasDocumentation=[''], hasDataset=['https://huggingface.co/datasets/stanford-crfm/air-bench-2024'], hasTasks=['Question Answering'], hasImplementation=['https://github.com/stanford-crfm/air-bench-2024/tree/main/evaluation'], hasUnitxtCard=[], hasLicense=None, hasRelatedRisk=[], bestValue=None, hasBenchmarkMetadata=None),\n",
       " AiEval(id='ai_eval_CTI-Bench', name='CTI-Bench', description=None, url='https://github.com/xashru/cti-bench\\n', dateCreated=None, dateModified=None, hasDocumentation=['https://arxiv.org/abs/2406.07599'], hasDataset=['https://huggingface.co/datasets/AI4Sec/cti-bench\\n'], hasTasks=['Multiple Choice\\nQuestion Answering'], hasImplementation=['https://github.com/xashru/cti-bench/tree/main/evaluatio'], hasUnitxtCard=[], hasLicense=None, hasRelatedRisk=[], bestValue=None, hasBenchmarkMetadata=None),\n",
       " AiEval(id='ai_eval_Prompt_Injection', name='Prompt Injection', description=None, url='https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks\\n', dateCreated=None, dateModified=None, hasDocumentation=['https://arxiv.org/abs/2408.01605'], hasDataset=['https://github.com/meta-llama/PurpleLlama/blob/main/CybersecurityBenchmarks/datasets/prompt_injection/prompt_injection.json\\n'], hasTasks=[], hasImplementation=[], hasUnitxtCard=[], hasLicense=None, hasRelatedRisk=[], bestValue=None, hasBenchmarkMetadata=None),\n",
       " AiEval(id='ai_eval_dataset:_https://huggingface.co/datasets/cais/wmdp', name='dataset: https://huggingface.co/datasets/cais/wmdp', description=None, url=None, dateCreated=None, dateModified=None, hasDocumentation=[], hasDataset=[], hasTasks=[], hasImplementation=[], hasUnitxtCard=[], hasLicense=None, hasRelatedRisk=[], bestValue=None, hasBenchmarkMetadata=None),\n",
       " AiEval(id='ai_eval_paper:_https://arxiv.org/abs/2403.03218\"', name='paper: https://arxiv.org/abs/2403.03218\"', description=None, url='', dateCreated=None, dateModified=None, hasDocumentation=[''], hasDataset=[''], hasTasks=['Value aggregation'], hasImplementation=[], hasUnitxtCard=[], hasLicense=None, hasRelatedRisk=[], bestValue=None, hasBenchmarkMetadata=None),\n",
       " AiEval(id='ai_eval_FRR', name='FRR', description=None, url='https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks\\n', dateCreated=None, dateModified=None, hasDocumentation=['https://arxiv.org/abs/2404.13161'], hasDataset=['https://github.com/meta-llama/PurpleLlama/blob/main/CybersecurityBenchmarks/datasets/frr/frr.json\\n'], hasTasks=['Generation'], hasImplementation=['https://github.com/meta-llama/PurpleLlama/blob/main/CybersecurityBenchmarks/benchmark/frr_benchmark.py'], hasUnitxtCard=[], hasLicense=None, hasRelatedRisk=[], bestValue=None, hasBenchmarkMetadata=None),\n",
       " AiEval(id='ai_eval_Ethos', name='Ethos', description=None, url='', dateCreated=None, dateModified=None, hasDocumentation=['https://arxiv.org/abs/2006.08328'], hasDataset=['https://huggingface.co/datasets/iamollas/ethos\\n'], hasTasks=[], hasImplementation=[], hasUnitxtCard=[], hasLicense=None, hasRelatedRisk=['Toxic Output'], bestValue=None, hasBenchmarkMetadata=None),\n",
       " AiEval(id='ai_eval_PopQA', name='PopQA', description=None, url='', dateCreated=None, dateModified=None, hasDocumentation=['https://arxiv.org/abs/2212.10511'], hasDataset=['https://huggingface.co/datasets/akariasai/PopQA\\n'], hasTasks=[], hasImplementation=[], hasUnitxtCard=[], hasLicense=None, hasRelatedRisk=['Hallucination'], bestValue=None, hasBenchmarkMetadata=None)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install lxml beautifulsoup4\n",
    "import urllib.request as libreq\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "data_evals_evals = []\n",
    "docs = []\n",
    "for index, row in data_evals.iterrows():\n",
    " \n",
    "    \n",
    "    print([row[\"Task\"]])\n",
    "\n",
    "    if type(row[\"Risk Mapping (IBM) - Minor\"]) != float:\n",
    "        risks  = row[\"Risk Mapping (IBM) - Minor\"].split(\",\")\n",
    "    else:\n",
    "        risks = []\n",
    "    \n",
    "    if type(row[\"Task\"]) != float:\n",
    "        tasks  = row[\"Task\"].split(\",\")\n",
    "    else:\n",
    "        tasks = []\n",
    "\n",
    "    if type(row[\"Implementation\"]) != float:\n",
    "        imps  = row[\"Implementation\"].split(\",\")\n",
    "    else:\n",
    "        imps = []\n",
    "\n",
    "    if type(row[\"Links\"]) != float:\n",
    "        paper = row[\"Links\"].partition(\"paper: \")[2].partition(\",\")[0]\n",
    "        url = row[\"Links\"].partition(\"repo: \")[2].partition(\"dataset\")[0]\n",
    "        dataset = row[\"Links\"].partition(\"dataset: \")[2].partition(\"paper\")[0]\n",
    "    else:\n",
    "        paper = None\n",
    "        url = None\n",
    "        dataset = None\n",
    "\n",
    "    a= AiEval(\n",
    "        id=\"ai_eval_\"+row[\"Name\"].replace(\" \", \"_\"), \n",
    "        name=row[\"Name\"], \n",
    "        url=url,\n",
    "        hasDocumentation=[paper] if paper != None else [],\n",
    "        hasDataset= [dataset] if dataset != None else [],\n",
    "        hasRelatedRisk=risks,\n",
    "        hasTasks=tasks,\n",
    "        hasImplementation=imps,\n",
    "        hasUnitxtCard=[],\n",
    " \n",
    "    )\n",
    "\n",
    "    data_evals_evals.append(a)\n",
    "\n",
    "    \n",
    "    if paper != None and 'arxiv' in paper:\n",
    "        with libreq.urlopen('http://export.arxiv.org/api/query?id_list='+ paper.partition(\"/abs/\")[2]) as url:\n",
    "            r = url.read()\n",
    "            soup = BeautifulSoup(r, features=\"xml\")\n",
    "            entries = soup.find_all('entry')\n",
    "            for entry in entries:\n",
    "                name = entry.find(\"title\").text\n",
    "                description = entry.find(\"summary\").text\n",
    "        print(r)\n",
    "\n",
    "\n",
    "\n",
    "    if paper != None:\n",
    "        b = Documentation(\n",
    "            id=paper,\n",
    "            name=name if name != None else paper,\n",
    "            description=description,\n",
    "            url=paper\n",
    "        )\n",
    "\n",
    "        docs.append(b)\n",
    "\n",
    "data_evals_evals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7f1eee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create container\n",
    "from risk_atlas_nexus.ai_risk_ontology.datamodel.ai_risk_ontology import Container\n",
    "from linkml_runtime.dumpers import YAMLDumper\n",
    "\n",
    "container = Container(\n",
    "    documents=docs,\n",
    "    evaluations=data_evals_evals\n",
    ")\n",
    "\n",
    "\n",
    "with open(\n",
    "        \"/Users/ingevejs/Documents/workspace/ingelise/risk-atlas-nexus/src/risk_atlas_nexus/data/knowledge_graph_prep/benchmark_metadata_cards_csv_list.yaml\",\n",
    "        \"+tw\",\n",
    "        encoding=\"utf-8\",\n",
    "    ) as output_file:\n",
    "        print(YAMLDumper().dumps(container), file=output_file)\n",
    "        output_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4789af09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-19 22:10:30:364] - INFO - RiskAtlasNexus - Created RiskAtlasNexus instance. Base_dir: /Users/ingevejs/Documents/workspace/ingelise/risk-atlas-nexus/src/risk_atlas_nexus/data/knowledge_graph_prep\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>dateCreated</th>\n",
       "      <th>dateModified</th>\n",
       "      <th>hasDocumentation</th>\n",
       "      <th>hasDataset</th>\n",
       "      <th>hasTasks</th>\n",
       "      <th>hasImplementation</th>\n",
       "      <th>hasUnitxtCard</th>\n",
       "      <th>hasLicense</th>\n",
       "      <th>hasRelatedRisk</th>\n",
       "      <th>bestValue</th>\n",
       "      <th>hasBenchmarkMetadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stanford-fmti</td>\n",
       "      <td>The Foundation Model Transparency Index</td>\n",
       "      <td>The Foundation Model Transparency Index is an ...</td>\n",
       "      <td>https://crfm.stanford.edu/fmti/</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[arxiv.org/2310.12941]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[atlas-lack-of-model-transparency, atlas-data-...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cards.value_alignment.hallucinations.truthfulqa</td>\n",
       "      <td>TruthfulQA</td>\n",
       "      <td>TruthfulQA is a benchmark to measure whether a...</td>\n",
       "      <td>https://github.com/sylinrl/TruthfulQA</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[arxiv.org/2109.07958]</td>\n",
       "      <td>[truthfulqa/truthful_qa]</td>\n",
       "      <td>[multiple-choice, text-generation, question-an...</td>\n",
       "      <td>None</td>\n",
       "      <td>[cards.value_alignment.hallucinations.truthfulqa]</td>\n",
       "      <td>None</td>\n",
       "      <td>[atlas-hallucination]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ai_eval_BOLD</td>\n",
       "      <td>BOLD: Bias in Open-ended Language Generation D...</td>\n",
       "      <td>Bias in Open-ended Language Generation Dataset...</td>\n",
       "      <td>https://github.com/amazon-research/bold</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://arxiv.org/abs/2101.11718]</td>\n",
       "      <td>[AlexaAI/bold]</td>\n",
       "      <td>[text-generation]</td>\n",
       "      <td>[https://github.com/IBM/unitxt/blob/main/prepa...</td>\n",
       "      <td>[https://github.com/IBM/unitxt/blob/main/prepa...</td>\n",
       "      <td>license-cc-by-4.0</td>\n",
       "      <td>[atlas-output-bias]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ai_eval_AttaQ</td>\n",
       "      <td>AttaQ</td>\n",
       "      <td>The AttaQ dataset is a benchmark for evaluatin...</td>\n",
       "      <td>https://huggingface.co/datasets/ibm-research/A...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://arxiv.org/abs/2311.04124]</td>\n",
       "      <td>[ibm-research/AttaQ]</td>\n",
       "      <td>[text-generation, text2text-generation]</td>\n",
       "      <td>[https://github.com/IBM/unitxt/blob/main/prepa...</td>\n",
       "      <td>[https://github.com/IBM/unitxt/blob/main/prepa...</td>\n",
       "      <td>license-mit</td>\n",
       "      <td>[atlas-toxic-output, atlas-harmful-output]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ai_eval_ProvoQ</td>\n",
       "      <td>ProvoQ (PROVOcative Questions about minority-a...</td>\n",
       "      <td>The ProvoQ dataset is designed to evaluate the...</td>\n",
       "      <td>https://huggingface.co/datasets/ibm-research/P...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://arxiv.org/abs/2311.04124]</td>\n",
       "      <td>[ibm-research/ProvoQ]</td>\n",
       "      <td>[question-answering]</td>\n",
       "      <td>[https://github.com/IBM/unitxt/blob/main/prepa...</td>\n",
       "      <td>[https://github.com/IBM/unitxt/blob/main/prepa...</td>\n",
       "      <td>license-cdla-permissive-2.0</td>\n",
       "      <td>[atlas-output-bias]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ai_eval_CrowS-Pairs</td>\n",
       "      <td>Crowdsourced Stereotype Pairs benchmark (CrowS...</td>\n",
       "      <td>This benchmark measures some forms of social b...</td>\n",
       "      <td>https://github.com/nyu-mll/crows-pairs</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://arxiv.org/abs/2010.00133]</td>\n",
       "      <td>[nyu-mll/crows_pairs]</td>\n",
       "      <td>[text-classification, text-scoring]</td>\n",
       "      <td>[https://github.com/EleutherAI/lm-evaluation-h...</td>\n",
       "      <td>None</td>\n",
       "      <td>license-cc-by-sa-4.0</td>\n",
       "      <td>[atlas-output-bias]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ai_eval_Alert</td>\n",
       "      <td>ALERT</td>\n",
       "      <td>A large-scale benchmark to assess the safety o...</td>\n",
       "      <td>https://github.com/Babelscape/ALERT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://arxiv.org/abs/2404.08676]</td>\n",
       "      <td>[Babelscape/ALERT]</td>\n",
       "      <td>[question-answering]</td>\n",
       "      <td>[https://github.com/Babelscape/ALERT/blob/mast...</td>\n",
       "      <td>None</td>\n",
       "      <td>license-cc-by-nc-sa-4.0</td>\n",
       "      <td>[atlas-toxic-output, atlas-toxic-output, atlas...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ai_eval_SALAD_Bench</td>\n",
       "      <td>SALAD-Bench</td>\n",
       "      <td>A challenging safety benchmark specifically de...</td>\n",
       "      <td>https://github.com/OpenSafetyLab/SALAD-BENCH</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://arxiv.org/abs/2402.05044]</td>\n",
       "      <td>[OpenSafetyLab/Salad-Data]</td>\n",
       "      <td>[text-classification, text-generation]</td>\n",
       "      <td>[https://github.com/OpenSafetyLab/SALAD-BENCH/...</td>\n",
       "      <td>None</td>\n",
       "      <td>license-apache-2.0</td>\n",
       "      <td>[atlas-toxic-output, atlas-harmful-output]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ai_eval_SorryBench</td>\n",
       "      <td>SorryBench</td>\n",
       "      <td>This is a benchmark for LLM safety refusal beh...</td>\n",
       "      <td>https://github.com/sorry-bench/sorry-bench</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://arxiv.org/abs/2406.14598]</td>\n",
       "      <td>[sorry-bench/sorry-bench-202406]</td>\n",
       "      <td>[question-answering]</td>\n",
       "      <td>[https://github.com/sorry-bench/sorry-bench]</td>\n",
       "      <td>None</td>\n",
       "      <td>license-mit</td>\n",
       "      <td>[atlas-harmful-code-generation]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ai_eval_ToxiGen</td>\n",
       "      <td>ToxiGen</td>\n",
       "      <td>This dataset is for implicit hate speech detec...</td>\n",
       "      <td>https://github.com/microsoft/TOXIGEN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://arxiv.org/abs/2203.09509]</td>\n",
       "      <td>[toxigen/toxigen-data]</td>\n",
       "      <td>[text-classification, hate-speech-detection]</td>\n",
       "      <td>[https://github.com/IBM/unitxt/blob/main/prepa...</td>\n",
       "      <td>[https://github.com/IBM/unitxt/blob/main/prepa...</td>\n",
       "      <td>license-mit</td>\n",
       "      <td>[atlas-toxic-output]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ai_eval_XSTest</td>\n",
       "      <td>XSTest: A Test Suite for Identifying Exaggerat...</td>\n",
       "      <td>XSTest is a test suite designed to identify ex...</td>\n",
       "      <td>https://github.com/paul-rottger/xstest</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://arxiv.org/abs/2308.01263]</td>\n",
       "      <td>[https://huggingface.co/datasets/Paul/XSTest]</td>\n",
       "      <td>[text2text-generation]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-cc-by-4.0</td>\n",
       "      <td>[atlas-harmful-output]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ai_eval_StrongReject</td>\n",
       "      <td>StrongReject</td>\n",
       "      <td>StrongREJECT is a state-of-the-art LLM jailbre...</td>\n",
       "      <td>https://github.com/dsbowen/strong_reject</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://arxiv.org/abs/2402.10260]</td>\n",
       "      <td>[Paul/XSTest]</td>\n",
       "      <td>[question-answering]</td>\n",
       "      <td>[https://github.com/dsbowen/strong_reject/tree...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[atlas-toxic-output, atlas-harmful-output]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ai_eval_SimpleSafetyTests</td>\n",
       "      <td>SimpleSafetyTests</td>\n",
       "      <td>SimpleSafetyTests assists in identifying criti...</td>\n",
       "      <td>https://huggingface.co/datasets/Bertievidgen/S...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://arxiv.org/abs/2311.08370]</td>\n",
       "      <td>[Bertievidgen/SimpleSafetyTests]</td>\n",
       "      <td>[question-answering]</td>\n",
       "      <td>[https://github.com/IBM/unitxt/blob/main/prepa...</td>\n",
       "      <td>[https://github.com/IBM/unitxt/blob/main/prepa...</td>\n",
       "      <td>None</td>\n",
       "      <td>[atlas-harmful-output]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ai_eval_BBQ</td>\n",
       "      <td>BBQ</td>\n",
       "      <td>Bias Benchmark for QA (BBQ), a dataset of ques...</td>\n",
       "      <td>https://github.com/nyu-mll/BBQ</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://arxiv.org/abs/2110.08193]</td>\n",
       "      <td>[heegyu/bbq]</td>\n",
       "      <td>[Multiple Choice]</td>\n",
       "      <td>[https://github.com/IBM/unitxt/blob/main/prepa...</td>\n",
       "      <td>[https://github.com/IBM/unitxt/blob/main/prepa...</td>\n",
       "      <td>None</td>\n",
       "      <td>[atlas-output-bias]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ai_eval_Discrim_eval</td>\n",
       "      <td>Discrim_eval</td>\n",
       "      <td>The data contains a diverse set of prompts cov...</td>\n",
       "      <td>https://huggingface.co/datasets/Anthropic/disc...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://arxiv.org/abs/2312.03689]</td>\n",
       "      <td>[Anthropic/discrim-eval]</td>\n",
       "      <td>[Multiple Choice]</td>\n",
       "      <td>[https://github.com/IBM/unitxt/blob/main/prepa...</td>\n",
       "      <td>[https://github.com/IBM/unitxt/blob/main/prepa...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ai_eval_XSafety</td>\n",
       "      <td>XSafety</td>\n",
       "      <td>A multilingual safety benchmark for LLMs, in r...</td>\n",
       "      <td>https://github.com/jarviswang94/multilingual_s...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://arxiv.org/abs/2310.00905]</td>\n",
       "      <td>[Jarviswang94_Multilingual_safety_benchmark]</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://github.ibm.com/IBM-Research-AI/fm-eva...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ai_eval_AILuminate</td>\n",
       "      <td>AILuminate</td>\n",
       "      <td>The MLCommons AILuminate v1.0 benchmark provid...</td>\n",
       "      <td>https://github.com/mlcommons/ailuminate</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://arxiv.org/abs/2404.12241]</td>\n",
       "      <td>[mlcommons_ailuminate_airr_official_1.0_demo_e...</td>\n",
       "      <td>[question-answering]</td>\n",
       "      <td>[https://github.com/mlcommons/modelbench/tree/...</td>\n",
       "      <td>None</td>\n",
       "      <td>license-cc-by-4.0</td>\n",
       "      <td>[atlas-toxic-output, atlas-harmful-output]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ai_eval_Airbench_2024</td>\n",
       "      <td>Airbench 2024</td>\n",
       "      <td>AIR-Bench is a regulation-aligned safety bench...</td>\n",
       "      <td>https://github.com/stanford-crfm/air-bench-2024</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://arxiv.org/abs/2407.17436]</td>\n",
       "      <td>[stanford-crfm/air-bench-2024]</td>\n",
       "      <td>[question-answering]</td>\n",
       "      <td>[https://github.com/stanford-crfm/air-bench-20...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ai_eval_CTI-Bench</td>\n",
       "      <td>CTI-Bench</td>\n",
       "      <td>CTIBench is a comprehensive suite of benchmark...</td>\n",
       "      <td>https://github.com/xashru/cti-bench</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://arxiv.org/abs/2406.07599]</td>\n",
       "      <td>[AI4Sec/cti-bench]</td>\n",
       "      <td>[Multiple Choice, Question Answering]</td>\n",
       "      <td>[https://github.com/xashru/cti-bench/tree/main...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ai_eval_Prompt_Injection</td>\n",
       "      <td>Prompt Injection</td>\n",
       "      <td>A benchmark to assess an LLM’s susceptibility ...</td>\n",
       "      <td>https://github.com/meta-llama/PurpleLlama/tree...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://arxiv.org/abs/2408.01605]</td>\n",
       "      <td>[CybersecurityBenchmarks_datasets_prompt_injec...</td>\n",
       "      <td>[Question Answering]</td>\n",
       "      <td>[https://github.com/meta-llama/PurpleLlama/blo...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[atlas-prompt-injection, atlas-jailbreaking]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ai_eval_WMDP</td>\n",
       "      <td>Weapons of Mass Destruction Proxy (WMDP)</td>\n",
       "      <td>The Weapons of Mass Destruction Proxy (WMDP) b...</td>\n",
       "      <td>https://github.com/centerforaisafety/wmdp</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://arxiv.org/abs/2403.03218]</td>\n",
       "      <td>[cais/wmdp]</td>\n",
       "      <td>[Multiple choice]</td>\n",
       "      <td>[https://github.com/EleutherAI/lm-evaluation-h...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[atlas-harmful-output]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ai_eval_FRR</td>\n",
       "      <td>False Refusal Rate (FRR)</td>\n",
       "      <td>These tests measure how often an LLM incorrect...</td>\n",
       "      <td>https://github.com/meta-llama/PurpleLlama/tree...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://arxiv.org/abs/2404.13161]</td>\n",
       "      <td>[CybersecurityBenchmarks_datasets_frr]</td>\n",
       "      <td>[text-generation]</td>\n",
       "      <td>[https://github.com/meta-llama/PurpleLlama/blo...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[atlas-harmful-code-generation]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ai_eval_Ethos</td>\n",
       "      <td>Ethos</td>\n",
       "      <td>ETHOS is an onlinE haTe speecH detectiOn dataS...</td>\n",
       "      <td>https://huggingface.co/datasets/iamollas/ethos</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://arxiv.org/abs/2006.08328]</td>\n",
       "      <td>[iamollas/ethos]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[atlas-toxic-output]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ai_eval_PopQA</td>\n",
       "      <td>PopQA</td>\n",
       "      <td>PopQA is a large-scale open-domain question an...</td>\n",
       "      <td>https://huggingface.co/datasets/akariasai/PopQA</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://arxiv.org/abs/2212.10511]</td>\n",
       "      <td>[akariasai/PopQA]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[atlas-hallucination]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 id  \\\n",
       "0                                     stanford-fmti   \n",
       "1   cards.value_alignment.hallucinations.truthfulqa   \n",
       "2                                      ai_eval_BOLD   \n",
       "3                                     ai_eval_AttaQ   \n",
       "4                                    ai_eval_ProvoQ   \n",
       "5                               ai_eval_CrowS-Pairs   \n",
       "6                                     ai_eval_Alert   \n",
       "7                               ai_eval_SALAD_Bench   \n",
       "8                                ai_eval_SorryBench   \n",
       "9                                   ai_eval_ToxiGen   \n",
       "10                                   ai_eval_XSTest   \n",
       "11                             ai_eval_StrongReject   \n",
       "12                        ai_eval_SimpleSafetyTests   \n",
       "13                                      ai_eval_BBQ   \n",
       "14                             ai_eval_Discrim_eval   \n",
       "15                                  ai_eval_XSafety   \n",
       "16                               ai_eval_AILuminate   \n",
       "17                            ai_eval_Airbench_2024   \n",
       "18                                ai_eval_CTI-Bench   \n",
       "19                         ai_eval_Prompt_Injection   \n",
       "20                                     ai_eval_WMDP   \n",
       "21                                      ai_eval_FRR   \n",
       "22                                    ai_eval_Ethos   \n",
       "23                                    ai_eval_PopQA   \n",
       "\n",
       "                                                 name  \\\n",
       "0             The Foundation Model Transparency Index   \n",
       "1                                          TruthfulQA   \n",
       "2   BOLD: Bias in Open-ended Language Generation D...   \n",
       "3                                               AttaQ   \n",
       "4   ProvoQ (PROVOcative Questions about minority-a...   \n",
       "5   Crowdsourced Stereotype Pairs benchmark (CrowS...   \n",
       "6                                               ALERT   \n",
       "7                                         SALAD-Bench   \n",
       "8                                          SorryBench   \n",
       "9                                             ToxiGen   \n",
       "10  XSTest: A Test Suite for Identifying Exaggerat...   \n",
       "11                                       StrongReject   \n",
       "12                                  SimpleSafetyTests   \n",
       "13                                                BBQ   \n",
       "14                                       Discrim_eval   \n",
       "15                                            XSafety   \n",
       "16                                         AILuminate   \n",
       "17                                      Airbench 2024   \n",
       "18                                          CTI-Bench   \n",
       "19                                   Prompt Injection   \n",
       "20           Weapons of Mass Destruction Proxy (WMDP)   \n",
       "21                           False Refusal Rate (FRR)   \n",
       "22                                              Ethos   \n",
       "23                                              PopQA   \n",
       "\n",
       "                                          description  \\\n",
       "0   The Foundation Model Transparency Index is an ...   \n",
       "1   TruthfulQA is a benchmark to measure whether a...   \n",
       "2   Bias in Open-ended Language Generation Dataset...   \n",
       "3   The AttaQ dataset is a benchmark for evaluatin...   \n",
       "4   The ProvoQ dataset is designed to evaluate the...   \n",
       "5   This benchmark measures some forms of social b...   \n",
       "6   A large-scale benchmark to assess the safety o...   \n",
       "7   A challenging safety benchmark specifically de...   \n",
       "8   This is a benchmark for LLM safety refusal beh...   \n",
       "9   This dataset is for implicit hate speech detec...   \n",
       "10  XSTest is a test suite designed to identify ex...   \n",
       "11  StrongREJECT is a state-of-the-art LLM jailbre...   \n",
       "12  SimpleSafetyTests assists in identifying criti...   \n",
       "13  Bias Benchmark for QA (BBQ), a dataset of ques...   \n",
       "14  The data contains a diverse set of prompts cov...   \n",
       "15  A multilingual safety benchmark for LLMs, in r...   \n",
       "16  The MLCommons AILuminate v1.0 benchmark provid...   \n",
       "17  AIR-Bench is a regulation-aligned safety bench...   \n",
       "18  CTIBench is a comprehensive suite of benchmark...   \n",
       "19  A benchmark to assess an LLM’s susceptibility ...   \n",
       "20  The Weapons of Mass Destruction Proxy (WMDP) b...   \n",
       "21  These tests measure how often an LLM incorrect...   \n",
       "22  ETHOS is an onlinE haTe speecH detectiOn dataS...   \n",
       "23  PopQA is a large-scale open-domain question an...   \n",
       "\n",
       "                                                  url dateCreated  \\\n",
       "0                     https://crfm.stanford.edu/fmti/        None   \n",
       "1               https://github.com/sylinrl/TruthfulQA        None   \n",
       "2             https://github.com/amazon-research/bold        None   \n",
       "3   https://huggingface.co/datasets/ibm-research/A...        None   \n",
       "4   https://huggingface.co/datasets/ibm-research/P...        None   \n",
       "5              https://github.com/nyu-mll/crows-pairs        None   \n",
       "6                 https://github.com/Babelscape/ALERT        None   \n",
       "7        https://github.com/OpenSafetyLab/SALAD-BENCH        None   \n",
       "8          https://github.com/sorry-bench/sorry-bench        None   \n",
       "9                https://github.com/microsoft/TOXIGEN        None   \n",
       "10             https://github.com/paul-rottger/xstest        None   \n",
       "11           https://github.com/dsbowen/strong_reject        None   \n",
       "12  https://huggingface.co/datasets/Bertievidgen/S...        None   \n",
       "13                     https://github.com/nyu-mll/BBQ        None   \n",
       "14  https://huggingface.co/datasets/Anthropic/disc...        None   \n",
       "15  https://github.com/jarviswang94/multilingual_s...        None   \n",
       "16            https://github.com/mlcommons/ailuminate        None   \n",
       "17    https://github.com/stanford-crfm/air-bench-2024        None   \n",
       "18                https://github.com/xashru/cti-bench        None   \n",
       "19  https://github.com/meta-llama/PurpleLlama/tree...        None   \n",
       "20          https://github.com/centerforaisafety/wmdp        None   \n",
       "21  https://github.com/meta-llama/PurpleLlama/tree...        None   \n",
       "22     https://huggingface.co/datasets/iamollas/ethos        None   \n",
       "23    https://huggingface.co/datasets/akariasai/PopQA        None   \n",
       "\n",
       "   dateModified                    hasDocumentation  \\\n",
       "0          None              [arxiv.org/2310.12941]   \n",
       "1          None              [arxiv.org/2109.07958]   \n",
       "2          None  [https://arxiv.org/abs/2101.11718]   \n",
       "3          None  [https://arxiv.org/abs/2311.04124]   \n",
       "4          None  [https://arxiv.org/abs/2311.04124]   \n",
       "5          None  [https://arxiv.org/abs/2010.00133]   \n",
       "6          None  [https://arxiv.org/abs/2404.08676]   \n",
       "7          None  [https://arxiv.org/abs/2402.05044]   \n",
       "8          None  [https://arxiv.org/abs/2406.14598]   \n",
       "9          None  [https://arxiv.org/abs/2203.09509]   \n",
       "10         None  [https://arxiv.org/abs/2308.01263]   \n",
       "11         None  [https://arxiv.org/abs/2402.10260]   \n",
       "12         None  [https://arxiv.org/abs/2311.08370]   \n",
       "13         None  [https://arxiv.org/abs/2110.08193]   \n",
       "14         None  [https://arxiv.org/abs/2312.03689]   \n",
       "15         None  [https://arxiv.org/abs/2310.00905]   \n",
       "16         None  [https://arxiv.org/abs/2404.12241]   \n",
       "17         None  [https://arxiv.org/abs/2407.17436]   \n",
       "18         None  [https://arxiv.org/abs/2406.07599]   \n",
       "19         None  [https://arxiv.org/abs/2408.01605]   \n",
       "20         None  [https://arxiv.org/abs/2403.03218]   \n",
       "21         None  [https://arxiv.org/abs/2404.13161]   \n",
       "22         None  [https://arxiv.org/abs/2006.08328]   \n",
       "23         None  [https://arxiv.org/abs/2212.10511]   \n",
       "\n",
       "                                           hasDataset  \\\n",
       "0                                                None   \n",
       "1                            [truthfulqa/truthful_qa]   \n",
       "2                                      [AlexaAI/bold]   \n",
       "3                                [ibm-research/AttaQ]   \n",
       "4                               [ibm-research/ProvoQ]   \n",
       "5                               [nyu-mll/crows_pairs]   \n",
       "6                                  [Babelscape/ALERT]   \n",
       "7                          [OpenSafetyLab/Salad-Data]   \n",
       "8                    [sorry-bench/sorry-bench-202406]   \n",
       "9                              [toxigen/toxigen-data]   \n",
       "10      [https://huggingface.co/datasets/Paul/XSTest]   \n",
       "11                                      [Paul/XSTest]   \n",
       "12                   [Bertievidgen/SimpleSafetyTests]   \n",
       "13                                       [heegyu/bbq]   \n",
       "14                           [Anthropic/discrim-eval]   \n",
       "15       [Jarviswang94_Multilingual_safety_benchmark]   \n",
       "16  [mlcommons_ailuminate_airr_official_1.0_demo_e...   \n",
       "17                     [stanford-crfm/air-bench-2024]   \n",
       "18                                 [AI4Sec/cti-bench]   \n",
       "19  [CybersecurityBenchmarks_datasets_prompt_injec...   \n",
       "20                                        [cais/wmdp]   \n",
       "21             [CybersecurityBenchmarks_datasets_frr]   \n",
       "22                                   [iamollas/ethos]   \n",
       "23                                  [akariasai/PopQA]   \n",
       "\n",
       "                                             hasTasks  \\\n",
       "0                                                None   \n",
       "1   [multiple-choice, text-generation, question-an...   \n",
       "2                                   [text-generation]   \n",
       "3             [text-generation, text2text-generation]   \n",
       "4                                [question-answering]   \n",
       "5                 [text-classification, text-scoring]   \n",
       "6                                [question-answering]   \n",
       "7              [text-classification, text-generation]   \n",
       "8                                [question-answering]   \n",
       "9        [text-classification, hate-speech-detection]   \n",
       "10                             [text2text-generation]   \n",
       "11                               [question-answering]   \n",
       "12                               [question-answering]   \n",
       "13                                  [Multiple Choice]   \n",
       "14                                  [Multiple Choice]   \n",
       "15                                               None   \n",
       "16                               [question-answering]   \n",
       "17                               [question-answering]   \n",
       "18              [Multiple Choice, Question Answering]   \n",
       "19                               [Question Answering]   \n",
       "20                                  [Multiple choice]   \n",
       "21                                  [text-generation]   \n",
       "22                                               None   \n",
       "23                                               None   \n",
       "\n",
       "                                    hasImplementation  \\\n",
       "0                                                None   \n",
       "1                                                None   \n",
       "2   [https://github.com/IBM/unitxt/blob/main/prepa...   \n",
       "3   [https://github.com/IBM/unitxt/blob/main/prepa...   \n",
       "4   [https://github.com/IBM/unitxt/blob/main/prepa...   \n",
       "5   [https://github.com/EleutherAI/lm-evaluation-h...   \n",
       "6   [https://github.com/Babelscape/ALERT/blob/mast...   \n",
       "7   [https://github.com/OpenSafetyLab/SALAD-BENCH/...   \n",
       "8        [https://github.com/sorry-bench/sorry-bench]   \n",
       "9   [https://github.com/IBM/unitxt/blob/main/prepa...   \n",
       "10                                               None   \n",
       "11  [https://github.com/dsbowen/strong_reject/tree...   \n",
       "12  [https://github.com/IBM/unitxt/blob/main/prepa...   \n",
       "13  [https://github.com/IBM/unitxt/blob/main/prepa...   \n",
       "14  [https://github.com/IBM/unitxt/blob/main/prepa...   \n",
       "15  [https://github.ibm.com/IBM-Research-AI/fm-eva...   \n",
       "16  [https://github.com/mlcommons/modelbench/tree/...   \n",
       "17  [https://github.com/stanford-crfm/air-bench-20...   \n",
       "18  [https://github.com/xashru/cti-bench/tree/main...   \n",
       "19  [https://github.com/meta-llama/PurpleLlama/blo...   \n",
       "20  [https://github.com/EleutherAI/lm-evaluation-h...   \n",
       "21  [https://github.com/meta-llama/PurpleLlama/blo...   \n",
       "22                                               None   \n",
       "23                                               None   \n",
       "\n",
       "                                        hasUnitxtCard  \\\n",
       "0                                                None   \n",
       "1   [cards.value_alignment.hallucinations.truthfulqa]   \n",
       "2   [https://github.com/IBM/unitxt/blob/main/prepa...   \n",
       "3   [https://github.com/IBM/unitxt/blob/main/prepa...   \n",
       "4   [https://github.com/IBM/unitxt/blob/main/prepa...   \n",
       "5                                                None   \n",
       "6                                                None   \n",
       "7                                                None   \n",
       "8                                                None   \n",
       "9   [https://github.com/IBM/unitxt/blob/main/prepa...   \n",
       "10                                               None   \n",
       "11                                               None   \n",
       "12  [https://github.com/IBM/unitxt/blob/main/prepa...   \n",
       "13  [https://github.com/IBM/unitxt/blob/main/prepa...   \n",
       "14  [https://github.com/IBM/unitxt/blob/main/prepa...   \n",
       "15                                               None   \n",
       "16                                               None   \n",
       "17                                               None   \n",
       "18                                               None   \n",
       "19                                               None   \n",
       "20                                               None   \n",
       "21                                               None   \n",
       "22                                               None   \n",
       "23                                               None   \n",
       "\n",
       "                     hasLicense  \\\n",
       "0                          None   \n",
       "1                          None   \n",
       "2             license-cc-by-4.0   \n",
       "3                   license-mit   \n",
       "4   license-cdla-permissive-2.0   \n",
       "5          license-cc-by-sa-4.0   \n",
       "6       license-cc-by-nc-sa-4.0   \n",
       "7            license-apache-2.0   \n",
       "8                   license-mit   \n",
       "9                   license-mit   \n",
       "10            license-cc-by-4.0   \n",
       "11                         None   \n",
       "12                         None   \n",
       "13                         None   \n",
       "14                         None   \n",
       "15                         None   \n",
       "16            license-cc-by-4.0   \n",
       "17                         None   \n",
       "18                         None   \n",
       "19                         None   \n",
       "20                         None   \n",
       "21                         None   \n",
       "22                         None   \n",
       "23                         None   \n",
       "\n",
       "                                       hasRelatedRisk bestValue  \\\n",
       "0   [atlas-lack-of-model-transparency, atlas-data-...      None   \n",
       "1                               [atlas-hallucination]      None   \n",
       "2                                 [atlas-output-bias]      None   \n",
       "3          [atlas-toxic-output, atlas-harmful-output]      None   \n",
       "4                                 [atlas-output-bias]      None   \n",
       "5                                 [atlas-output-bias]      None   \n",
       "6   [atlas-toxic-output, atlas-toxic-output, atlas...      None   \n",
       "7          [atlas-toxic-output, atlas-harmful-output]      None   \n",
       "8                     [atlas-harmful-code-generation]      None   \n",
       "9                                [atlas-toxic-output]      None   \n",
       "10                             [atlas-harmful-output]      None   \n",
       "11         [atlas-toxic-output, atlas-harmful-output]      None   \n",
       "12                             [atlas-harmful-output]      None   \n",
       "13                                [atlas-output-bias]      None   \n",
       "14                                               None      None   \n",
       "15                                               None      None   \n",
       "16         [atlas-toxic-output, atlas-harmful-output]      None   \n",
       "17                                               None      None   \n",
       "18                                               None      None   \n",
       "19       [atlas-prompt-injection, atlas-jailbreaking]      None   \n",
       "20                             [atlas-harmful-output]      None   \n",
       "21                    [atlas-harmful-code-generation]      None   \n",
       "22                               [atlas-toxic-output]      None   \n",
       "23                              [atlas-hallucination]      None   \n",
       "\n",
       "   hasBenchmarkMetadata  \n",
       "0                  None  \n",
       "1                  None  \n",
       "2                  None  \n",
       "3                  None  \n",
       "4                  None  \n",
       "5                  None  \n",
       "6                  None  \n",
       "7                  None  \n",
       "8                  None  \n",
       "9                  None  \n",
       "10                 None  \n",
       "11                 None  \n",
       "12                 None  \n",
       "13                 None  \n",
       "14                 None  \n",
       "15                 None  \n",
       "16                 None  \n",
       "17                 None  \n",
       "18                 None  \n",
       "19                 None  \n",
       "20                 None  \n",
       "21                 None  \n",
       "22                 None  \n",
       "23                 None  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ran2 = RiskAtlasNexus(base_dir='/Users/ingevejs/Documents/workspace/ingelise/risk-atlas-nexus/src/risk_atlas_nexus/data/knowledge_graph_prep') \n",
    "ran2.get_all_evaluations()\n",
    "df_ev = pd.DataFrame([item.__dict__ for item in ran2._ontology.evaluations])\n",
    "df_ev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99a5d1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>dateCreated</th>\n",
       "      <th>dateModified</th>\n",
       "      <th>hasLicense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10a99803d8afd656</td>\n",
       "      <td>Foundation models: Opportunities, risks and mi...</td>\n",
       "      <td>In this document we: Explore the benefits of f...</td>\n",
       "      <td>https://www.ibm.com/downloads/documents/us-en/...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NIST.AI.600-1</td>\n",
       "      <td>Artificial Intelligence Risk Management Framew...</td>\n",
       "      <td>This document is a cross-sectoral profile of a...</td>\n",
       "      <td>https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.6...</td>\n",
       "      <td>2024-07-25</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AILuminate-doc</td>\n",
       "      <td>AILuminate: Introducing v1.0 of the AI Risk an...</td>\n",
       "      <td>The rapid advancement and deployment of AI sys...</td>\n",
       "      <td>https://arxiv.org/pdf/2503.05731</td>\n",
       "      <td>2025-02-19</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arxiv.org/2408.12622</td>\n",
       "      <td>The AI Risk Repository: A Comprehensive Meta-R...</td>\n",
       "      <td>The risks posed by Artificial Intelligence (AI...</td>\n",
       "      <td>https://arxiv.org/abs/2408.12622</td>\n",
       "      <td>2024-08-14</td>\n",
       "      <td>2024-08-14</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arxiv.org/pdf/2406.17864</td>\n",
       "      <td>The AI Risk Taxonomy (AIR 2024)</td>\n",
       "      <td>We present a comprehensive AI risk taxonomy de...</td>\n",
       "      <td>https://arxiv.org/pdf/2406.17864</td>\n",
       "      <td>2024-09-05</td>\n",
       "      <td>2024-09-05</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>arxiv.org/2310.12941</td>\n",
       "      <td>The Foundation Model Transparency Index</td>\n",
       "      <td>To assess the transparency of the foundation m...</td>\n",
       "      <td>https://arxiv.org/abs/2310.12941</td>\n",
       "      <td>2023-10-19</td>\n",
       "      <td>2023-10-19</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>arxiv.org/2109.07958</td>\n",
       "      <td>TruthfulQA: Measuring How Models Mimic Human F...</td>\n",
       "      <td>TruthfulQA is a benchmark to measure whether a...</td>\n",
       "      <td>https://arxiv.org/abs/2109.07958</td>\n",
       "      <td>2021-09-08</td>\n",
       "      <td>2022-05-08</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>repo_truthful_qa</td>\n",
       "      <td>TruthfulQA</td>\n",
       "      <td>Repository for TruthfulQA.  TruthfulQA: Measur...</td>\n",
       "      <td>https://github.com/sylinrl/TruthfulQA</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-apache-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://arxiv.org/abs/2101.11718</td>\n",
       "      <td>BOLD: Dataset and Metrics for Measuring Biases...</td>\n",
       "      <td>Recent advances in deep learning techniques ha...</td>\n",
       "      <td>https://arxiv.org/abs/2101.11718</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://arxiv.org/abs/2311.04124</td>\n",
       "      <td>Unveiling Safety Vulnerabilities of Large Lang...</td>\n",
       "      <td>As large language models become more prevalent...</td>\n",
       "      <td>https://arxiv.org/abs/2311.04124</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://arxiv.org/abs/2010.00133</td>\n",
       "      <td>CrowS-Pairs: A Challenge Dataset for Measuring...</td>\n",
       "      <td>Pretrained language models, especially masked ...</td>\n",
       "      <td>https://arxiv.org/abs/2010.00133</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://arxiv.org/abs/2404.08676</td>\n",
       "      <td>ALERT: A Comprehensive Benchmark for Assessing...</td>\n",
       "      <td>When building Large Language Models (LLMs), it...</td>\n",
       "      <td>https://arxiv.org/abs/2404.08676</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>repo_Babelscape_ALERT</td>\n",
       "      <td>ALERT: A Comprehensive Benchmark for Assessing...</td>\n",
       "      <td>Official repository for the paper 'ALERT: A Co...</td>\n",
       "      <td>https://github.com/Babelscape/ALERT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-mit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://arxiv.org/abs/2402.05044</td>\n",
       "      <td>SALAD-Bench: A Hierarchical and Comprehensive ...</td>\n",
       "      <td>In the rapidly evolving landscape of Large Lan...</td>\n",
       "      <td>https://arxiv.org/abs/2402.05044</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://arxiv.org/abs/2406.14598</td>\n",
       "      <td>SORRY-Bench: Systematically Evaluating Large L...</td>\n",
       "      <td>Evaluating aligned large language models' (LLM...</td>\n",
       "      <td>https://arxiv.org/abs/2406.14598</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://arxiv.org/abs/2203.09509</td>\n",
       "      <td>ToxiGen: A Large-Scale Machine-Generated Datas...</td>\n",
       "      <td>Toxic language detection systems often falsely...</td>\n",
       "      <td>https://arxiv.org/abs/2203.09509</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-cc-by-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>repo_microsoft_toxigen</td>\n",
       "      <td>ToxiGen</td>\n",
       "      <td>Repository for ToxiGen. This repo contains the...</td>\n",
       "      <td>https://github.com/microsoft/toxigen</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-mit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://arxiv.org/abs/2308.01263</td>\n",
       "      <td>XSTest: A Test Suite for Identifying Exaggerat...</td>\n",
       "      <td>Without proper safeguards, large language mode...</td>\n",
       "      <td>https://arxiv.org/abs/2308.01263</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-cc-by-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>repo_paul-rottger_xstest</td>\n",
       "      <td>XSTest: A Test Suite for Identifying Exaggerat...</td>\n",
       "      <td>Repository for XSTest. Röttger et al. This rep...</td>\n",
       "      <td>https://github.com/paul-rottger/xstest</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-cc-by-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>https://arxiv.org/abs/2311.08370</td>\n",
       "      <td>SimpleSafetyTests: a Test Suite for Identifyin...</td>\n",
       "      <td>The past year has seen rapid acceleration in t...</td>\n",
       "      <td>https://arxiv.org/abs/2311.08370</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-cc-by-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>repo_bertiev_SimpleSafetyTests</td>\n",
       "      <td>SimpleSafetyTests</td>\n",
       "      <td>SimpleSafetyTests contains prompts that relate...</td>\n",
       "      <td>https://github.com/bertiev/SimpleSafetyTests</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-cc-by-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>https://arxiv.org/abs/2110.08193</td>\n",
       "      <td>BBQ: A Hand-Built Bias Benchmark for Question ...</td>\n",
       "      <td>It is well documented that NLP models learn so...</td>\n",
       "      <td>https://arxiv.org/abs/2110.08193</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>repo_nyu-mll_BBQ</td>\n",
       "      <td>BBQ</td>\n",
       "      <td>Repository for the Bias Benchmark for QA dataset.</td>\n",
       "      <td>https://github.com/nyu-mll/BBQ</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-cc-by-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>https://arxiv.org/abs/2312.03689</td>\n",
       "      <td>Evaluating and Mitigating Discrimination in La...</td>\n",
       "      <td>As language models (LMs) advance, interest is ...</td>\n",
       "      <td>https://arxiv.org/abs/2312.03689</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>https://arxiv.org/abs/2310.00905</td>\n",
       "      <td>All Languages Matter: On the Multilingual Safe...</td>\n",
       "      <td>Safety lies at the core of developing and depl...</td>\n",
       "      <td>https://arxiv.org/abs/2310.00905</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>https://arxiv.org/abs/2406.07599</td>\n",
       "      <td>CTIBench: A Benchmark for Evaluating LLMs in C...</td>\n",
       "      <td>Cyber threat intelligence (CTI) is crucial in ...</td>\n",
       "      <td>https://arxiv.org/abs/2406.07599</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-cc-by-nc-sa-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>repo_xashru_cti-bench</td>\n",
       "      <td>cti-bench</td>\n",
       "      <td>Repository for CTIBench. This repository conta...</td>\n",
       "      <td>https://github.com/xashru/cti-bench</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-cc-by-nc-sa-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>https://arxiv.org/abs/2408.01605</td>\n",
       "      <td>CYBERSECEVAL 3: Advancing the Evaluation of Cy...</td>\n",
       "      <td>We are releasing a new suite of security bench...</td>\n",
       "      <td>https://arxiv.org/abs/2408.01605</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-cc-by-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>https://arxiv.org/abs/2404.13161</td>\n",
       "      <td>CyberSecEval 2: A Wide-Ranging Cybersecurity E...</td>\n",
       "      <td>Large language models (LLMs) introduce new sec...</td>\n",
       "      <td>https://arxiv.org/abs/2404.13161</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-cc-by-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>https://arxiv.org/abs/2006.08328</td>\n",
       "      <td>ETHOS: an Online Hate Speech Detection Dataset</td>\n",
       "      <td>Online hate speech is a recent problem in our ...</td>\n",
       "      <td>https://arxiv.org/abs/2006.08328</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>https://arxiv.org/abs/2212.10511</td>\n",
       "      <td>When Not to Trust Language Models: Investigati...</td>\n",
       "      <td>Despite their impressive performance on divers...</td>\n",
       "      <td>https://arxiv.org/abs/2212.10511</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>https://arxiv.org/abs/2403.03218</td>\n",
       "      <td>The WMDP Benchmark: Measuring and Reducing Mal...</td>\n",
       "      <td>The White House Executive Order on Artificial ...</td>\n",
       "      <td>https://arxiv.org/abs/2403.03218</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>repo_centerforaisafety_wmdp</td>\n",
       "      <td>The WMDP Benchmark: Measuring and Reducing Mal...</td>\n",
       "      <td>Repository for the WMDP Benchmark.  WMDP is a ...</td>\n",
       "      <td>https://github.com/centerforaisafety/wmdp</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>https://arxiv.org/abs/2402.10260</td>\n",
       "      <td>A StrongREJECT for Empty Jailbreaks</td>\n",
       "      <td>Most jailbreak papers claim the jailbreaks the...</td>\n",
       "      <td>https://arxiv.org/abs/2402.10260</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-cc-by-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>repo_dsbowen_strong_reject</td>\n",
       "      <td>StrongREJECT jailbreak benchmark</td>\n",
       "      <td>Repository for StrongREJECT jailbreak benchmar...</td>\n",
       "      <td>https://github.com/dsbowen/strong_reject</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-mit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>https://arxiv.org/abs/2404.12241</td>\n",
       "      <td>Introducing v0.5 of the AI Safety Benchmark fr...</td>\n",
       "      <td>This paper introduces v0.5 of the AI Safety Be...</td>\n",
       "      <td>https://arxiv.org/abs/2404.12241</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>https://arxiv.org/abs/2407.17436</td>\n",
       "      <td>AIR-Bench 2024: A Safety Benchmark Based on Ri...</td>\n",
       "      <td>Foundation models (FMs) provide societal benef...</td>\n",
       "      <td>https://arxiv.org/abs/2407.17436</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>repo_stanford_air_bench_2024</td>\n",
       "      <td>AIR-Bench 2024: A Safety Benchmark Based on Ri...</td>\n",
       "      <td>Repository for AIR-Bench 2024</td>\n",
       "      <td>https://github.com/stanford-crfm/air-bench-2024</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-apache-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>https://aclanthology.org/2023.acl-long.546.pdf</td>\n",
       "      <td>When Not to Trust Language Models: Investigati...</td>\n",
       "      <td>Despite their impressive performance on divers...</td>\n",
       "      <td>https://aclanthology.org/2023.acl-long.546.pdf</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-cc-by-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>https://arxiv.org/abs/2503.05731</td>\n",
       "      <td>AILuminate: Introducing v1.0 of the AI Risk an...</td>\n",
       "      <td>The rapid advancement and deployment of AI sys...</td>\n",
       "      <td>https://arxiv.org/abs/2503.05731</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-cc-by-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>arxiv.org/2310.06786</td>\n",
       "      <td>OpenWebMath: An Open Dataset of High-Quality M...</td>\n",
       "      <td>There is growing evidence that pretraining on ...</td>\n",
       "      <td>https://arxiv.org/abs/2310.06786</td>\n",
       "      <td>2023-10-10</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>granite-3.0-paper</td>\n",
       "      <td>Granite 3.0 Language Models</td>\n",
       "      <td>This report presents Granite 3.0, a new set of...</td>\n",
       "      <td>https://github.com/ibm-granite/granite-3.0-lan...</td>\n",
       "      <td>2024-10-21</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>granite-guardian-paper</td>\n",
       "      <td>Granite Guardian</td>\n",
       "      <td>We introduce the Granite Guardian models, a su...</td>\n",
       "      <td>https://arxiv.org/abs/2412.07724</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>credo-doc</td>\n",
       "      <td>The Unified Control Framework: Establishing a ...</td>\n",
       "      <td>The rapid advancement and deployment of AI sys...</td>\n",
       "      <td>https://arxiv.org/pdf/2503.05937v1</td>\n",
       "      <td>2025-03-07</td>\n",
       "      <td>2025-03-07</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                id  \\\n",
       "0                                 10a99803d8afd656   \n",
       "1                                    NIST.AI.600-1   \n",
       "2                                   AILuminate-doc   \n",
       "3                             arxiv.org/2408.12622   \n",
       "4                         arxiv.org/pdf/2406.17864   \n",
       "5                             arxiv.org/2310.12941   \n",
       "6                             arxiv.org/2109.07958   \n",
       "7                                 repo_truthful_qa   \n",
       "8                 https://arxiv.org/abs/2101.11718   \n",
       "9                 https://arxiv.org/abs/2311.04124   \n",
       "10                https://arxiv.org/abs/2010.00133   \n",
       "11                https://arxiv.org/abs/2404.08676   \n",
       "12                           repo_Babelscape_ALERT   \n",
       "13                https://arxiv.org/abs/2402.05044   \n",
       "14                https://arxiv.org/abs/2406.14598   \n",
       "15                https://arxiv.org/abs/2203.09509   \n",
       "16                          repo_microsoft_toxigen   \n",
       "17                https://arxiv.org/abs/2308.01263   \n",
       "18                        repo_paul-rottger_xstest   \n",
       "19                https://arxiv.org/abs/2311.08370   \n",
       "20                  repo_bertiev_SimpleSafetyTests   \n",
       "21                https://arxiv.org/abs/2110.08193   \n",
       "22                                repo_nyu-mll_BBQ   \n",
       "23                https://arxiv.org/abs/2312.03689   \n",
       "24                https://arxiv.org/abs/2310.00905   \n",
       "25                https://arxiv.org/abs/2406.07599   \n",
       "26                           repo_xashru_cti-bench   \n",
       "27                https://arxiv.org/abs/2408.01605   \n",
       "28                https://arxiv.org/abs/2404.13161   \n",
       "29                https://arxiv.org/abs/2006.08328   \n",
       "30                https://arxiv.org/abs/2212.10511   \n",
       "31                https://arxiv.org/abs/2403.03218   \n",
       "32                     repo_centerforaisafety_wmdp   \n",
       "33                https://arxiv.org/abs/2402.10260   \n",
       "34                      repo_dsbowen_strong_reject   \n",
       "35                https://arxiv.org/abs/2404.12241   \n",
       "36                https://arxiv.org/abs/2407.17436   \n",
       "37                    repo_stanford_air_bench_2024   \n",
       "38  https://aclanthology.org/2023.acl-long.546.pdf   \n",
       "39                https://arxiv.org/abs/2503.05731   \n",
       "40                            arxiv.org/2310.06786   \n",
       "41                               granite-3.0-paper   \n",
       "42                          granite-guardian-paper   \n",
       "43                                       credo-doc   \n",
       "\n",
       "                                                 name  \\\n",
       "0   Foundation models: Opportunities, risks and mi...   \n",
       "1   Artificial Intelligence Risk Management Framew...   \n",
       "2   AILuminate: Introducing v1.0 of the AI Risk an...   \n",
       "3   The AI Risk Repository: A Comprehensive Meta-R...   \n",
       "4                     The AI Risk Taxonomy (AIR 2024)   \n",
       "5             The Foundation Model Transparency Index   \n",
       "6   TruthfulQA: Measuring How Models Mimic Human F...   \n",
       "7                                          TruthfulQA   \n",
       "8   BOLD: Dataset and Metrics for Measuring Biases...   \n",
       "9   Unveiling Safety Vulnerabilities of Large Lang...   \n",
       "10  CrowS-Pairs: A Challenge Dataset for Measuring...   \n",
       "11  ALERT: A Comprehensive Benchmark for Assessing...   \n",
       "12  ALERT: A Comprehensive Benchmark for Assessing...   \n",
       "13  SALAD-Bench: A Hierarchical and Comprehensive ...   \n",
       "14  SORRY-Bench: Systematically Evaluating Large L...   \n",
       "15  ToxiGen: A Large-Scale Machine-Generated Datas...   \n",
       "16                                            ToxiGen   \n",
       "17  XSTest: A Test Suite for Identifying Exaggerat...   \n",
       "18  XSTest: A Test Suite for Identifying Exaggerat...   \n",
       "19  SimpleSafetyTests: a Test Suite for Identifyin...   \n",
       "20                                  SimpleSafetyTests   \n",
       "21  BBQ: A Hand-Built Bias Benchmark for Question ...   \n",
       "22                                                BBQ   \n",
       "23  Evaluating and Mitigating Discrimination in La...   \n",
       "24  All Languages Matter: On the Multilingual Safe...   \n",
       "25  CTIBench: A Benchmark for Evaluating LLMs in C...   \n",
       "26                                          cti-bench   \n",
       "27  CYBERSECEVAL 3: Advancing the Evaluation of Cy...   \n",
       "28  CyberSecEval 2: A Wide-Ranging Cybersecurity E...   \n",
       "29     ETHOS: an Online Hate Speech Detection Dataset   \n",
       "30  When Not to Trust Language Models: Investigati...   \n",
       "31  The WMDP Benchmark: Measuring and Reducing Mal...   \n",
       "32  The WMDP Benchmark: Measuring and Reducing Mal...   \n",
       "33                A StrongREJECT for Empty Jailbreaks   \n",
       "34                   StrongREJECT jailbreak benchmark   \n",
       "35  Introducing v0.5 of the AI Safety Benchmark fr...   \n",
       "36  AIR-Bench 2024: A Safety Benchmark Based on Ri...   \n",
       "37  AIR-Bench 2024: A Safety Benchmark Based on Ri...   \n",
       "38  When Not to Trust Language Models: Investigati...   \n",
       "39  AILuminate: Introducing v1.0 of the AI Risk an...   \n",
       "40  OpenWebMath: An Open Dataset of High-Quality M...   \n",
       "41                        Granite 3.0 Language Models   \n",
       "42                                   Granite Guardian   \n",
       "43  The Unified Control Framework: Establishing a ...   \n",
       "\n",
       "                                          description  \\\n",
       "0   In this document we: Explore the benefits of f...   \n",
       "1   This document is a cross-sectoral profile of a...   \n",
       "2   The rapid advancement and deployment of AI sys...   \n",
       "3   The risks posed by Artificial Intelligence (AI...   \n",
       "4   We present a comprehensive AI risk taxonomy de...   \n",
       "5   To assess the transparency of the foundation m...   \n",
       "6   TruthfulQA is a benchmark to measure whether a...   \n",
       "7   Repository for TruthfulQA.  TruthfulQA: Measur...   \n",
       "8   Recent advances in deep learning techniques ha...   \n",
       "9   As large language models become more prevalent...   \n",
       "10  Pretrained language models, especially masked ...   \n",
       "11  When building Large Language Models (LLMs), it...   \n",
       "12  Official repository for the paper 'ALERT: A Co...   \n",
       "13  In the rapidly evolving landscape of Large Lan...   \n",
       "14  Evaluating aligned large language models' (LLM...   \n",
       "15  Toxic language detection systems often falsely...   \n",
       "16  Repository for ToxiGen. This repo contains the...   \n",
       "17  Without proper safeguards, large language mode...   \n",
       "18  Repository for XSTest. Röttger et al. This rep...   \n",
       "19  The past year has seen rapid acceleration in t...   \n",
       "20  SimpleSafetyTests contains prompts that relate...   \n",
       "21  It is well documented that NLP models learn so...   \n",
       "22  Repository for the Bias Benchmark for QA dataset.   \n",
       "23  As language models (LMs) advance, interest is ...   \n",
       "24  Safety lies at the core of developing and depl...   \n",
       "25  Cyber threat intelligence (CTI) is crucial in ...   \n",
       "26  Repository for CTIBench. This repository conta...   \n",
       "27  We are releasing a new suite of security bench...   \n",
       "28  Large language models (LLMs) introduce new sec...   \n",
       "29  Online hate speech is a recent problem in our ...   \n",
       "30  Despite their impressive performance on divers...   \n",
       "31  The White House Executive Order on Artificial ...   \n",
       "32  Repository for the WMDP Benchmark.  WMDP is a ...   \n",
       "33  Most jailbreak papers claim the jailbreaks the...   \n",
       "34  Repository for StrongREJECT jailbreak benchmar...   \n",
       "35  This paper introduces v0.5 of the AI Safety Be...   \n",
       "36  Foundation models (FMs) provide societal benef...   \n",
       "37                      Repository for AIR-Bench 2024   \n",
       "38  Despite their impressive performance on divers...   \n",
       "39  The rapid advancement and deployment of AI sys...   \n",
       "40  There is growing evidence that pretraining on ...   \n",
       "41  This report presents Granite 3.0, a new set of...   \n",
       "42  We introduce the Granite Guardian models, a su...   \n",
       "43  The rapid advancement and deployment of AI sys...   \n",
       "\n",
       "                                                  url dateCreated  \\\n",
       "0   https://www.ibm.com/downloads/documents/us-en/...        None   \n",
       "1   https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.6...  2024-07-25   \n",
       "2                    https://arxiv.org/pdf/2503.05731  2025-02-19   \n",
       "3                    https://arxiv.org/abs/2408.12622  2024-08-14   \n",
       "4                    https://arxiv.org/pdf/2406.17864  2024-09-05   \n",
       "5                    https://arxiv.org/abs/2310.12941  2023-10-19   \n",
       "6                    https://arxiv.org/abs/2109.07958  2021-09-08   \n",
       "7               https://github.com/sylinrl/TruthfulQA        None   \n",
       "8                    https://arxiv.org/abs/2101.11718        None   \n",
       "9                    https://arxiv.org/abs/2311.04124        None   \n",
       "10                   https://arxiv.org/abs/2010.00133        None   \n",
       "11                   https://arxiv.org/abs/2404.08676        None   \n",
       "12                https://github.com/Babelscape/ALERT        None   \n",
       "13                   https://arxiv.org/abs/2402.05044        None   \n",
       "14                   https://arxiv.org/abs/2406.14598        None   \n",
       "15                   https://arxiv.org/abs/2203.09509        None   \n",
       "16               https://github.com/microsoft/toxigen        None   \n",
       "17                   https://arxiv.org/abs/2308.01263        None   \n",
       "18             https://github.com/paul-rottger/xstest        None   \n",
       "19                   https://arxiv.org/abs/2311.08370        None   \n",
       "20       https://github.com/bertiev/SimpleSafetyTests        None   \n",
       "21                   https://arxiv.org/abs/2110.08193        None   \n",
       "22                     https://github.com/nyu-mll/BBQ        None   \n",
       "23                   https://arxiv.org/abs/2312.03689        None   \n",
       "24                   https://arxiv.org/abs/2310.00905        None   \n",
       "25                   https://arxiv.org/abs/2406.07599        None   \n",
       "26                https://github.com/xashru/cti-bench        None   \n",
       "27                   https://arxiv.org/abs/2408.01605        None   \n",
       "28                   https://arxiv.org/abs/2404.13161        None   \n",
       "29                   https://arxiv.org/abs/2006.08328        None   \n",
       "30                   https://arxiv.org/abs/2212.10511        None   \n",
       "31                   https://arxiv.org/abs/2403.03218        None   \n",
       "32          https://github.com/centerforaisafety/wmdp        None   \n",
       "33                   https://arxiv.org/abs/2402.10260        None   \n",
       "34           https://github.com/dsbowen/strong_reject        None   \n",
       "35                   https://arxiv.org/abs/2404.12241        None   \n",
       "36                   https://arxiv.org/abs/2407.17436        None   \n",
       "37    https://github.com/stanford-crfm/air-bench-2024        None   \n",
       "38     https://aclanthology.org/2023.acl-long.546.pdf        None   \n",
       "39                   https://arxiv.org/abs/2503.05731        None   \n",
       "40                   https://arxiv.org/abs/2310.06786  2023-10-10   \n",
       "41  https://github.com/ibm-granite/granite-3.0-lan...  2024-10-21   \n",
       "42                   https://arxiv.org/abs/2412.07724        None   \n",
       "43                 https://arxiv.org/pdf/2503.05937v1  2025-03-07   \n",
       "\n",
       "   dateModified               hasLicense  \n",
       "0          None                     None  \n",
       "1          None                     None  \n",
       "2          None                     None  \n",
       "3    2024-08-14                     None  \n",
       "4    2024-09-05                     None  \n",
       "5    2023-10-19                     None  \n",
       "6    2022-05-08                     None  \n",
       "7          None       license-apache-2.0  \n",
       "8          None                     None  \n",
       "9          None                     None  \n",
       "10         None                     None  \n",
       "11         None                     None  \n",
       "12         None              license-mit  \n",
       "13         None                     None  \n",
       "14         None                     None  \n",
       "15         None        license-cc-by-4.0  \n",
       "16         None              license-mit  \n",
       "17         None        license-cc-by-4.0  \n",
       "18         None        license-cc-by-4.0  \n",
       "19         None        license-cc-by-4.0  \n",
       "20         None        license-cc-by-4.0  \n",
       "21         None                     None  \n",
       "22         None        license-cc-by-4.0  \n",
       "23         None                     None  \n",
       "24         None                     None  \n",
       "25         None  license-cc-by-nc-sa-4.0  \n",
       "26         None  license-cc-by-nc-sa-4.0  \n",
       "27         None        license-cc-by-4.0  \n",
       "28         None        license-cc-by-4.0  \n",
       "29         None                     None  \n",
       "30         None                     None  \n",
       "31         None                     None  \n",
       "32         None                     None  \n",
       "33         None        license-cc-by-4.0  \n",
       "34         None              license-mit  \n",
       "35         None                     None  \n",
       "36         None                     None  \n",
       "37         None       license-apache-2.0  \n",
       "38         None        license-cc-by-4.0  \n",
       "39         None        license-cc-by-4.0  \n",
       "40         None                     None  \n",
       "41         None                     None  \n",
       "42         None                     None  \n",
       "43   2025-03-07                     None  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_docs = pd.DataFrame([item.__dict__ for item in ran2._ontology.documents])\n",
    "df_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2186bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>dateCreated</th>\n",
       "      <th>dateModified</th>\n",
       "      <th>hasLicense</th>\n",
       "      <th>hasDocumentation</th>\n",
       "      <th>provider</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>truthfulqa/truthful_qa</td>\n",
       "      <td>truthful_qa</td>\n",
       "      <td>TruthfulQA is a benchmark to measure whether a...</td>\n",
       "      <td>https://huggingface.co/datasets/truthfulqa/tru...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-apache-2.0</td>\n",
       "      <td>[arxiv.org/2109.07958, repo_truthful_qa]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AlexaAI/bold</td>\n",
       "      <td>BOLD (Bias in Open-ended Language Generation D...</td>\n",
       "      <td>Bias in Open-ended Language Generation Dataset...</td>\n",
       "      <td>https://huggingface.co/datasets/AlexaAI/bold</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-cc-by-4.0</td>\n",
       "      <td>[https://arxiv.org/abs/2101.11718]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ibm-research/AttaQ</td>\n",
       "      <td>AttaQ Dataset</td>\n",
       "      <td>The AttaQ red teaming dataset, consisting of 1...</td>\n",
       "      <td>https://huggingface.co/datasets/ibm-research/A...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-mit</td>\n",
       "      <td>[https://arxiv.org/abs/2311.04124]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ibm-research/ProvoQ</td>\n",
       "      <td>The ProvoQ (PROVOcative Questions about minori...</td>\n",
       "      <td>The ProvoQ dataset is designed to evaluate the...</td>\n",
       "      <td>https://huggingface.co/datasets/ibm-research/P...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-cdla-permissive-2.0</td>\n",
       "      <td>[https://arxiv.org/abs/2311.04124]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nyu-mll/crows_pairs</td>\n",
       "      <td>Crowdsourced Stereotype Pairs benchmark (CrowS...</td>\n",
       "      <td>a challenge dataset for measuring the degree t...</td>\n",
       "      <td>https://huggingface.co/datasets/nyu-mll/crows_...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-cc-by-sa-4.0</td>\n",
       "      <td>[https://arxiv.org/abs/2010.00133]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Babelscape/ALERT</td>\n",
       "      <td>ALERT</td>\n",
       "      <td>A large-scale benchmark to assess the safety o...</td>\n",
       "      <td>https://huggingface.co/datasets/Babelscape/ALERT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-cc-by-nc-sa-4.0</td>\n",
       "      <td>[https://arxiv.org/abs/2404.08676, repo_Babels...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OpenSafetyLab/Salad-Data</td>\n",
       "      <td>Salad Data</td>\n",
       "      <td>A challenging safety benchmark specifically de...</td>\n",
       "      <td>https://huggingface.co/datasets/OpenSafetyLab/...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-apache-2.0</td>\n",
       "      <td>[https://arxiv.org/abs/2402.05044]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sorry-bench/sorry-bench-202406</td>\n",
       "      <td>SorryBench</td>\n",
       "      <td>This dataset contains 9.5K potentially unsafe ...</td>\n",
       "      <td>https://huggingface.co/datasets/sorry-bench/so...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-sorrybench</td>\n",
       "      <td>[https://arxiv.org/abs/2406.14598]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>toxigen/toxigen-data</td>\n",
       "      <td>Toxigen</td>\n",
       "      <td>This dataset is for implicit hate speech detec...</td>\n",
       "      <td>https://huggingface.co/datasets/toxigen/toxige...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[repo_microsoft_toxigen, https://arxiv.org/abs...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Paul/XSTest</td>\n",
       "      <td>XSTest: A Test Suite for Identifying Exaggerat...</td>\n",
       "      <td>XSTest is a test suite designed to identify ex...</td>\n",
       "      <td>https://huggingface.co/datasets/Paul/XSTest</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-cc-by-4.0</td>\n",
       "      <td>[https://arxiv.org/abs/2308.01263, repo_paul-r...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>strong_reject</td>\n",
       "      <td>StrongREJECT jailbreak benchmark dataset</td>\n",
       "      <td>StrongREJECT jailbreak benchmark dataset</td>\n",
       "      <td>https://github.com/dsbowen/strong_reject/tree/...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-mit</td>\n",
       "      <td>[https://arxiv.org/abs/2402.10260, repo_dsbowe...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Bertievidgen/SimpleSafetyTests</td>\n",
       "      <td>SimpleSafetyTests</td>\n",
       "      <td>SimpleSafetyTests contains prompts that relate...</td>\n",
       "      <td>https://huggingface.co/datasets/Bertievidgen/S...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-cc-by-2.0</td>\n",
       "      <td>[https://arxiv.org/abs/2311.08370, repo_bertie...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>heegyu/bbq</td>\n",
       "      <td>Bias Benchmark for QA dataset</td>\n",
       "      <td>A dataset of question sets constructed by the ...</td>\n",
       "      <td>https://huggingface.co/datasets/heegyu/bbq</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-cc-by-4.0</td>\n",
       "      <td>[repo_nyu-mll_BBQ, https://arxiv.org/abs/2110....</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Anthropic/discrim-eval</td>\n",
       "      <td>Discrim-Eval</td>\n",
       "      <td>The data contains a diverse set of prompts cov...</td>\n",
       "      <td>https://huggingface.co/datasets/Anthropic/disc...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-cc-by-4.0</td>\n",
       "      <td>[https://arxiv.org/abs/2312.03689]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mlcommons_ailuminate_airr_official_1.0_demo_en...</td>\n",
       "      <td>AILuminate v1.0 DEMO Prompt Set</td>\n",
       "      <td>This file contains the DEMO prompt library of ...</td>\n",
       "      <td>https://github.com/mlcommons/ailuminate/blob/m...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-apache-2.0</td>\n",
       "      <td>[https://arxiv.org/abs/2503.05731]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>stanford-crfm/air-bench-2024</td>\n",
       "      <td>AIRBench 2024</td>\n",
       "      <td>AIRBench 2024 is a AI safety benchmark that al...</td>\n",
       "      <td>https://huggingface.co/datasets/stanford-crfm/...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-cc-by-4.0</td>\n",
       "      <td>[https://arxiv.org/abs/2407.17436, repo_stanfo...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AI4Sec/cti-bench</td>\n",
       "      <td>CTIBench</td>\n",
       "      <td>A set of benchmark tasks and datasets designed...</td>\n",
       "      <td>https://huggingface.co/datasets/AI4Sec/cti-bench</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-cc-by-nc-sa-4.0</td>\n",
       "      <td>[https://arxiv.org/abs/2406.07599, repo_xashru...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>CybersecurityBenchmarks_datasets_prompt_injection</td>\n",
       "      <td>Prompt injection</td>\n",
       "      <td>Textual Prompt Injection Tests: An english-onl...</td>\n",
       "      <td>https://github.com/meta-llama/PurpleLlama/blob...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-llama-3.2-community</td>\n",
       "      <td>[https://arxiv.org/abs/2408.01605]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cais/wmdp</td>\n",
       "      <td>Weapons of Mass Destruction Proxy (WMDP)</td>\n",
       "      <td>The Weapons of Mass Destruction Proxy (WMDP) b...</td>\n",
       "      <td>https://huggingface.co/datasets/cais/wmdp</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-mit</td>\n",
       "      <td>[https://arxiv.org/abs/2403.03218, repo_center...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CybersecurityBenchmarks_datasets_frr</td>\n",
       "      <td>False Refusal Rate (FRR)</td>\n",
       "      <td>A dataset to evaluate the False Refusal Rate (...</td>\n",
       "      <td>https://github.com/meta-llama/PurpleLlama/blob...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-llama-3.2-community</td>\n",
       "      <td>[https://arxiv.org/abs/2404.13161]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>iamollas/ethos</td>\n",
       "      <td>ETHOS: an Online Hate Speech Detection Dataset</td>\n",
       "      <td>ETHOS: onlinE haTe speecH detectiOn dataSet. T...</td>\n",
       "      <td>https://huggingface.co/datasets/iamollas/ethos</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-gnu-gplv3</td>\n",
       "      <td>[https://arxiv.org/abs/2006.08328]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>akariasai/PopQA</td>\n",
       "      <td>PopQA</td>\n",
       "      <td>PopQA is a large-scale open-domain question an...</td>\n",
       "      <td>https://huggingface.co/datasets/akariasai/PopQA</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-cc-by-4.0</td>\n",
       "      <td>[https://aclanthology.org/2023.acl-long.546.pdf]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Jarviswang94_Multilingual_safety_benchmark</td>\n",
       "      <td>XSafety dataset</td>\n",
       "      <td>We build the first multilingual safety benchma...</td>\n",
       "      <td>https://github.com/Jarviswang94/Multilingual_s...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-apache-2.0</td>\n",
       "      <td>[https://arxiv.org/abs/2310.00905]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>github-code-clean</td>\n",
       "      <td>Github-code-clean</td>\n",
       "      <td>This is a cleaner version of Github-code datas...</td>\n",
       "      <td>https://huggingface.co/datasets/codeparrot/git...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>license-apache-2.0</td>\n",
       "      <td>None</td>\n",
       "      <td>codeparrot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>starcoder</td>\n",
       "      <td>Star Coder Training Dataset</td>\n",
       "      <td>This is the dataset used for training StarCode...</td>\n",
       "      <td>https://huggingface.co/datasets/bigcode/starco...</td>\n",
       "      <td>2023-03-30</td>\n",
       "      <td>2023-05-16</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>bigcode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>open-web-math</td>\n",
       "      <td>OpenWebMath Dataset</td>\n",
       "      <td>OpenWebMath is a dataset containing the majori...</td>\n",
       "      <td>https://huggingface.co/datasets/open-web-math/...</td>\n",
       "      <td>2023-09-06</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[arxiv.org/2310.06786]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   id  \\\n",
       "0                              truthfulqa/truthful_qa   \n",
       "1                                        AlexaAI/bold   \n",
       "2                                  ibm-research/AttaQ   \n",
       "3                                 ibm-research/ProvoQ   \n",
       "4                                 nyu-mll/crows_pairs   \n",
       "5                                    Babelscape/ALERT   \n",
       "6                            OpenSafetyLab/Salad-Data   \n",
       "7                      sorry-bench/sorry-bench-202406   \n",
       "8                                toxigen/toxigen-data   \n",
       "9                                         Paul/XSTest   \n",
       "10                                      strong_reject   \n",
       "11                     Bertievidgen/SimpleSafetyTests   \n",
       "12                                         heegyu/bbq   \n",
       "13                             Anthropic/discrim-eval   \n",
       "14  mlcommons_ailuminate_airr_official_1.0_demo_en...   \n",
       "15                       stanford-crfm/air-bench-2024   \n",
       "16                                   AI4Sec/cti-bench   \n",
       "17  CybersecurityBenchmarks_datasets_prompt_injection   \n",
       "18                                          cais/wmdp   \n",
       "19               CybersecurityBenchmarks_datasets_frr   \n",
       "20                                     iamollas/ethos   \n",
       "21                                    akariasai/PopQA   \n",
       "22         Jarviswang94_Multilingual_safety_benchmark   \n",
       "23                                  github-code-clean   \n",
       "24                                          starcoder   \n",
       "25                                      open-web-math   \n",
       "\n",
       "                                                 name  \\\n",
       "0                                         truthful_qa   \n",
       "1   BOLD (Bias in Open-ended Language Generation D...   \n",
       "2                                       AttaQ Dataset   \n",
       "3   The ProvoQ (PROVOcative Questions about minori...   \n",
       "4   Crowdsourced Stereotype Pairs benchmark (CrowS...   \n",
       "5                                               ALERT   \n",
       "6                                          Salad Data   \n",
       "7                                          SorryBench   \n",
       "8                                             Toxigen   \n",
       "9   XSTest: A Test Suite for Identifying Exaggerat...   \n",
       "10           StrongREJECT jailbreak benchmark dataset   \n",
       "11                                  SimpleSafetyTests   \n",
       "12                      Bias Benchmark for QA dataset   \n",
       "13                                       Discrim-Eval   \n",
       "14                    AILuminate v1.0 DEMO Prompt Set   \n",
       "15                                      AIRBench 2024   \n",
       "16                                           CTIBench   \n",
       "17                                   Prompt injection   \n",
       "18           Weapons of Mass Destruction Proxy (WMDP)   \n",
       "19                           False Refusal Rate (FRR)   \n",
       "20     ETHOS: an Online Hate Speech Detection Dataset   \n",
       "21                                              PopQA   \n",
       "22                                    XSafety dataset   \n",
       "23                                  Github-code-clean   \n",
       "24                        Star Coder Training Dataset   \n",
       "25                                OpenWebMath Dataset   \n",
       "\n",
       "                                          description  \\\n",
       "0   TruthfulQA is a benchmark to measure whether a...   \n",
       "1   Bias in Open-ended Language Generation Dataset...   \n",
       "2   The AttaQ red teaming dataset, consisting of 1...   \n",
       "3   The ProvoQ dataset is designed to evaluate the...   \n",
       "4   a challenge dataset for measuring the degree t...   \n",
       "5   A large-scale benchmark to assess the safety o...   \n",
       "6   A challenging safety benchmark specifically de...   \n",
       "7   This dataset contains 9.5K potentially unsafe ...   \n",
       "8   This dataset is for implicit hate speech detec...   \n",
       "9   XSTest is a test suite designed to identify ex...   \n",
       "10           StrongREJECT jailbreak benchmark dataset   \n",
       "11  SimpleSafetyTests contains prompts that relate...   \n",
       "12  A dataset of question sets constructed by the ...   \n",
       "13  The data contains a diverse set of prompts cov...   \n",
       "14  This file contains the DEMO prompt library of ...   \n",
       "15  AIRBench 2024 is a AI safety benchmark that al...   \n",
       "16  A set of benchmark tasks and datasets designed...   \n",
       "17  Textual Prompt Injection Tests: An english-onl...   \n",
       "18  The Weapons of Mass Destruction Proxy (WMDP) b...   \n",
       "19  A dataset to evaluate the False Refusal Rate (...   \n",
       "20  ETHOS: onlinE haTe speecH detectiOn dataSet. T...   \n",
       "21  PopQA is a large-scale open-domain question an...   \n",
       "22  We build the first multilingual safety benchma...   \n",
       "23  This is a cleaner version of Github-code datas...   \n",
       "24  This is the dataset used for training StarCode...   \n",
       "25  OpenWebMath is a dataset containing the majori...   \n",
       "\n",
       "                                                  url dateCreated  \\\n",
       "0   https://huggingface.co/datasets/truthfulqa/tru...        None   \n",
       "1        https://huggingface.co/datasets/AlexaAI/bold        None   \n",
       "2   https://huggingface.co/datasets/ibm-research/A...        None   \n",
       "3   https://huggingface.co/datasets/ibm-research/P...        None   \n",
       "4   https://huggingface.co/datasets/nyu-mll/crows_...        None   \n",
       "5    https://huggingface.co/datasets/Babelscape/ALERT        None   \n",
       "6   https://huggingface.co/datasets/OpenSafetyLab/...        None   \n",
       "7   https://huggingface.co/datasets/sorry-bench/so...        None   \n",
       "8   https://huggingface.co/datasets/toxigen/toxige...        None   \n",
       "9         https://huggingface.co/datasets/Paul/XSTest        None   \n",
       "10  https://github.com/dsbowen/strong_reject/tree/...        None   \n",
       "11  https://huggingface.co/datasets/Bertievidgen/S...        None   \n",
       "12         https://huggingface.co/datasets/heegyu/bbq        None   \n",
       "13  https://huggingface.co/datasets/Anthropic/disc...        None   \n",
       "14  https://github.com/mlcommons/ailuminate/blob/m...        None   \n",
       "15  https://huggingface.co/datasets/stanford-crfm/...        None   \n",
       "16   https://huggingface.co/datasets/AI4Sec/cti-bench        None   \n",
       "17  https://github.com/meta-llama/PurpleLlama/blob...        None   \n",
       "18          https://huggingface.co/datasets/cais/wmdp        None   \n",
       "19  https://github.com/meta-llama/PurpleLlama/blob...        None   \n",
       "20     https://huggingface.co/datasets/iamollas/ethos        None   \n",
       "21    https://huggingface.co/datasets/akariasai/PopQA        None   \n",
       "22  https://github.com/Jarviswang94/Multilingual_s...        None   \n",
       "23  https://huggingface.co/datasets/codeparrot/git...        None   \n",
       "24  https://huggingface.co/datasets/bigcode/starco...  2023-03-30   \n",
       "25  https://huggingface.co/datasets/open-web-math/...  2023-09-06   \n",
       "\n",
       "   dateModified                   hasLicense  \\\n",
       "0          None           license-apache-2.0   \n",
       "1          None            license-cc-by-4.0   \n",
       "2          None                  license-mit   \n",
       "3          None  license-cdla-permissive-2.0   \n",
       "4          None         license-cc-by-sa-4.0   \n",
       "5          None      license-cc-by-nc-sa-4.0   \n",
       "6          None           license-apache-2.0   \n",
       "7          None           license-sorrybench   \n",
       "8          None                         None   \n",
       "9          None            license-cc-by-4.0   \n",
       "10         None                  license-mit   \n",
       "11         None            license-cc-by-2.0   \n",
       "12         None            license-cc-by-4.0   \n",
       "13         None            license-cc-by-4.0   \n",
       "14         None           license-apache-2.0   \n",
       "15         None            license-cc-by-4.0   \n",
       "16         None      license-cc-by-nc-sa-4.0   \n",
       "17         None  license-llama-3.2-community   \n",
       "18         None                  license-mit   \n",
       "19         None  license-llama-3.2-community   \n",
       "20         None            license-gnu-gplv3   \n",
       "21         None            license-cc-by-4.0   \n",
       "22         None           license-apache-2.0   \n",
       "23         None           license-apache-2.0   \n",
       "24   2023-05-16                         None   \n",
       "25         None                         None   \n",
       "\n",
       "                                     hasDocumentation    provider  \n",
       "0            [arxiv.org/2109.07958, repo_truthful_qa]        None  \n",
       "1                  [https://arxiv.org/abs/2101.11718]        None  \n",
       "2                  [https://arxiv.org/abs/2311.04124]        None  \n",
       "3                  [https://arxiv.org/abs/2311.04124]        None  \n",
       "4                  [https://arxiv.org/abs/2010.00133]        None  \n",
       "5   [https://arxiv.org/abs/2404.08676, repo_Babels...        None  \n",
       "6                  [https://arxiv.org/abs/2402.05044]        None  \n",
       "7                  [https://arxiv.org/abs/2406.14598]        None  \n",
       "8   [repo_microsoft_toxigen, https://arxiv.org/abs...        None  \n",
       "9   [https://arxiv.org/abs/2308.01263, repo_paul-r...        None  \n",
       "10  [https://arxiv.org/abs/2402.10260, repo_dsbowe...        None  \n",
       "11  [https://arxiv.org/abs/2311.08370, repo_bertie...        None  \n",
       "12  [repo_nyu-mll_BBQ, https://arxiv.org/abs/2110....        None  \n",
       "13                 [https://arxiv.org/abs/2312.03689]        None  \n",
       "14                 [https://arxiv.org/abs/2503.05731]        None  \n",
       "15  [https://arxiv.org/abs/2407.17436, repo_stanfo...        None  \n",
       "16  [https://arxiv.org/abs/2406.07599, repo_xashru...        None  \n",
       "17                 [https://arxiv.org/abs/2408.01605]        None  \n",
       "18  [https://arxiv.org/abs/2403.03218, repo_center...        None  \n",
       "19                 [https://arxiv.org/abs/2404.13161]        None  \n",
       "20                 [https://arxiv.org/abs/2006.08328]        None  \n",
       "21   [https://aclanthology.org/2023.acl-long.546.pdf]        None  \n",
       "22                 [https://arxiv.org/abs/2310.00905]        None  \n",
       "23                                               None  codeparrot  \n",
       "24                                               None     bigcode  \n",
       "25                             [arxiv.org/2310.06786]        None  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_datasets = pd.DataFrame([item.__dict__ for item in ran2._ontology.datasets])\n",
    "df_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "715e226e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([None, list(['truthfulqa/truthful_qa']),\n",
       "       list(['https://huggingface.co/datasets/AlexaAI/bold']),\n",
       "       list(['https://huggingface.co/datasets/ibm-research/AttaQ']),\n",
       "       list(['https://huggingface.co/datasets/ibm-research/ProvoQ']),\n",
       "       list(['https://huggingface.co/datasets/nyu-mll/crows_pairs']),\n",
       "       list(['https://huggingface.co/datasets/Babelscape/ALERT']),\n",
       "       list(['https://huggingface.co/datasets/OpenSafetyLab/Salad-Data']),\n",
       "       list(['https://huggingface.co/datasets/sorry-bench/sorry-bench-202406']),\n",
       "       list(['https://huggingface.co/datasets/toxigen/toxigen-data']),\n",
       "       list(['https://huggingface.co/datasets/Paul/XSTest']),\n",
       "       list(['https://github.com/dsbowen/strong_reject/tree/main/strong_reject']),\n",
       "       list(['https://huggingface.co/datasets/Bertievidgen/SimpleSafetyTests']),\n",
       "       list(['https://huggingface.co/datasets/heegyu/bbq']),\n",
       "       list(['https://huggingface.co/datasets/Anthropic/discrim-eval']),\n",
       "       None,\n",
       "       list(['https://github.com/mlcommons/ailuminate/blob/main/airr_official_1.0_practice_prompt_set_release_public_subset.csv']),\n",
       "       list(['https://huggingface.co/datasets/stanford-crfm/air-bench-2024']),\n",
       "       list(['https://huggingface.co/datasets/AI4Sec/cti-bench']),\n",
       "       list(['https://github.com/meta-llama/PurpleLlama/blob/main/CybersecurityBenchmarks/datasets/prompt_injection/prompt_injection.json']),\n",
       "       list(['https://huggingface.co/datasets/cais/wmdp']),\n",
       "       list(['https://github.com/meta-llama/PurpleLlama/blob/main/CybersecurityBenchmarks/datasets/frr/frr.json']),\n",
       "       list(['https://huggingface.co/datasets/iamollas/ethos']),\n",
       "       list(['https://huggingface.co/datasets/akariasai/PopQA'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ev[\"hasDataset\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039c33c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_evals_bmcs = []\n",
    "for item in data_evals:\n",
    "    print(item) \n",
    "    \n",
    "    ai_ev = AiEval(\n",
    "        id=\"ai_eval_\"+item[\"Benchmark Details\"][\"Name\"].replace(\" \", \"_\"), \n",
    "        name=item[\"Benchmark Details\"][\"Name\"], \n",
    "        description= item[\"Benchmark Details\"][\"Overview\"],\n",
    "        hasBenchmarkMetadata= [\"ai_eval_bmc_\"+item[\"Benchmark Details\"][\"Name\"].replace(\" \", \"_\") ],\n",
    "        hasUnitxtCard=item[\"Benchmark Details\"][\"ID\"],\n",
    "        hasLicense=item[\"Ethical and Legal Considerations\"][\"Data Licensing\"],\n",
    "        \n",
    "      \n",
    "        # url: str | None = None, \n",
    "        # dateCreated: date | None = None, \n",
    "        # dateModified: date | None = None, \n",
    "        # hasDocumentation: List[str] | None = None, \n",
    "        # hasDataset: List[str] | None = None, \n",
    "     \n",
    "        # hasRelatedRisk: List[str] | None = None, \n",
    "        # bestValue: str | None = None, \n",
    "\n",
    "        )\n",
    "    data_evals_evals.append(ai_ev)\n",
    "    '''\n",
    "        AiEval  - id: cards.value_alignment.hallucinations.truthfulqa\n",
    "            name: \"TruthfulQA\"\n",
    "            description: \"TruthfulQA is a benchmark to measure whether a language model is truthful in generating answers to questions.\"\n",
    "            url: https://github.com/sylinrl/TruthfulQA\n",
    "            hasUnitxtCard: cards.value_alignment.hallucinations.truthfulqa\n",
    "            hasRelatedRisk: [\"atlas-hallucination\"]\n",
    "            hasDocumentation: [\"arxiv.org/2109.07958\"]\n",
    "            hasDataset: [\"truthfulqa/truthful_qa\"]\n",
    "\n",
    "        '''\n",
    "    \n",
    "\n",
    "    bmc = BenchmarkMetadataCard(\n",
    "        id=\"ai_eval_bmc_\"+item[\"Benchmark Details\"][\"Name\"].replace(\" \", \"_\"), \n",
    "        name=item[\"Benchmark Details\"][\"Name\"], \n",
    "        overview=item[\"Benchmark Details\"][\"Overview\"],\n",
    "        describesAiEval=[\"ai_eval_\"+item[\"Benchmark Details\"][\"Name\"].replace(\" \", \"_\")],\n",
    "        hasDataType=[item[\"Benchmark Details\"][\"Data Type\"]], \n",
    "        hasDomains=item[\"Benchmark Details\"][\"Domains\"], \n",
    "        hasLanguages=item[\"Benchmark Details\"][\"Languages\"], \n",
    "        hasGoal=item[\"Purpose and Intended Users\"][\"Goal\"], \n",
    "        hasAudience=item[\"Purpose and Intended Users\"][\"Audience\"],\n",
    "        hasTasks=item[\"Purpose and Intended Users\"][\"Tasks\"],\n",
    "        hasLimitations=[item[\"Risks\"][\"Limitations\"]],\n",
    "        hasOutOfScopeUses=[item[\"Purpose and Intended Users\"][\"Out-of-Scope Uses\"]],\n",
    "        hasDataSource=[item[\"Data\"][\"Source\"]],\n",
    "        hasDataSize=item[\"Data\"][\"Size\"],\n",
    "        hasDataFormat=item[\"Data\"][\"Format\"],\n",
    "        hasAnnotation=item[\"Data\"][\"Annotation\"],\n",
    "        hasMethods=[item[\"Methodology\"][\"Methods\"]],\n",
    "        hasMetrics=[item[\"Methodology\"][\"Metrics\"]],\n",
    "        hasCalculation=[item[\"Methodology\"][\"Calculation\"]],\n",
    "        hasInterpretation=[item[\"Methodology\"][\"Interpretation\"]],\n",
    "        hasValidation=[item[\"Methodology\"][\"Validation\"]],\n",
    "        hasDemographicAnalysis=item[\"Risks\"][\"Demographic Analysis\"] or None,\n",
    "        hasLicense=item[\"Ethical and Legal Considerations\"][\"Data Licensing\"],\n",
    "        hasConsiderationPrivacyAndAnonymity=item[\"Ethical and Legal Considerations\"][\"Privacy and Anonymity\"],\n",
    "        hasConsiderationConsentProcedures=item[\"Ethical and Legal Considerations\"][\"Consent Procedures\"],\n",
    "        hasConsiderationComplianceWithRegulations=item[\"Ethical and Legal Considerations\"][\"Compliance with Regulations\"],\n",
    "        )\n",
    "\n",
    "    data_evals_bmcs.append(bmc)\n",
    "    \n",
    "\n",
    "'''\n",
    "        #hasResources=[value for dictionary in item[\"Benchmark Details\"][\"Resources\"] for value in dictionary.values()],\n",
    "        #hasSimilarBenchmarks: List[str] | None = None,\n",
    "        \n",
    "        url: str | None = None,\n",
    "        dateCreated: date | None = None,\n",
    "        dateModified: date | None = None,\n",
    "        \n",
    "\n",
    "        hasBaselineResults: str | None = None,\n",
    "      \n",
    "        hasRelatedRisks: List[str] | None = None,\n",
    "        hasDocumentation: List[str] | None = None,\n",
    "      \n",
    "\n",
    "        '''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vrisk-atlas-nexus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
